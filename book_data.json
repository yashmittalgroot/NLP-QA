{"0": {"Section": "1", "Title": "Introduction", "Content": "introduction the problem searching for patterns data fundamental one and has long and successful history for instance the extensive astronomical observations tycho brahe the century allowed johannes kepler discover the empirical laws planetary motion which turn provided springboard for the development classical mechanics similarly the discovery regularities atomic spectra played key role the development and verification quantum physics the early twentieth century the field pattern recognition concerned with the automatic discovery regularities data through the use computer algorithms and with the use these regularities take actions such classifying the data into different categories consider the example recognizing handwritten digits illustrated figure each digit corresponds pixel image and can represented vector comprising real numbers the goal build machine that will take such vector input and that will produce the identity the digit the output this nontrivial problem due the wide variability handwriting could introduction figure examples hand written digits taken from zip codes tackled using handcrafted rules heuristics for distinguishing the digits based the shapes the strokes but practice such approach leads proliferation rules and exceptions the rules and and invariably gives poor results far better results can obtained adopting machine learning approach which large set digits called training set used tune the parameters adaptive model the categories the digits the training set are known advance typically inspecting them individually and hand labelling them can express the category digit using target vector which represents the identity the corresponding digit suitable techniques for representing categories terms vectors will discussed later note that there one such target vector for each digit image the result running the machine learning algorithm can expressed function which takes new digit image input and that generates output vector encoded the same way the target vectors the precise form the function determined during the training phase also known the learning phase the basis the training data once the model trained can then determine the identity new digit images which are said comprise test set the ability categorize correctly new examples that differ from those used for training known generalization practical applications the variability the input vectors will such that the training data can comprise only tiny fraction all possible input vectors and generalization central goal pattern recognition for most practical applications the original input variables are typically preprocessed transform them into some new space variables where hoped the pattern recognition problem will easier solve for instance the digit recognition problem the images the digits are typically translated and scaled that each digit contained within box fixed size this greatly reduces the variability within each digit class because the location and scale all the digits are now the same which makes much easier for subsequent pattern recognition algorithm distinguish between the different classes this pre processing stage sometimes also called feature extraction note that new test data must pre processed using the same steps the training data pre processing might also performed order speed computation for example the goal real time face detection high resolution video stream the computer must handle huge numbers pixels per second and presenting these directly complex pattern recognition algorithm may computationally infeasible instead the aim find useful features that are fast compute and yet that introduction also preserve useful discriminatory information enabling faces distinguished from non faces these features are then used the inputs the pattern recognition algorithm for instance the average value the image intensity over rectangular subregion can evaluated extremely efficiently viola and jones and set such features can prove very effective fast face detection because the number such features smaller than the number pixels this kind pre processing represents form dimensionality reduction care must taken during pre processing because often information discarded and this information important the solution the problem then the overall accuracy the system can suffer applications which the training data comprises examples the input vectors along with their corresponding target vectors are known supervised learning problems cases such the digit recognition example which the aim assign each input vector one finite number discrete categories are called classification problems the desired output consists one more continuous variables then the task called regression example regression problem would the prediction the yield chemical manufacturing process which the inputs consist the concentrations reactants the temperature and the pressure other pattern recognition problems the training data consists set input vectors without any corresponding target values the goal such unsupervised learning problems may discover groups similar examples within the data where called clustering determine the distribution data within the input space known density estimation project the data from high dimensional space down two three dimensions for the purpose visualization finally the technique reinforcement learning sutton and barto concerned with the problem finding suitable actions take given situation order maximize reward here the learning algorithm not given examples optimal outputs contrast supervised learning but must instead discover them process trial and error typically there sequence states and actions which the learning algorithm interacting with its environment many cases the current action not only affects the immediate reward but also has impact the reward all subsequent time steps for example using appropriate reinforcement learning techniques neural network can learn play the game backgammon high standard tesauro here the network must learn take board position input along with the result dice throw and produce strong move the output this done having the network play against copy itself for perhaps million games major challenge that game backgammon can involve dozens moves and yet only the end the game that the reward the form victory achieved the reward must then attributed appropriately all the moves that led even though some moves will have been good ones and others less this example credit assignment problem general feature reinforcement learning the trade off between exploration which the system tries out new kinds actions see how effective they are and exploitation which the system makes use actions that are known yield high reward too strong focus either exploration exploitation will yield poor results reinforcement learning continues active area machine learning research however introduction figure plot training data set points shown blue circles each comprising observation the input variable along with the corresponding target variable the green curve shows the function sin used generate the data our goal predict the value for some new value without knowledge the green curve detailed treatment lies beyond the scope this book although each these tasks needs its own tools and techniques many the key ideas that underpin them are common all such problems one the main goals this chapter introduce relatively informal way several the most important these concepts and illustrate them using simple examples later the book shall see these same ideas emerge the context more sophisticated models that are applicable real world pattern recognition applications this chapter also provides self contained introduction three important tools that will used throughout the book namely probability theory decision theory and information theory although these might sound like daunting topics they are fact straightforward and clear understanding them essential machine learning techniques are used best effect practical applications"}, "1": {"Section": "1.1", "Title": "Example: Polynomial Curve Fitting", "Content": "begin introducing simple regression problem which shall use running example throughout this chapter motivate number key concepts suppose observe real valued input variable and wish use this observation predict the value real valued target variable for the present purposes instructive consider artificial example using synthetically generated data because then know the precise process that generated the data for comparison against any learned model the data for this example generated from the function sin with random noise included the target values described detail appendix now suppose that are given training set comprising observations written together with corresponding observations the values denoted figure shows plot training set comprising data points the input data set figure was generated choosing values for spaced uniformly range and the target data set was obtained first computing the corresponding values the function example polynomial curve fitting sin and then adding small level random noise having gaussian distribution the gaussian distribution discussed section each such point order obtain the corresponding value generating data this way are capturing property many real data sets namely that they possess underlying regularity which wish learn but that individual observations are corrupted random noise this noise might arise from intrinsically stochastic random processes such radioactive decay but more typically due there being sources variability that are themselves unobserved our goal exploit this training set order make predictions the value the target variable for some new value the input variable shall see later this involves implicitly trying discover the underlying function sin this intrinsically difficult problem have generalize from finite data set furthermore the observed data are corrupted with noise and for given probability theory discussed there uncertainty the appropriate value for section provides framework for expressing such uncertainty precise and quantitative manner and decision theory discussed section allows exploit this probabilistic representation order make predictions that are optimal according appropriate criteria for the moment however shall proceed rather informally and consider simple approach based curve fitting particular shall fit the data using polynomial function the form wjxj where the order the polynomial and denotes raised the power the polynomial coefficients are collectively denoted the vector note that although the polynomial function nonlinear function linear function the coefficients functions such the polynomial which are linear the unknown parameters have important properties and are called linear models and will discussed extensively chapters and the values the coefficients will determined fitting the polynomial the training data this can done minimizing error function that measures the misfit between the function for any given value and the training set data points one simple choice error function which widely used given the sum the squares the errors between the predictions for each data point and the corresponding target values that minimize where the factor included for later convenience shall discuss the motivation for this choice error function later this chapter for the moment simply note that nonnegative quantity that would zero and only the introduction figure the error function corresponds one half the sum the squares the displacements shown the vertical green bars each data point from the function function were pass exactly through each training data point the geometrical interpretation the sum squares error function illustrated figure can solve the curve fitting problem choosing the value for which small possible because the error function quadratic function the coefficients its derivatives with respect the coefficients will linear the elements and the minimization the error function has unique solution denoted which can found closed form the resulting polynomial given the function exercise there remains the problem choosing the order the polynomial and shall see this will turn out example important concept called model comparison model selection figure show four examples the results fitting polynomials having orders and the data set shown figure notice that the constant and first order polynomials give rather poor fits the data and consequently rather poor representations the function sin the third order polynomial seems give the best fit the function sin the examples shown figure when much higher order polynomial obtain excellent fit the training data fact the polynomial passes exactly through each data point and however the fitted curve oscillates wildly and gives very poor representation the function sin this latter behaviour known over fitting have noted earlier the goal achieve good generalization making accurate predictions for new data can obtain some quantitative insight into the dependence the generalization performance considering separate test set comprising data points generated using exactly the same procedure used generate the training set points but with new choices for the random noise values included the target values for each choice can then evaluate the residual value given for the training data and can also evaluate for the test data set sometimes more convenient use the root mean square figure plots polynomials having various orders shown red curves fitted the data set shown figure rms error defined erms which the division allows compare different sizes data sets equal footing and the square root ensures that erms measured the same scale and the same units the target variable graphs the training and test set rms errors are shown for various values figure the test set error measure how well are doing predicting the values for new data observations note from figure that small values give relatively large values the test set error and this can attributed the fact that the corresponding polynomials are rather inflexible and are incapable capturing the oscillations the function sin values the range give small values for the test set error and these also give reasonable representations the generating function sin can seen for the case from figure example polynomial curve fitting introduction figure graphs the root mean square error defined evaluated the training set and independent test set for various values training test for the training set error goes zero might expect because this polynomial contains degrees freedom corresponding the coefficients and can tuned exactly the data points the training set however the test set error has become very large and saw figure the corresponding function exhibits wild oscillations this may seem paradoxical because polynomial given order contains all lower order polynomials special cases the polynomial therefore capable generating results least good the polynomial furthermore might suppose that the best predictor new data would the function sin from which the data was generated and shall see later that this indeed the case know that power series expansion the function sin contains terms all orders might expect that results should improve monotonically increase can gain some insight into the problem examining the values the coefficients obtained from polynomials various order shown table see that increases the magnitude the coefficients typically gets larger particular for the polynomial the coefficients have become finely tuned the data developing large positive and negative values that the correspondtable table the coefficients for polynomials various order observe how the typical magnitude the coefficients increases dramatically the order the polynomial increases example polynomial curve fitting figure plots the solutions obtained minimizing the sum squares error function using the polynomial for data points left plot and data points right plot see that increasing the size the data set reduces the over fitting problem ing polynomial function matches each the data points exactly but between data points particularly near the ends the range the function exhibits the large oscillations observed figure intuitively what happening that the more flexible polynomials with larger values are becoming increasingly tuned the random noise the target values also interesting examine the behaviour given model the size the data set varied shown figure see that for given model complexity the over fitting problem become less severe the size the data set increases another way say this that the larger the data set the more complex other words more flexible the model that can afford fit the data one rough heuristic that sometimes advocated that the number data points should less than some multiple say the number adaptive parameters the model however shall see chapter the number parameters not necessarily the most appropriate measure model complexity also there something rather unsatisfying about having limit the number parameters model according the size the available training set would seem more reasonable choose the complexity the model according the complexity the problem being solved shall see that the least squares approach finding the model parameters represents specific case maximum likelihood discussed section and that the over fitting problem can understood general property maximum likelihood adopting bayesian approach the over fitting problem can avoided shall see that there difficulty from bayesian perspective employing models for which the number parameters greatly exceeds the number data points indeed bayesian model the effective number parameters adapts automatically the size the data set for the moment however instructive continue with the current approach and consider how practice can apply data sets limited size where section may wish use relatively complex and flexible models one technique that often used control the over fitting phenomenon such cases that regularization which involves adding penalty term the error function order discourage the coefficients from reaching large values the simplest such penalty term takes the form sum squares all the coefficients leading modified error function the form where wtw and the coefficient governs the relative importance the regularization term compared with the sum squares error term note that often the coefficient omitted from the regularizer because its inclusion causes the results depend the choice origin for the target variable hastie may included but with its own regularization coefficient shall discuss this topic more detail section again the error function can minimized exactly closed form techniques such this are known the statistics literature shrinkage methods because they reduce the value the coefficients the particular case quadratic regularizer called ridge regression hoerl and kennard the context neural networks this approach known weight decay figure shows the results fitting the polynomial order the same data set before but now using the regularized error function given see that for value the over fitting has been suppressed and now obtain much closer representation the underlying function sin however use too large value for then again obtain poor fit shown figure for the corresponding coefficients from the fitted polynomials are given table showing that regularization has the desired effect reducing exercise introduction figure plots polynomials fitted the data set shown figure using the regularized error function for two values the regularization parameter corresponding and the case regularizer corresponding shown the bottom right figure example polynomial curve fitting table table the coefficients for polynomials with various values for the regularization parameter note that corresponds model with regularization the graph the bottom right figure see that the value increases the typical magnitude the coefficients gets smaller the magnitude the coefficients the impact the regularization term the generalization error can seen plotting the value the rms error for both training and test sets against shown figure see that effect now controls the effective complexity the model and hence determines the degree over fitting the issue model complexity important one and will discussed length section here simply note that were trying solve practical application using this approach minimizing error function would have find way determine suitable value for the model complexity the results above suggest simple way achieving this namely taking the available data and partitioning into training set used determine the coefficients and separate validation set also called hold out set used optimize the model complexity either many cases however this will prove too wasteful valuable training data and have seek more sophisticated approaches far our discussion polynomial curve fitting has appealed largely intuition now seek more principled approach solving problems pattern recognition turning discussion probability theory well providing the foundation for nearly all the subsequent developments this book will also section figure graph the root mean square error versus for the polynomial training test introduction give some important insights into the concepts have introduced the context polynomial curve fitting and will allow extend these more complex situations"}, "2": {"Section": "1.2", "Title": "Probability Theory", "Content": "key concept the field pattern recognition that uncertainty arises both through noise measurements well through the finite size data sets probability theory provides consistent framework for the quantification and manipulation uncertainty and forms one the central foundations for pattern recognition when combined with decision theory discussed section allows make optimal predictions given all the information available even though that information may incomplete ambiguous will introduce the basic concepts probability theory considering simple example imagine have two boxes one red and one blue and the red box have apples and oranges and the blue box have apples and orange this illustrated figure now suppose randomly pick one the boxes and from that box randomly select item fruit and having observed which sort fruit replace the box from which came could imagine repeating this process many times let suppose that doing pick the red box the time and pick the blue box the time and that when remove item fruit from box are equally likely select any the pieces fruit the box this example the identity the box that will chosen random variable which shall denote this random variable can take one two possible values namely corresponding the red box corresponding the blue box similarly the identity the fruit also random variable and will denoted can take either the values for apple for orange begin with shall define the probability event the fraction times that event occurs out the total number trials the limit that the total number trials goes infinity thus the probability selecting the red box figure use simple example two coloured boxes each containing fruit apples shown green and oranges shown orange introduce the basic ideas probability figure can derive the sum and product rules probability considering two random variables which takes the values where and which takes the values where this illustration have and consider total number instances these variables then denote the number instances where and nij which the number points the corresponding cell the array the number points column corresponding denoted and the number points row corresponding denoted probability theory nij and the probability selecting the blue box write these probabilities and note that definition probabilities must lie the interval also the events are mutually exclusive and they include all possible outcomes for instance this example the box must either red blue then see that the probabilities for those events must sum one can now ask questions such what the overall probability that the selection procedure will pick apple given that have chosen orange what the probability that the box chose was the blue one can answer questions such these and indeed much more complex questions associated with problems pattern recognition once have equipped ourselves with the two elementary rules probability known the sum rule and the product rule having obtained these rules shall then return our boxes fruit example order derive the rules probability consider the slightly more general example shown figure involving two random variables and which could for instance the box and fruit variables considered above shall suppose that can take any the values where and can take the values where consider total trials which sample both the variables and and let the number such trials which and nij also let the number trials which takes the value irrespective the value that takes denoted and similarly let the number trials which takes the value denoted the probability that will take the value and will take the value written and called the joint probability and given the number points falling the cell fraction the total number points and hence here are implicitly considering the limit similarly the probability that takes the value irrespective the value written and given the fraction the total number points that fall column that nij because the number instances column figure just the sum the number instances each cell that column have nij and therefore introduction from and have which the sum rule probability note that sometimes called the marginal probability because obtained marginalizing summing out the other variables this case consider only those instances for which then the fraction such instances for which written and called the conditional probability given obtained finding the fraction those points column that fall cell and hence given nij from and can then derive the following relationship nij nij which the product rule probability far have been quite careful make distinction between random variable such the box the fruit example and the values that the random variable can take for example the box were the red one thus the probability that takes the value denoted although this helps avoid ambiguity leads rather cumbersome notation and many cases there will need for such pedantry instead may simply write denote distribution over the random variable denote the distribution evaluated for the particular value provided that the interpretation clear from the context with this more compact notation can write the two fundamental rules probability theory the following form the rules probability sum rule product rule here joint probability and verbalized the probability and similarly the quantity conditional probability and verbalized the probability given whereas the quantity marginal probability and simply the probability these two simple rules form the basis for all the probabilistic machinery that use throughout this book from the product rule together with the symmetry property immediately obtain the following relationship between conditional probabilities probability theory which called bayes theorem and which plays central role pattern recognition and machine learning using the sum rule the denominator bayes theorem can expressed terms the quantities appearing the numerator can view the denominator bayes theorem being the normalization constant required ensure that the sum the conditional probability the left hand side over all values equals one figure show simple example involving joint distribution over two variables illustrate the concept marginal and conditional distributions here finite sample data points has been drawn from the joint distribution and shown the top left the top right histogram the fractions data points having each the two values from the definition probability these fractions would equal the corresponding probabilities the limit can view the histogram simple way model probability distribution given only finite number points drawn from that distribution modelling distributions from data lies the heart statistical pattern recognition and will explored great detail this book the remaining two plots figure show the corresponding histogram estimates and let now return our example involving boxes fruit for the moment shall once again explicit about distinguishing between the random variables and their instantiations have seen that the probabilities selecting either the red the blue boxes are given respectively note that these satisfy now suppose that pick box random and turns out the blue box then the probability selecting apple just the fraction apples the blue box which and fact can write out all four conditional probabilities for the type fruit given the selected box introduction figure illustration distribution over two variables which takes possible values and which takes two possible values the top left figure shows sample points drawn from joint probability distribution over these variables the remaining figures show histogram estimates the marginal distributions and well the conditional distribution corresponding the bottom row the top left figure again note that these probabilities are normalized that and similarly can now use the sum and product rules probability evaluate the overall probability choosing apple from which follows using the sum rule that probability theory suppose instead are told that piece fruit has been selected and orange and would like know which box came from this requires that evaluate the probability distribution over boxes conditioned the identity the fruit whereas the probabilities give the probability distribution over the fruit conditioned the identity the box can solve the problem reversing the conditional probability using bayes theorem give from the sum rule then follows that can provide important interpretation bayes theorem follows had been asked which box had been chosen before being told the identity the selected item fruit then the most complete information have available provided the probability call this the prior probability because the probability available before observe the identity the fruit once are told that the fruit orange can then use bayes theorem compute the probability which shall call the posterior probability because the probability obtained after have observed note that this example the prior probability selecting the red box was that were more likely select the blue box than the red one however once have observed that the piece selected fruit orange find that the posterior probability the red box now that now more likely that the box selected was fact the red one this result accords with our intuition the proportion oranges much higher the red box than the blue box and the observation that the fruit was orange provides significant evidence favouring the red box fact the evidence sufficiently strong that outweighs the prior and makes more likely that the red box was chosen rather than the blue one finally note that the joint distribution two variables factorizes into the product the marginals that then and are said independent from the product rule see that and the conditional distribution given indeed independent the value for instance our boxes fruit example each box contained the same fraction apples and oranges then that the probability selecting say apple independent which box chosen"}, "3": {"Section": "1.2.1", "Title": "Probability densities", "Content": "well considering probabilities defined over discrete sets events also wish consider probabilities with respect continuous variables shall limit ourselves relatively informal discussion the probability real valued variable falling the interval given for then called the probability density over this illustrated figure the probability that will lie interval then given introduction figure the concept probability for discrete variables can extended that probability density over continuous variable and such that the probability lying the interval given for the probability density can expressed the derivative cumulative distribution function because probabilities are nonnegative and because the value must lie somewhere the real axis the probability density must satisfy the two conditions under nonlinear change variable probability density transforms differently from simple function due the jacobian factor for instance consider change variables then function becomes now consider probability density that corresponds density with respect the new variable where the suffices denote the fact that and are different densities observations falling the range will for small values transformed into the range where and hence one consequence this property that the concept the maximum probability density dependent the choice variable the probability that lies the interval given the cumulative distribution function defined which satisfies shown figure have several continuous variables denoted collectively the vector then can define joint probability density such exercise probability theory that the probability falling infinitesimal volume containing the point given this multivariate probability density must satisfy which the integral taken over the whole space can also consider joint probability distributions over combination discrete and continuous variables note that discrete variable then sometimes called probability mass function because can regarded set probability masses concentrated the allowed values the sum and product rules probability well bayes theorem apply equally the case probability densities combinations discrete and continuous variables for instance and are two real variables then the sum and product rules take the form formal justification the sum and product rules for continuous variables feller requires branch mathematics called measure theory and lies outside the scope this book its validity can seen informally however dividing each real variable into intervals width and considering the discrete probability distribution over these intervals taking the limit then turns sums into integrals and gives the desired result"}, "4": {"Section": "1.2.2", "Title": "Expectations and covariances", "Content": "one the most important operations involving probabilities that finding weighted averages functions the average value some function under probability distribution called the expectation and will denoted for discrete distribution given that the average weighted the relative probabilities the different values the case continuous variables expectations are expressed terms integration with respect the corresponding probability density either case are given finite number points drawn from the probability distribution probability density then the expectation can approximated introduction finite sum over these points shall make extensive use this result when discuss sampling methods chapter the approximation becomes exact the limit sometimes will considering expectations functions several variables which case can use subscript indicate which variable being averaged over that for instance denotes the average the function with respect the distribution note that will function can also consider conditional expectation with respect conditional distribution that with analogous definition for continuous variables the variance defined var and provides measure how much variability there around its mean value expanding out the square see that the variance can also written terms the expectations and particular can consider the variance the variable itself which given var var for two random variables and the covariance defined cov which expresses the extent which and vary together and are independent then their covariance vanishes the case two vectors random variables and the covariance matrix cov xyt consider the covariance the components vector with each other then use slightly simpler notation cov cov exercise exercise probability theory"}, "5": {"Section": "1.2.3", "Title": "Bayesian probabilities", "Content": "far this chapter have viewed probabilities terms the frequencies random repeatable events shall refer this the classical frequentist interpretation probability now turn the more general bayesian view which probabilities provide quantification uncertainty consider uncertain event for example whether the moon was once its own orbit around the sun whether the arctic ice cap will have disappeared the end the century these are not events that can repeated numerous times order define notion probability did earlier the context boxes fruit nevertheless will generally have some idea for example how quickly think the polar ice melting now obtain fresh evidence for instance from new earth observation satellite gathering novel forms diagnostic information may revise our opinion the rate ice loss our assessment such matters will affect the actions take for instance the extent which endeavour reduce the emission greenhouse gasses such circumstances would like able quantify our expression uncertainty and make precise revisions uncertainty the light new evidence well subsequently able take optimal actions decisions consequence this can all achieved through the elegant and very general bayesian interpretation probability the use probability represent uncertainty however not hoc choice but inevitable are respect common sense while making rational coherent inferences for instance cox showed that numerical values are used represent degrees belief then simple set axioms encoding common sense properties such beliefs leads uniquely set rules for manipulating degrees belief that are equivalent the sum and product rules probability this provided the first rigorous proof that probability theory could regarded extension boolean logic situations involving uncertainty jaynes numerous other authors have proposed different sets properties axioms that such measures uncertainty should satisfy ramsey good savage definetti lindley each case the resulting numerical quantities behave precisely according the rules probability therefore natural refer these quantities bayesian probabilities the field pattern recognition too helpful have more general nothomas bayes thomas bayes was born tunbridge wells and was clergyman well amateur scientist and mathematician studied logic and theology edinburgh university and was elected fellow the royal society during the century issues regarding probability arose connection with gambling and with the new concept insurance one particularly important problem concerned called inverse probability solution was proposed thomas bayes his paper essay towards solving problem the doctrine chances which was published some three years after his death the philosophical transactions the royal society fact bayes only formulated his theory for the case uniform prior and was pierre simon laplace who independently rediscovered the theory general form and who demonstrated its broad applicability introduction tion probability consider the example polynomial curve fitting discussed section seems reasonable apply the frequentist notion probability the random values the observed variables however would like address and quantify the uncertainty that surrounds the appropriate choice for the model parameters shall see that from bayesian perspective can use the machinery probability theory describe the uncertainty model parameters such indeed the choice model itself bayes theorem now acquires new significance recall that the boxes fruit example the observation the identity the fruit provided relevant information that altered the probability that the chosen box was the red one that example bayes theorem was used convert prior probability into posterior probability incorporating the evidence provided the observed data shall see detail later can adopt similar approach when making inferences about quantities such the parameters the polynomial curve fitting example capture our assumptions about before observing the data the form prior probability distribution the effect the observed data expressed through the conditional probability and shall see later section how this can represented explicitly bayes theorem which takes the form then allows evaluate the uncertainty after have observed the form the posterior probability the quantity the right hand side bayes theorem evaluated for the observed data set and can viewed function the parameter vector which case called the likelihood function expresses how probable the observed data set for different settings the parameter vector note that the likelihood not probability distribution over and its integral with respect does not necessarily equal one given this definition likelihood can state bayes theorem words posterior likelihood prior where all these quantities are viewed functions the denominator the normalization constant which ensures that the posterior distribution the left hand side valid probability density and integrates one indeed integrating both sides with respect can express the denominator bayes theorem terms the prior distribution and the likelihood function both the bayesian and frequentist paradigms the likelihood function plays central role however the manner which used fundamentally different the two approaches frequentist setting considered fixed parameter whose value determined some form estimator and error bars probability theory this estimate are obtained considering the distribution possible data sets contrast from the bayesian viewpoint there only single data set namely the one that actually observed and the uncertainty the parameters expressed through probability distribution over widely used frequentist estimator maximum likelihood which set the value that maximizes the likelihood function this corresponds choosing the value for which the probability the observed data set maximized the machine learning literature the negative log the likelihood function called error function because the negative logarithm monotonically decreasing function maximizing the likelihood equivalent minimizing the error one approach determining frequentist error bars the bootstrap efron hastie which multiple data sets are created follows suppose our original data set consists data points can create new data set drawing points random from with replacement that some points may replicated whereas other points may absent from this process can repeated times generate data sets each size and each obtained sampling from the original data set the statistical accuracy parameter estimates can then evaluated looking the variability predictions between the different bootstrap data sets one advantage the bayesian viewpoint that the inclusion prior knowledge arises naturally suppose for instance that fair looking coin tossed three times and lands heads each time classical maximum likelihood estimate the probability landing heads would give implying that all future tosses will land heads contrast bayesian approach with any reasonable prior will lead much less extreme conclusion there has been much controversy and debate associated with the relative merits the frequentist and bayesian paradigms which have not been helped the fact that there unique frequentist even bayesian viewpoint for instance one common criticism the bayesian approach that the prior distribution often selected the basis mathematical convenience rather than reflection any prior beliefs even the subjective nature the conclusions through their dependence the choice prior seen some source difficulty reducing the dependence the prior one motivation for called noninformative priors however these lead difficulties when comparing different models and indeed bayesian methods based poor choices prior can give poor results with high confidence frequentist evaluation methods offer some protection from such problems and techniques such cross validation remain useful areas such model comparison this book places strong emphasis the bayesian viewpoint reflecting the huge growth the practical importance bayesian methods the past few years while also discussing useful frequentist concepts required although the bayesian framework has its origins the century the practical application bayesian methods was for long time severely limited the difficulties carrying through the full bayesian procedure particularly the need marginalize sum integrate over the whole parameter space which shall section section section introduction see required order make predictions compare different models the development sampling methods such markov chain monte carlo discussed chapter along with dramatic improvements the speed and memory capacity computers opened the door the practical use bayesian techniques impressive range problem domains monte carlo methods are very flexible and can applied wide range models however they are computationally intensive and have mainly been used for small scale problems more recently highly efficient deterministic approximation schemes such variational bayes and expectation propagation discussed chapter have been developed these offer complementary alternative sampling methods and have allowed bayesian techniques used large scale applications blei"}, "6": {"Section": "1.2.4", "Title": "The Gaussian distribution", "Content": "shall devote the whole chapter study various probability distributions and their key properties convenient however introduce here one the most important probability distributions for continuous variables called the normal gaussian distribution shall make extensive use this distribution the remainder this chapter and indeed throughout much the book for the case single real valued variable the gaussian distribution defined exp which governed two parameters called the mean and called the variance the square root the variance given called the standard deviation and the reciprocal the variance written called the precision shall see the motivation for these terms shortly figure shows plot the gaussian distribution from the form see that the gaussian distribution satisfies exercise also straightforward show that the gaussian normalized that pierre simon laplace said that laplace was seriously lacking modesty and one point declared himself the best mathematician france the time claim that was arguably true well being prolific mathematics also made numerous contributions astronomy including the nebular hypothesis which the earth thought have formed from the condensation and cooling large rotating disk gas and dust published the first edition eorie analytique des probabilit which laplace states that probability theory nothing but common sense reduced calculation this work included discussion the inverse probability calculation later termed bayes theorem poincar which used solve problems life expectancy jurisprudence planetary masses triangulation and error estimation figure plot the univariate gaussian showing the mean and the standard deviation probability theory exercise exercise thus satisfies the two requirements for valid probability density can readily find expectations functions under the gaussian distribution particular the average value given because the parameter represents the average value under the distribution referred the mean similarly for the second order moment from and follows that the variance given var and hence referred the variance parameter the maximum distribution known its mode for gaussian the mode coincides with the mean are also interested the gaussian distribution defined over dimensional vector continuous variables which given exp where the dimensional vector called the mean the matrix called the covariance and denotes the determinant shall make use the multivariate gaussian distribution briefly this chapter although its properties will studied detail section introduction figure illustration the likelihood function for gaussian distribution shown the red curve here the black points denote data set values and the likelihood function given corresponds the product the blue values maximizing the likelihood involves adjusting the mean and variance the gaussian maximize this product now suppose that have data set observations representing observations the scalar variable note that are using the typeface distinguish this from single observation the vector valued variable which denote shall suppose that the observations are drawn independently from gaussian distribution whose mean and variance are unknown and would like determine these parameters from the data set data points that are drawn independently from the same distribution are said independent and identically distributed which often abbreviated have seen that the joint probability two independent events given the product the marginal probabilities for each event separately because our data set can therefore write the probability the data set given and the form section when viewed function and this the likelihood function for the gaussian and interpreted diagrammatically figure one common criterion for determining the parameters probability distribution using observed data set find the parameter values that maximize the likelihood function this might seem like strange criterion because from our foregoing discussion probability theory would seem more natural maximize the probability the parameters given the data not the probability the data given the parameters fact these two criteria are related shall discuss the context curve fitting for the moment however shall determine values for the unknown parameters and the gaussian maximizing the likelihood function practice more convenient maximize the log the likelihood function because the logarithm monotonically increasing function its argument maximization the log function equivalent maximization the function itself taking the log not only simplifies the subsequent mathematical analysis but also helps numerically because the product large number small probabilities can easily underflow the numerical precision the computer and this resolved computing instead the sum the log probabilities from and the log likelihood exercise section exercise probability theory function can written the form maximizing with respect obtain the maximum likelihood solution given which the sample mean the mean the observed values similarly maximizing with respect obtain the maximum likelihood solution for the variance the form which the sample variance measured with respect the sample mean note that are performing joint maximization with respect and but the case the gaussian distribution the solution for decouples from that for that can first evaluate and then subsequently use this result evaluate later this chapter and also subsequent chapters shall highlight the significant limitations the maximum likelihood approach here give indication the problem the context our solutions for the maximum likelihood parameter settings for the univariate gaussian distribution particular shall show that the maximum likelihood approach systematically underestimates the variance the distribution this example phenomenon called bias and related the problem over fitting encountered the context polynomial curve fitting first note that the maximum likelihood solutions and are functions the data set values consider the expectations these quantities with respect the data set values which themselves come from gaussian distribution with parameters and straightforward show that that average the maximum likelihood estimate will obtain the correct mean but will underestimate the true variance factor the intuition behind this result given figure from follows that the following estimate for the variance parameter unbiased introduction figure illustration how bias arises using maximum likelihood determine the variance gaussian the green curve shows the true gaussian distribution from which data generated and the three red curves show the gaussian distributions obtained fitting three data sets each consisting two data points shown blue using the maximum likelihood results and averaged across the three data sets the mean correct but the variance systematically under estimated because measured relative the sample mean and not relative the true mean section section shall see how this result arises automatically when adopt bayesian approach note that the bias the maximum likelihood solution becomes less significant the number data points increases and the limit the maximum likelihood solution for the variance equals the true variance the distribution that generated the data practice for anything other than small this bias will not prove serious problem however throughout this book shall interested more complex models with many parameters for which the bias problems associated with maximum likelihood will much more severe fact shall see the issue bias maximum likelihood lies the root the over fitting problem that encountered earlier the context polynomial curve fitting"}, "7": {"Section": "1.2.5", "Title": "Curve fitting re-visited", "Content": "have seen how the problem polynomial curve fitting can expressed terms error minimization here return the curve fitting example and view from probabilistic perspective thereby gaining some insights into error functions and regularization well taking towards full bayesian treatment the goal the curve fitting problem able make predictions for the target variable given some new value the input variable the basis set training data comprising input values and their corresponding target values can express our uncertainty over the value the target variable using probability distribution for this purpose shall assume that given the value the corresponding value has gaussian distribution with mean equal the value the polynomial curve given thus have where for consistency with the notation later chapters have defined precision parameter corresponding the inverse variance the distribution this illustrated schematically figure figure schematic illustration gaussian conditional distribution for given given which the mean given the polynomial function and the precision given the parameter which related the variance probability theory now use the training data determine the values the unknown parameters and maximum likelihood the data are assumed drawn independently from the distribution then the likelihood function given did the case the simple gaussian distribution earlier convenient maximize the logarithm the likelihood function substituting for the form the gaussian distribution given obtain the log likelihood function the form consider first the determination the maximum likelihood solution for the polynomial coefficients which will denoted wml these are determined maximizing with respect for this purpose can omit the last two terms the right hand side because they not depend also note that scaling the log likelihood positive constant coefficient does not alter the location the maximum with respect and can replace the coefficient with finally instead maximizing the log likelihood can equivalently minimize the negative log likelihood therefore see that maximizing likelihood equivalent far determining concerned minimizing the sum squares error function defined thus the sum squares error function has arisen consequence maximizing likelihood under the assumption gaussian noise distribution can also use maximum likelihood determine the precision parameter the gaussian conditional distribution maximizing with respect gives wml introduction section again can first determine the parameter vector wml governing the mean and subsequently use this find the precision was the case for the simple gaussian distribution having determined the parameters and can now make predictions for new values because now have probabilistic model these are expressed terms the predictive distribution that gives the probability distribution over rather than simply point estimate and obtained substituting the maximum likelihood parameters into give wml wml now let take step towards more bayesian approach and introduce prior distribution over the polynomial coefficients for simplicity let consider gaussian distribution the form exp wtw where the precision the distribution and the total number elements the vector for order polynomial variables such which control the distribution model parameters are called hyperparameters using bayes theorem the posterior distribution for proportional the product the prior distribution and the likelihood function can now determine finding the most probable value given the data other words maximizing the posterior distribution this technique called maximum posterior simply map taking the negative logarithm and combining with and find that the maximum the posterior given the minimum wtw thus see that maximizing the posterior distribution equivalent minimizing the regularized sum squares error function encountered earlier the form with regularization parameter given"}, "8": {"Section": "1.2.6", "Title": "Bayesian curve fitting", "Content": "although have included prior distribution are far still making point estimate and this does not yet amount bayesian treatment fully bayesian approach should consistently apply the sum and product rules probability which requires shall see shortly that integrate over all values such marginalizations lie the heart bayesian methods for pattern recognition probability theory the curve fitting problem are given the training data and along with new test point and our goal predict the value therefore wish evaluate the predictive distribution here shall assume that the parameters and are fixed and known advance later chapters shall discuss how such parameters can inferred from data bayesian setting bayesian treatment simply corresponds consistent application the sum and product rules probability which allow the predictive distribution written the form here given and have omitted the dependence and simplify the notation here the posterior distribution over parameters and can found normalizing the right hand side shall see section that for problems such the curve fitting example this posterior distribution gaussian and can evaluated analytically similarly the integration can also performed analytically with the result that the predictive distribution given gaussian the form where the mean and variance are given here the matrix given where the unit matrix and have defined the vector with elements for see that the variance well the mean the predictive distribution dependent the first term represents the uncertainty the predicted value due the noise the target variables and was expressed already the maximum likelihood predictive distribution through however the second term arises from the uncertainty the parameters and consequence the bayesian treatment the predictive distribution for the synthetic sinusoidal regression problem illustrated figure introduction figure the predictive distribution resulting from bayesian treatment polynomial curve fitting using polynomial with the fixed parameters and corresponding the known noise variance which the red curve denotes the mean the predictive distribution and the red region corresponds standard deviation around the mean"}, "9": {"Section": "1.3", "Title": "Model Selection", "Content": "our example polynomial curve fitting using least squares saw that there was optimal order polynomial that gave the best generalization the order the polynomial controls the number free parameters the model and thereby governs the model complexity with regularized least squares the regularization coefficient also controls the effective complexity the model whereas for more complex models such mixture distributions neural networks there may multiple parameters governing complexity practical application need determine the values such parameters and the principal objective doing usually achieve the best predictive performance new data furthermore well finding the appropriate values for complexity parameters within given model may wish consider range different types model order find the best one for our particular application have already seen that the maximum likelihood approach the performance the training set not good indicator predictive performance unseen data due the problem over fitting data plentiful then one approach simply use some the available data train range models given model with range values for its complexity parameters and then compare them independent data sometimes called validation set and select the one having the best predictive performance the model design iterated many times using limited size data set then some over fitting the validation data can occur and may necessary keep aside third test set which the performance the selected model finally evaluated many applications however the supply data for training and testing will limited and order build good models wish use much the available data possible for training however the validation set small will give relatively noisy estimate predictive performance one solution this dilemma use cross validation which illustrated figure this allows proportion the available data used for training while making use all the"}, "10": {"Section": "1.4", "Title": "The Curse of Dimensionality", "Content": "figure the technique fold cross validation illustrated here for the case involves taking the available data and partitioning into groups the simplest case these are equal size then the groups are used train set models that are then evaluated the remaining group this procedure then repeated for all possible choices for the held out group indicated here the red blocks and the performance scores from the runs are then averaged run run run run data assess performance when data particularly scarce may appropriate consider the case where the total number data points which gives the leave one out technique one major drawback cross validation that the number training runs that must performed increased factor and this can prove problematic for models which the training itself computationally expensive further problem with techniques such cross validation that use separate data assess performance that might have multiple complexity parameters for single model for instance there might several regularization parameters exploring combinations settings for such parameters could the worst case require number training runs that exponential the number parameters clearly need better approach ideally this should rely only the training data and should allow multiple hyperparameters and model types compared single training run therefore need find measure performance which depends only the training data and which does not suffer from bias due over fitting historically various information criteria have been proposed that attempt correct for the bias maximum likelihood the addition penalty term compensate for the over fitting more complex models for example the akaike information criterion aic akaike chooses the model for which the quantity wml largest here wml the best fit log likelihood and the number adjustable parameters the model variant this quantity called the bayesian information criterion bic will discussed section such criteria not take account the uncertainty the model parameters however and practice they tend favour overly simple models therefore turn section fully bayesian approach where shall see how complexity penalties arise natural and principled way the curse dimensionality the polynomial curve fitting example had just one input variable for practical applications pattern recognition however will have deal with spaces introduction figure scatter plot the oil flow data for input variables and which red denotes the homogenous class green denotes the annular class and blue denotes the laminar class our goal classify the new test point denoted high dimensionality comprising many input variables now discuss this poses some serious challenges and important factor influencing the design pattern recognition techniques order illustrate the problem consider synthetically generated data set representing measurements taken from pipeline containing mixture oil water and gas bishop and james these three materials can present one three different geometrical configurations known homogenous annular and laminar and the fractions the three materials can also vary each data point comprises dimensional input vector consisting measurements taken with gamma ray densitometers that measure the attenuation gamma rays passing along narrow beams through the pipe this data set described detail appendix figure shows points from this data set plot showing two the measurements and the remaining ten input values are ignored for the purposes this illustration each data point labelled according which the three geometrical classes belongs and our goal use this data training set order able classify new observation such the one denoted the cross figure observe that the cross surrounded numerous red points and might suppose that belongs the red class however there are also plenty green points nearby might think that could instead belong the green class seems unlikely that belongs the blue class the intuition here that the identity the cross should determined more strongly nearby points from the training set and less strongly more distant points fact this intuition turns out reasonable and will discussed more fully later chapters how can turn this intuition into learning algorithm one very simple approach would divide the input space into regular cells indicated figure when are given test point and wish predict its class first decide which cell belongs and then find all the training data points that the curse dimensionality figure illustration simple approach the solution classification problem which the input space divided into cells and any new test point assigned the class that has majority number representatives the same cell the test point shall see shortly this simplistic approach has some severe shortcomings fall the same cell the identity the test point predicted being the same the class having the largest number training points the same cell the test point with ties being broken random there are numerous problems with this naive approach but one the most severe becomes apparent when consider its extension problems having larger numbers input variables corresponding input spaces higher dimensionality the origin the problem illustrated figure which shows that divide region space into regular cells then the number such cells grows exponentially with the dimensionality the space the problem with exponentially large number cells that would need exponentially large quantity training data order ensure that the cells are not empty clearly have hope applying such technique space more than few variables and need find more sophisticated approach can gain further insight into the problems high dimensional spaces returning the example polynomial curve fitting and considering how would section figure illustration the curse dimensionality showing how the number regions regular grid grows exponentially with the dimensionality the space for clarity only subset the cubical regions are shown for introduction extend this approach deal with input spaces having several variables have input variables then general polynomial with coefficients order would take the form exercise wixi wijxixj wijkxixjxk increases the number independent coefficients not all the coefficients are independent due interchange symmetries amongst the variables grows proportionally practice capture complex dependencies the data may need use higher order polynomial for polynomial order the growth the number coefficients like although this now power law growth rather than exponential growth still points the method becoming rapidly unwieldy and limited practical utility our geometrical intuitions formed through life spent space three dimensions can fail badly when consider spaces higher dimensionality simple example consider sphere radius space dimensions and ask what the fraction the volume the sphere that lies between radius and can evaluate this fraction noting that the volume sphere radius dimensions must scale and write kdrd exercise where the constant depends only thus the required fraction given exercise which plotted function for various values figure see that for large this fraction tends even for small values thus spaces high dimensionality most the volume sphere concentrated thin shell near the surface further example direct relevance pattern recognition consider the behaviour gaussian distribution high dimensional space transform from cartesian polar coordinates and then integrate out the directional variables obtain expression for the density function radius from the origin thus the probability mass inside thin shell thickness located radius this distribution plotted for various values figure and see that for large the probability mass the gaussian concentrated thin shell the severe difficulty that can arise spaces many dimensions sometimes called the curse dimensionality bellman this book shall make extensive use illustrative examples involving input spaces one two dimensions because this makes particularly easy illustrate the techniques graphically the reader should warned however that not all intuitions developed spaces low dimensionality will generalize spaces many dimensions the curse dimensionality figure plot the fraction the volume sphere lying the range for various values the dimensionality although the curse dimensionality certainly raises important issues for pattern recognition applications does not prevent from finding effective techniques applicable high dimensional spaces the reasons for this are twofold first real data will often confined region the space having lower effective dimensionality and particular the directions over which important variations the target variables occur may confined second real data will typically exhibit some smoothness properties least locally that for the most part small changes the input variables will produce small changes the target variables and can exploit local interpolation like techniques allow make predictions the target variables for new values the input variables successful pattern recognition techniques exploit one both these properties consider for example application manufacturing which images are captured identical planar objects conveyor belt which the goal determine their orientation each image point figure plot the probability density with respect radius gaussian distribution for various values high dimensional space most the probability mass gaussian located within thin shell specific radius the dimensionality introduction high dimensional space whose dimensionality determined the number pixels because the objects can occur different positions within the image and different orientations there are three degrees freedom variability between images and set images will live three dimensional manifold embedded within the high dimensional space due the complex relationships between the object position orientation and the pixel intensities this manifold will highly nonlinear the goal learn model that can take input image and output the orientation the object irrespective its position then there only one degree freedom variability within the manifold that significant"}, "11": {"Section": "1.5", "Title": "Decision Theory", "Content": "have seen section how probability theory provides with consistent mathematical framework for quantifying and manipulating uncertainty here turn discussion decision theory that when combined with probability theory allows make optimal decisions situations involving uncertainty such those encountered pattern recognition suppose have input vector together with corresponding vector target variables and our goal predict given new value for for regression problems will comprise continuous variables whereas for classification problems will represent class labels the joint probability distribution provides complete summary the uncertainty associated with these variables determination from set training data example inference and typically very difficult problem whose solution forms the subject much this book practical application however must often make specific prediction for the value more generally take specific action based our understanding the values likely take and this aspect the subject decision theory consider for example medical diagnosis problem which have taken ray image patient and wish determine whether the patient has cancer not this case the input vector the set pixel intensities the image and output variable will represent the presence cancer which denote the class the absence cancer which denote the class might for instance choose binary variable such that corresponds class and corresponds class shall see later that this choice label values particularly convenient for probabilistic models the general inference problem then involves determining the joint distribution equivalently which gives the most complete probabilistic description the situation although this can very useful and informative quantity the end must decide either give treatment the patient not and would like this choice optimal some appropriate sense duda and hart this the decision step and the subject decision theory tell how make optimal decisions given the appropriate probabilities shall see that the decision stage generally very simple even trivial once have solved the inference problem here give introduction the key ideas decision theory required for decision theory the rest the book further background well more detailed accounts can found berger and bather before giving more detailed analysis let first consider informally how might expect probabilities play role making decisions when obtain the ray image for new patient our goal decide which the two classes assign the image are interested the probabilities the two classes given the image which are given using bayes theorem these probabilities can expressed the form note that any the quantities appearing bayes theorem can obtained from the joint distribution either marginalizing conditioning with respect the appropriate variables can now interpret the prior probability for the class and the corresponding posterior probability thus represents the probability that person has cancer before take the ray measurement similarly the corresponding probability revised using bayes theorem light the information contained the ray our aim minimize the chance assigning the wrong class then intuitively would choose the class having the higher posterior probability now show that this intuition correct and also discuss more general criteria for making decisions"}, "12": {"Section": "1.5.1", "Title": "Minimizing the misclassification rate", "Content": "suppose that our goal simply make few misclassifications possible need rule that assigns each value one the available classes such rule will divide the input space into regions called decision regions one for each class such that all points are assigned class the boundaries between decision regions are called decision boundaries decision surfaces note that each decision region need not contiguous but could comprise some number disjoint regions shall encounter examples decision boundaries and decision regions later chapters order find the optimal decision rule consider first all the case two classes the cancer problem for instance mistake occurs when input vector belonging class assigned class vice versa the probability this occurring given mistake are free choose the decision rule that assigns each point one the two classes clearly minimize mistake should arrange that each assigned whichever class has the smaller value the integrand thus for given value then should assign that class from the product rule probability have because the factor common both terms can restate this result saying that the minimum introduction values figure schematic illustration the joint probabilities for each two classes plotted against together with the decision boundary are classified class and hence belong decision region whereas points are classified and belong errors arise from the blue green and red regions that for the errors are due points from class being misclassified represented the sum the red and green regions and conversely for points the region the errors are due points from class being misclassified represented the blue region vary the location the decision boundary the combined areas the blue and green regions remains constant whereas the size the red region varies the optimal choice for where the curves for and cross corresponding because this case the red region disappears this equivalent the minimum misclassification rate decision rule which assigns each value the class having the higher posterior probability probability making mistake obtained each value assigned the class for which the posterior probability largest this result illustrated for two classes and single input variable figure for the more general case classes slightly easier maximize the probability being correct which given correct which maximized when the regions are chosen such that each assigned the class for which largest again using the product rule and noting that the factor common all terms see that each should assigned the class having the largest posterior probability cancer figure example loss matrix with elements lkj for the cancer treatment problem the rows correspond the true class whereas the columns correspond the assignment class made our decision criterion cancer normal normal decision theory"}, "13": {"Section": "1.5.2", "Title": "Minimizing the expected loss", "Content": "for many applications our objective will more complex than simply minimizing the number misclassifications let consider again the medical diagnosis problem note that patient who does not have cancer incorrectly diagnosed having cancer the consequences may some patient distress plus the need for further investigations conversely patient with cancer diagnosed healthy the result may premature death due lack treatment thus the consequences these two types mistake can dramatically different would clearly better make fewer mistakes the second kind even this was the expense making more mistakes the first kind can formalize such issues through the introduction loss function also called cost function which single overall measure loss incurred taking any the available decisions actions our goal then minimize the total loss incurred note that some authors consider instead utility function whose value they aim maximize these are equivalent concepts take the utility simply the negative the loss and throughout this text shall use the loss function convention suppose that for new value the true class and that assign class where may may not equal doing incur some level loss that denote lkj which can view the element loss matrix for instance our cancer example might have loss matrix the form shown figure this particular loss matrix says that there loss incurred the correct decision made there loss healthy patient diagnosed having cancer whereas there loss patient having cancer diagnosed healthy the optimal solution the one which minimizes the loss function however the loss function depends the true class which unknown for given input vector our uncertainty the true class expressed through the joint probability distribution and seek instead minimize the average loss where the average computed with respect this distribution which given lkjp each can assigned independently one the decision regions our goal choose the regions order minimize the expected loss which lkjp before can use implies that for each should minimize the product rule eliminate the common factor thus the decision rule that minimizes the expected loss the one that assigns each new the class for which the quantity inputs such that the larger the two posterior probabilities less than equal some threshold will rejected introduction figure illustration the reject option reject region lkjp minimum this clearly trivial once know the posterior class probabilities"}, "14": {"Section": "1.5.3", "Title": "The reject option", "Content": "have seen that classification errors arise from the regions input space where the largest the posterior probabilities significantly less than unity equivalently where the joint distributions have comparable values these are the regions where are relatively uncertain about class membership some applications will appropriate avoid making decisions the difficult cases anticipation lower error rate those examples for which classification decision made this known the reject option for example our hypothetical medical illustration may appropriate use automatic system classify those ray images for which there little doubt the correct class while leaving human expert classify the more ambiguous cases can achieve this introducing threshold and rejecting those inputs for which the largest the posterior probabilities less than equal this illustrated for the case two classes and single continuous input variable figure note that setting will ensure that all examples are rejected whereas there are classes then setting will ensure that examples are rejected thus the fraction examples that get rejected controlled the value can easily extend the reject criterion minimize the expected loss when loss matrix given taking account the loss incurred when reject decision made"}, "15": {"Section": "1.5.4", "Title": "Inference and decision", "Content": "have broken the classification problem down into two separate stages the inference stage which use training data learn model for and the exercise find the posterior class probabilities usual the denominator bayes theorem can found terms the quantities appearing the numerator because equivalently can model the joint distribution directly and then normalize obtain the posterior probabilities having found the posterior probabilities use decision theory determine class membership for each new input approaches that explicitly implicitly model the distribution inputs well outputs are known generative models because sampling from them possible generate synthetic data points the input space first solve the inference problem determining the posterior class probabilities and then subsequently use decision theory assign each new one the classes approaches that model the posterior probabilities directly are called discriminative models find function called discriminant function which maps each input directly onto class label for instance the case two class problems might binary valued and such that represents class and represents class this case probabilities play role let consider the relative merits these three alternatives approach the most demanding because involves finding the joint distribution over both and for many applications will have high dimensionality and consequently may need large training set order able determine the class conditional densities reasonable accuracy note that the class priors can often estimated simply from the fractions the training set data points each the classes one advantage approach however that also allows the marginal density data determined from this can useful for detecting new data points that have low probability under the model and for which the predictions may decision theory subsequent decision stage which use these posterior probabilities make optimal class assignments alternative possibility would solve both problems together and simply learn function that maps inputs directly into decisions such function called discriminant function fact can identify three distinct approaches solving decision problems all which have been used practical applications these are given decreasing order complexity first solve the inference problem determining the class conditional densities for each class individually also separately infer the prior class probabilities then use bayes theorem the form introduction figure example the class conditional densities for two classes having single input variable left plot together with the corresponding posterior probabilities right plot note that the left hand mode the class conditional density shown blue the left plot has effect the posterior probabilities the vertical green line the right plot shows the decision boundary that gives the minimum misclassification rate low accuracy which known outlier detection novelty detection bishop tarassenko however only wish make classification decisions then can wasteful computational resources and excessively demanding data find the joint distribution when fact only really need the posterior probabilities which can obtained directly through approach indeed the classconditional densities may contain lot structure that has little effect the posterior probabilities illustrated figure there has been much interest exploring the relative merits generative and discriminative approaches machine learning and finding ways combine them jebara lasserre even simpler approach which use the training data find discriminant function that maps each directly onto class label thereby combining the inference and decision stages into single learning problem the example figure this would correspond finding the value shown the vertical green line because this the decision boundary giving the minimum probability misclassification with option however longer have access the posterior probabilities there are many powerful reasons for wanting compute the posterior probabilities even subsequently use them make decisions these include minimizing risk consider problem which the elements the loss matrix are subjected revision from time time such might occur financial decision theory application know the posterior probabilities can trivially revise the minimum risk decision criterion modifying appropriately have only discriminant function then any change the loss matrix would require that return the training data and solve the classification problem afresh reject option posterior probabilities allow determine rejection criterion that will minimize the misclassification rate more generally the expected loss for given fraction rejected data points compensating for class priors consider our medical ray problem again and suppose that have collected large number ray images from the general population for use training data order build automated screening system because cancer rare amongst the general population might find that say only every examples corresponds the presence cancer used such data set train adaptive model could run into severe difficulties due the small proportion the cancer class for instance classifier that assigned every point the normal class would already achieve accuracy and would difficult avoid this trivial solution also even large data set will contain very few examples ray images corresponding cancer and the learning algorithm will not exposed broad range examples such images and hence not likely generalize well balanced data set which have selected equal numbers examples from each the classes would allow find more accurate model however then have compensate for the effects our modifications the training data suppose have used such modified data set and found models for the posterior probabilities from bayes theorem see that the posterior probabilities are proportional the prior probabilities which can interpret the fractions points each class can therefore simply take the posterior probabilities obtained from our artificially balanced data set and first divide the class fractions that data set and then multiply the class fractions the population which wish apply the model finally need normalize ensure that the new posterior probabilities sum one note that this procedure cannot applied have learned discriminant function directly instead determining posterior probabilities combining models for complex applications may wish break the problem into number smaller subproblems each which can tackled separate module for example our hypothetical medical diagnosis problem may have information available from say blood tests well ray images rather than combine all this heterogeneous information into one huge input space may more effective build one system interpret the xray images and different one interpret the blood data long each the two models gives posterior probabilities for the classes can combine the outputs systematically using the rules probability one simple way this assume that for each class separately the distributions inputs for the ray images denoted and the blood data denoted are independent that this example conditional independence property because the independence holds when the distribution conditioned the class the posterior probability given both the ray and blood data then given thus need the class prior probabilities which can easily estimate from the fractions data points each class and then need normalize the resulting posterior probabilities they sum one the particular conditional independence assumption example the naive bayes model note that the joint marginal distribution will typically not factorize under this model shall see later chapters how construct models for combining data that not require the conditional independence assumption"}, "16": {"Section": "1.5.5", "Title": "Loss functions for regression", "Content": "far have discussed decision theory the context classification problems now turn the case regression problems such the curve fitting example discussed earlier the decision stage consists choosing specific estimate the value for each input suppose that doing incur loss the average expected loss then given common choice loss function regression problems the squared loss given this case the expected loss can written our goal choose minimize assume completely flexible function can this formally using the calculus variations give solving for and using the sum and product rules probability obtain introduction section section section appendix figure the regression function which minimizes the expected squared loss given the mean the conditional distribution decision theory exercise which the conditional average conditioned and known the regression function this result illustrated figure can readily extended multiple target variables represented the vector which case the optimal solution the conditional average can also derive this result slightly different way which will also shed light the nature the regression problem armed with the knowledge that the optimal solution the conditional expectation can expand the square term follows where keep the notation uncluttered use denote substituting into the loss function and performing the integral over see that the cross term vanishes and obtain expression for the loss function the form the function seek determine enters only the first term which will minimized when equal which case this term will vanish this simply the result that derived previously and that shows that the optimal least squares predictor given the conditional mean the second term the variance the distribution averaged over represents the intrinsic variability the target data and can regarded noise because independent represents the irreducible minimum value the loss function with the classification problem can either determine the appropriate probabilities and then use these make optimal decisions can build models that make decisions directly indeed can identify three distinct approaches solving regression problems given order decreasing complexity first solve the inference problem determining the joint density then normalize find the conditional density and finally marginalize find the conditional mean given section exercise which reduces the expected squared loss for the function plotted against for various values figure the minimum given the conditional mean for the conditional median for and the conditional mode for information theory this chapter have discussed variety concepts from probability theory and decision theory that will form the foundations for much the subsequent discussion this book close this chapter introducing some additional concepts from the field information theory which will also prove useful our development pattern recognition and machine learning techniques again shall focus only the key concepts and refer the reader elsewhere for more detailed discussions viterbi and omura cover and thomas mackay begin considering discrete random variable and ask how much information received when observe specific value for this variable the amount information can viewed the degree surprise learning the value are told that highly improbable event has just occurred will have received more information than were told that some very likely event has just occurred and knew that the event was certain happen would receive information our measure information content will therefore depend the probability distribution and therefore look for quantity that monotonic function the probability and that expresses the information content the form can found noting that have two events and that are unrelated then the information gain from observing both them should the sum the information gained from each them separately that two unrelated events will statistically independent and from these two relationships easily shown that must given the logarithm and have exercise introduction first solve the inference problem determining the conditional density and then subsequently marginalize find the conditional mean given find regression function directly from the training data the relative merits these three approaches follow the same lines for classification problems above the squared loss not the only possible choice loss function for regression indeed there are situations which squared loss can lead very poor results and where need develop more sophisticated approaches important example concerns situations which the conditional distribution multimodal often arises the solution inverse problems here consider briefly one simple generalization the squared loss called the minkowski loss whose expectation given"}, "17": {"Section": "1.6", "Title": "Information Theory", "Content": "figure plots the quantity for various values log where the negative sign ensures that information positive zero note that low probability events correspond high information content the choice basis for the logarithm arbitrary and for the moment shall adopt the convention prevalent information theory using logarithms the base this case shall see shortly the units are bits binary digits now suppose that sender wishes transmit the value random variable receiver the average amount information that they transmit the process obtained taking the expectation with respect the distribution and given log this important quantity called the entropy the random variable note that limp and shall take whenever encounter value for such that far have given rather heuristic motivation for the definition informa introduction tion and the corresponding entropy now show that these definitions indeed possess useful properties consider random variable having possible states each which equally likely order communicate the value receiver would need transmit message length bits notice that the entropy this variable given log bits now consider example cover and thomas variable having possible states for which the respective probabilities are given the entropy this case given log log log log log bits see that the nonuniform distribution has smaller entropy than the uniform one and shall gain some insight into this shortly when discuss the interpretation entropy terms disorder for the moment let consider how would transmit the identity the variable state receiver could this before using bit number however can take advantage the nonuniform distribution using shorter codes for the more probable events the expense longer codes for the less probable events the hope getting shorter average code length this can done representing the states using for instance the following set code strings the average length the code that has transmitted then average code length bits which again the same the entropy the random variable note that shorter code strings cannot used because must possible disambiguate concatenation such strings into its component parts for instance decodes uniquely into the state sequence this relation between entropy and shortest coding length general one the noiseless coding theorem shannon states that the entropy lower bound the number bits needed transmit the state random variable from now shall switch the use natural logarithms defining entropy this will provide more convenient link with ideas elsewhere this book this case the entropy measured units nats instead bits which differ simply factor have introduced the concept entropy terms the average amount information needed specify the state random variable fact the concept entropy has much earlier origins physics where was introduced the context equilibrium thermodynamics and later given deeper interpretation measure disorder through developments statistical mechanics can understand this alternative view entropy considering set identical objects that are divided amongst set bins such that there are objects the ith bin consider information theory the number different ways allocating the objects the bins there are ways choose the first object ways choose the second object and leading total ways allocate all objects the bins where pronounced factorial denotes the product however don wish distinguish between rearrangements objects within each bin the ith bin there are ways reordering the objects and the total number ways allocating the objects the bins given which called the multiplicity the entropy then defined the logarithm the multiplicity scaled appropriate constant now consider the limit which the fractions are held fixed and apply stirling approximation which gives lim here limn the probability where have used object being assigned the ith bin physics terminology the specific arrangements objects the bins called microstate and the overall distribution occupation numbers expressed through the ratios called macrostate the multiplicity also known the weight the macrostate can interpret the bins the states discrete random variable where the entropy the random variable then distributions that are sharply peaked around few values will have relatively low entropy whereas those that are spread more evenly across many values will have higher entropy illustrated figure because the entropy nonnegative and will equal its minimum value when one the and all other the maximum entropy configuration can found maximizing using lagrange multiplier enforce the normalization constraint the probabilities thus maximize appendix introduction figure histograms two probability distributions over bins illustrating the higher value the entropy for the broader distribution the largest entropy would arise from uniform distribution that would give exercise from which find that all the are equal and are given where the total number states the corresponding value the entropy then this result can also derived from jensen inequality discussed shortly verify that the stationary point indeed maximum can evaluate the second derivative the entropy which gives iij where iij are the elements the identity matrix can extend the definition entropy include distributions over continuous variables follows first divide into bins width then assuming continuous the mean value theorem weisstein tells that for each such bin there must exist value such that can now quantize the continuous variable assigning any value the value whenever falls the ith bin the probability observing the value then this gives discrete distribution for which the entropy takes the form which follows from now omit where have used the second term the right hand side and then consider the limit information theory the first term the right hand side will approach the integral this limit that lim where the quantity the right hand side called the differential entropy see that the discrete and continuous forms the entropy differ quantity which diverges the limit this reflects the fact that specify continuous variable very precisely requires large number bits for density defined over multiple continuous variables denoted collectively the vector the differential entropy given the case discrete distributions saw that the maximum entropy configuration corresponded equal distribution probabilities across the possible states the variable let now consider the maximum entropy configuration for continuous variable order for this maximum well defined will necessary constrain the first and second moments well preserving the normalization constraint therefore maximize the differential entropy with the ludwig boltzmann ludwig eduard boltzmann was austrian physicist who created the field statistical mechanics prior boltzmann the concept entropy was already known from classical thermodynamics where quantifies the fact that when take energy from system not all that energy typically available useful work boltzmann showed that the thermodynamic entropy macroscopic quantity could related the statistical properties the microscopic level this expressed through the famous equation which represents the number possible microstates macrostate and units joules per kelvin known boltzmann constant boltzmann ideas were disputed many scientists they day one difficulty they saw arose from the second law thermodynamics which states that the entropy closed system tends increase with time contrast the microscopic level the classical newtonian equations physics are reversible and they found difficult see how the latter could explain the former they didn fully appreciate boltzmann arguments which were statistical nature and which concluded not that entropy could never decrease over time but simply that with overwhelming probability would generally increase boltzmann even had longrunning dispute with the editor the leading german physics journal who refused let him refer atoms and molecules anything other than convenient theoretical constructs the continued attacks his work lead bouts depression and eventually committed suicide shortly after boltzmann death new experiments perrin colloidal suspensions verified his theories and confirmed the value the boltzmann constant the equation carved boltzmann tombstone introduction three constraints appendix appendix exercise exercise the constrained maximization can performed using lagrange multipliers that maximize the following functional with respect using the calculus variations set the derivative this functional zero giving exp the lagrange multipliers can found back substitution this result into the three constraint equations leading finally the result exp and the distribution that maximizes the differential entropy the gaussian note that did not constrain the distribution nonnegative when maximized the entropy however because the resulting distribution indeed nonnegative see with hindsight that such constraint not necessary evaluate the differential entropy the gaussian obtain thus see again that the entropy increases the distribution becomes broader increases this result also shows that the differential entropy unlike the discrete entropy can negative because for suppose have joint distribution from which draw pairs values and value already known then the additional information needed specify the corresponding value given thus the average additional information needed specify can written exercise which called the conditional entropy given easily seen using the product rule that the conditional entropy satisfies the relation information theory where the differential entropy and the differential entropy the marginal distribution thus the information needed describe and given the sum the information needed describe alone plus the additional information required specify given"}, "18": {"Section": "1.6.1", "Title": "Relative entropy and mutual information", "Content": "far this section have introduced number concepts from information theory including the key notion entropy now start relate these ideas pattern recognition consider some unknown distribution and suppose that have modelled this using approximating distribution use construct coding scheme for the purpose transmitting values receiver then the average additional amount information nats required specify the value assuming choose efficient coding scheme result using instead the true distribution given this known the relative entropy kullback leibler divergence divergence kullback and leibler between the distributions and note that not symmetrical quantity that say now show that the kullback leibler divergence satisfies with equality and only this first introduce the concept convex functions function said convex has the property that every chord lies above the function shown figure any value the interval from can written the form where the corresponding point the chord given claude shannon after graduating from michigan and mit shannon joined the bell telephone laboratories his paper mathematical theory communication published the bell system technical journal laid the foundations for modern information theory this paper introduced the word bit and his concept that information could sent stream and paved the way for the communications revolution said that von neumann recommended shannon that use the term entropy not only because its similarity the quantity used physics but also because nobody knows what entropy really any discussion you will always have advantage introduction figure convex function one for which every chord shown blue lies above the function shown red chord and the corresponding value the function convexity then implies this equivalent the requirement that the second derivative the function everywhere positive examples convex functions are for and function called strictly convex the equality satisfied only for and function has the opposite property namely that every chord lies below the function called concave with corresponding definition for strictly concave function convex then will concave convex function satisfies using the technique proof induction can show from that exercise exercise ixi where and for any set points the result known jensen inequality interpret the the probability distribution over discrete variable taking the values then can written where denotes the expectation for continuous variables jensen inequality takes the form can apply jensen inequality the form the kullback leibler divergence give information theory where have used the fact that convex function together with the norq fact strictly convex function malization condition the equality will hold and only for all thus can interpret the kullback leibler divergence measure the dissimilarity the two distributions and see that there intimate relationship between data compression and density estimation the problem modelling unknown probability distribution because the most efficient compression achieved when know the true distribution use distribution that different from the true one then must necessarily have less efficient coding and average the additional information that must transmitted least equal the kullback leibler divergence between the two distributions suppose that data being generated from unknown distribution that wish model can try approximate this distribution using some parametric distribution governed set adjustable parameters for example multivariate gaussian one way determine minimize the kullback leibler divergence between and with respect cannot this directly because don know suppose however that have observed finite set training points for drawn from then the expectation with respect can approximated finite sum over these points using that the second term the right hand side independent and the first term the negative log likelihood function for under the distribution evaluated using the training set thus see that minimizing this kullback leibler divergence equivalent maximizing the likelihood function now consider the joint distribution between two sets variables and given the sets variables are independent then their joint distribution will factorize into the product their marginals the variables are not independent can gain some idea whether they are close being independent considering the kullback leibler divergence between the joint distribution and the product the marginals given exercise which called the mutual information between the variables and from the properties the kullback leibler divergence see that with equality and only and are independent using the sum and product rules probability see that the mutual information related the conditional entropy through introduction thus can view the mutual information the reduction the uncertainty about virtue being told the value vice versa from bayesian perspective can view the prior distribution for and the posterior distribution after have observed new data the mutual information therefore represents the reduction uncertainty about consequence the new observation"}, "19": {"Section": "2", "Title": "Probability Distributions", "Content": "chapter emphasized the central role played probability theory the solution pattern recognition problems turn now exploration some particular examples probability distributions and their properties well being great interest their own right these distributions can form building blocks for more complex models and will used extensively throughout the book the distributions introduced this chapter will also serve another important purpose namely provide with the opportunity discuss some key statistical concepts such bayesian inference the context simple models before encounter them more complex situations later chapters one role for the distributions discussed this chapter model the probability distribution random variable given finite set observations this problem known density estimation for the purposes this chapter shall assume that the data points are independent and identically distributed should emphasized that the problem density estimation fun probability distributions damentally ill posed because there are infinitely many probability distributions that could have given rise the observed finite data set indeed any distribution that nonzero each the data points potential candidate the issue choosing appropriate distribution relates the problem model selection that has already been encountered the context polynomial curve fitting chapter and that central issue pattern recognition begin considering the binomial and multinomial distributions for discrete random variables and the gaussian distribution for continuous random variables these are specific examples parametric distributions called because they are governed small number adaptive parameters such the mean and variance the case gaussian for example apply such models the problem density estimation need procedure for determining suitable values for the parameters given observed data set frequentist treatment choose specific values for the parameters optimizing some criterion such the likelihood function contrast bayesian treatment introduce prior distributions over the parameters and then use bayes theorem compute the corresponding posterior distribution given the observed data shall see that important role played conjugate priors that lead posterior distributions having the same functional form the prior and that therefore lead greatly simplified bayesian analysis for example the conjugate prior for the parameters the multinomial distribution called the dirichlet distribution while the conjugate prior for the mean gaussian another gaussian all these distributions are examples the exponential family distributions which possess number important properties and which will discussed some detail one limitation the parametric approach that assumes specific functional form for the distribution which may turn out inappropriate for particular application alternative approach given nonparametric density estimation methods which the form the distribution typically depends the size the data set such models still contain parameters but these control the model complexity rather than the form the distribution end this chapter considering three nonparametric methods based respectively histograms nearest neighbours and kernels"}, "20": {"Section": "2.1", "Title": "Binary Variables", "Content": "begin considering single binary random variable for example might describe the outcome flipping coin with representing heads and representing tails can imagine that this damaged coin that the probability landing heads not necessarily the same that landing tails the probability will denoted the parameter that binary variables where from which follows that the probability distribution over can therefore written the form bern which known the bernoulli distribution easily verified that this distribution normalized and that has mean and variance given var now suppose have data set observed values can construct the likelihood function which function the assumption that the observations are drawn independently from that frequentist setting can estimate value for maximizing the likelihood function equivalently maximizing the logarithm the likelihood the case the bernoulli distribution the log likelihood function given this point worth noting that the log likelihood function depends the observations only through their sum this sum provides example sufficient statistic for the data under this distribution and shall study the important role sufficient statistics some detail set the derivative with respect equal zero obtain the maximum likelihood estimator exercise section jacob bernoulli jacob bernoulli also known jacques james bernoulli was swiss mathematician and was the first many the bernoulli family pursue career science and mathematics although compelled study philosophy and theology against his will his parents travelled extensively after graduating order meet with many the leading scientists his time including boyle and hooke england when returned switzerland taught mechanics and became professor mathematics basel unfortunately rivalry between jacob and his younger brother johann turned initially productive collaboration into bitter and public dispute jacob most significant contributions mathematics appeared the artofconjecture published eight years after his death which deals with topics probability theory including what has become known the bernoulli distribution which also known the sample mean denote the number observations heads within this data set then can write the form that the probability landing heads given this maximum likelihood framework the fraction observations heads the data set now suppose flip coin say times and happen observe heads then and this case the maximum likelihood result would predict that all future observations should give heads common sense tells that this unreasonable and fact this extreme example the over fitting associated with maximum likelihood shall see shortly how arrive more sensible conclusions through the introduction prior distribution over can also work out the distribution the number observations given that the data set has size this called the binomial distribution and from see that proportional order obtain the normalization coefficient note that out coin flips have add all the possible ways obtaining heads that the binomial distribution can written bin the number ways choosing objects out total identical objects figure shows plot the binomial distribution for and the mean and variance the binomial distribution can found using the result exercise which shows that for independent events the mean the sum the sum the means and the variance the sum the sum the variances because and for each observation the mean and variance are probability distributions figure histogram plot the binomial distribution function for and where exercise given and respectively have binary variables mbin var bin these results can also proved directly using calculus"}, "21": {"Section": "2.1.1", "Title": "The beta distribution", "Content": "have seen that the maximum likelihood setting for the parameter the bernoulli distribution and hence the binomial distribution given the fraction the observations the data set having have already noted this can give severely over fitted results for small data sets order develop bayesian treatment for this problem need introduce prior distribution over the parameter here consider form prior distribution that has simple interpretation well some useful analytical properties motivate this prior note that the likelihood function takes the form the product factors the form choose prior proportional powers and then the posterior distribution which proportional the product the prior and the likelihood function will have the same functional form the prior this property called conjugacy and will see several examples later this chapter therefore choose prior called the beta distribution given beta where the gamma function defined and the coefficient ensures that the beta distribution normalized that exercise exercise exercise the mean and variance the beta distribution are given beta var the parameters and are often called hyperparameters because they control the distribution the parameter figure shows plots the beta distribution for various values the hyperparameters the posterior distribution now obtained multiplying the beta prior the binomial likelihood function and normalizing keeping only the factors that depend see that this posterior distribution has the form probability distributions figure plots the beta distribution beta given function for various values the hyperparameters and where and therefore corresponds the number tails the coin example see that has the same functional dependence the prior distribution reflecting the conjugacy properties the prior with respect the likelihood function indeed simply another beta distribution and its normalization coefficient can therefore obtained comparison with give see that the effect observing data set observations and observations has been increase the value and the value going from the prior distribution the posterior distribution this allows provide simple interpretation the hyperparameters and the prior effective number observations and respectively note that and need not integers furthermore the posterior distribution can act the prior subsequently observe additional data see this can imagine taking observations one time and after each observation updating the current posterior likelihood function prior binary variables posterior figure illustration one step sequential bayesian inference the prior given beta distribution with parameters and the likelihood function given with corresponds single observation that the posterior given beta distribution with parameters section distribution multiplying the likelihood function for the new observation and then normalizing obtain the new revised posterior distribution each stage the posterior beta distribution with some total number prior and actual observed values for and given the parameters and incorporation additional observation simply corresponds incrementing the value whereas for observation increment figure illustrates one step this process see that this sequential approach learning arises naturally when adopt bayesian viewpoint independent the choice prior and the likelihood function and depends only the assumption data sequential methods make use observations one time small batches and then discard them before the next observations are used they can used for example real time learning scenarios where steady stream data arriving and predictions must made before all the data seen because they not require the whole data set stored loaded into memory sequential methods are also useful for large data sets maximum likelihood methods can also cast into sequential framework our goal predict best can the outcome the next trial then must evaluate the predictive distribution given the observed data set from the sum and product rules probability this takes the form using the result for the posterior distribution together with the result for the mean the beta distribution obtain which has simple interpretation the total fraction observations both real observations and fictitious prior observations that correspond note that the limit infinitely large data set the result reduces the maximum likelihood result shall see very general property that the bayesian and maximum likelihood results will agree the limit infinitely probability distributions exercise exercise large data set for finite data set the posterior mean for always lies between the prior mean and the maximum likelihood estimate for corresponding the relative frequencies events given from figure see that the number observations increases the posterior distribution becomes more sharply peaked this can also seen from the result for the variance the beta distribution which see that the variance goes zero for fact might wonder whether general property bayesian learning that observe more and more data the uncertainty represented the posterior distribution will steadily decrease address this can take frequentist view bayesian learning and show that average such property does indeed hold consider general bayesian inference problem for parameter for which have observed data set described the joint distribution the following result where says that the posterior mean averaged over the distribution generating the data equal the prior mean similarly can show that var var vard the term the left hand side the prior variance the righthand side the first term the average posterior variance and the second term measures the variance the posterior mean because this variance positive quantity this result shows that average the posterior variance smaller than the prior variance the reduction variance greater the variance the posterior mean greater note however that this result only holds average and that for particular observed data set possible for the posterior variance larger than the prior variance"}, "22": {"Section": "2.2", "Title": "Multinomial Variables", "Content": "binary variables can used describe quantities that can take one two possible values often however encounter discrete variables that can take one possible mutually exclusive states although there are various alternative ways express such variables shall see shortly that particularly convenient representation the scheme which the variable represented dimensional vector which one the elements equals and all remaining elements equal xnk multinomial variables for instance have variable that can take states and particular observation the variable happens correspond the state where then will represented note that such vectors satisfy the parameter then the distribution given denote the probability where and the parameters are constrained satisfy because they represent probabilities the distribution can and regarded generalization the bernoulli distribution more than two outcomes easily seen that the distribution normalized and that now consider data set independent observations the corresponding likelihood function takes the form xnk xnk see that the likelihood function depends the data points only through the quantities section appendix which represent the number observations these are called the sufficient statistics for this distribution order find the maximum likelihood solution for need maximize with respect taking account the constraint that the must sum one this can achieved using lagrange multiplier and maximizing setting the derivative with respect zero obtain probability distributions can solve for the lagrange multiplier substituting into the constraint give thus obtain the maximum likelihood solution the form which the fraction the observations for which can consider the joint distribution the quantities conditioned the parameters and the total number observations from this takes the form mult which known the multinomial distribution the normalization coefficient the number ways partitioning objects into groups size and given note that the variables are subject the constraint"}, "23": {"Section": "2.2.1", "Title": "The Dirichlet distribution", "Content": "now introduce family prior distributions for the parameters the multinomial distribution inspection the form the multinomial distribution see that the conjugate prior given exercise where and here are the parameters the distribution and denotes note that because the summation constraint the distribution over the space the confined simplex dimensionality illustrated for figure the normalized form for this distribution dir which called the dirichlet distribution here the gamma function defined while multinomial variables figure the dirichlet distribution over three variables confined simplex bounded linear manifold the form shown consequence the constraints and plots the dirichlet distribution over the simplex for various settings the parameters are shown figure multiplying the prior the likelihood function obtain the posterior distribution for the parameters the form see that the posterior distribution again takes the form dirichlet distribution confirming that the dirichlet indeed conjugate prior for the multinomial this allows determine the normalization coefficient comparison with that dir where have denoted for the case the binomial distribution with its beta prior can interpret the parameters the dirichlet prior effective number observations note that two state quantities can either represented binary variables and lejeune dirichlet lejeune johann peter gustav dirichlet was modest and reserved mathematician who made contributions number theory mechanics and astronomy and who gave the first rigorous analysis fourier series his family originated from richelet belgium and the name lejeune dirichlet comes from jeune richelet the young person from richelet dirichlet first paper which was published brought him instant fame concerned fermat last theorem which claims that there are positive integer solutions for dirichlet gave partial proof for the case which was sent legendre for review and who turn completed the proof later dirichlet gave complete proof for although full proof fermat last theorem for arbitrary had wait until the work andrew wiles the closing years the century probability distributions figure plots the dirichlet distribution over three variables where the two horizontal axes are coordinates the plane the simplex and the vertical axis corresponds the value the density here the left plot the centre plot and the right plot modelled using the binomial distribution variables and modelled using the multinomial distribution with"}, "24": {"Section": "2.3", "Title": "The Gaussian Distribution", "Content": "the gaussian also known the normal distribution widely used model for the distribution continuous variables the case single variable the gaussian distribution can written the form exp where the mean and the variance for dimensional vector the multivariate gaussian distribution takes the form section exercise exp where dimensional mean vector covariance matrix and denotes the determinant the gaussian distribution arises many different contexts and can motivated from variety different perspectives for example have already seen that for single real variable the distribution that maximizes the entropy the gaussian this property applies also the multivariate gaussian another situation which the gaussian distribution arises when consider the sum multiple random variables the central limit theorem due laplace tells that subject certain mild conditions the sum set random variables which course itself random variable has distribution that becomes increasingly gaussian the number terms the sum increases walker can the gaussian distribution figure histogram plots the mean uniformly distributed numbers for various values observe that increases the distribution tends towards gaussian illustrate this considering variables each which has uniform distribution over the interval and then considering the distribution the mean for large this distribution tends gaussian illustrated figure practice the convergence gaussian increases can very rapid one consequence this result that the binomial distribution which distribution over defined the sum observations the random binary variable will tend gaussian see figure for the case the gaussian distribution has many important analytical properties and shall consider several these detail result this section will rather more technically involved than some the earlier sections and will require familiarity with various matrix identities however strongly encourage the reader become proficient manipulating gaussian distributions using the techniques presented here this will prove invaluable understanding the more complex models presented later chapters begin considering the geometrical form the gaussian distribution the appendix carl friedrich gauss said that when gauss went elementary school age his teacher uttner trying keep the class occupied asked the pupils sum the integers from the teacher amazement gauss arrived the answer matter moments noting that the sum can represented pairs etc each which added giving the answer now believed that the problem which was actually set was the same form but somewhat harder that the sequence had larger starting value and larger increment gauss was german mathematician and scientist with reputation for being hard working perfectionist one his many contributions was show that least squares can derived under the assumption normally distributed errors also created early formulation non euclidean geometry self consistent geometrical theory that violates the axioms euclid but was reluctant discuss openly for fear that his reputation might suffer were seen that believed such geometry one point gauss was asked conduct geodetic survey the state hanover which led his formulation the normal distribution now also known the gaussian after his death study his diaries revealed that had discovered several important mathematical results years even decades before they were published others iui iij iij otherwise iuiut uiut probability distributions exercise exercise exercise functional dependence the gaussian through the quadratic form which appears the exponent the quantity called the mahalanobis distance from and reduces the euclidean distance when the identity matrix the gaussian distribution will constant surfaces space for which this quadratic form constant first all note that the matrix can taken symmetric without loss generality because any antisymmetric component would disappear from the exponent now consider the eigenvector equation for the covariance matrix where because real symmetric matrix its eigenvalues will real and its eigenvectors can chosen form orthonormal set that where iij the element the identity matrix and satisfies the covariance matrix can expressed expansion terms its eigenvectors the form and similarly the inverse covariance matrix can expressed substituting into the quadratic form becomes where have defined can interpret new coordinate system defined the orthonormal vectors that are shifted and rotated with respect the original coordinates forming the vector have appendix figure the red curve shows the elliptical surface constant probability density for gaussian two dimensional space which the density its value exp the major axes the ellipse are defined the eigenvectors the covariance matrix with corresponding eigenvalues the gaussian distribution where matrix whose rows are given from follows that orthogonal matrix satisfies uut and hence also utu where the identity matrix the quadratic form and hence the gaussian density will constant surfaces all the eigenvalues are positive then these for which constant surfaces represent ellipsoids with their centres and their axes oriented along and with scaling factors the directions the axes given illustrated figure for the gaussian distribution well defined necessary for all the eigenvalues the covariance matrix strictly positive otherwise the distribution cannot properly normalized matrix whose eigenvalues are strictly positive said positive definite chapter will encounter gaussian distributions for which one more the eigenvalues are zero which case the distribution singular and confined subspace lower dimensionality all the eigenvalues are nonnegative then the covariance matrix said positive semidefinite now consider the form the gaussian distribution the new coordinate system defined the going from the the coordinate system have jacobian matrix with elements given jij uji where uji are the elements the matrix using the orthonormality property the matrix see that the square the determinant the jacobian matrix and hence also the determinant the covariance matrix can written utu probability distributions the product its eigenvalues and hence thus the coordinate system the gaussian distribution takes the form exp which the product independent univariate gaussian distributions the eigenvectors therefore define new set shifted and rotated coordinates with respect which the joint probability distribution factorizes into product independent distributions the integral the distribution the coordinate system then exp dyj where have used the result for the normalization the univariate gaussian this confirms that the multivariate gaussian indeed normalized now look the moments the gaussian distribution and thereby provide interpretation the parameters and the expectation under the gaussian distribution given exp exp where have changed variables using now note that the exponent even function the components and because the integrals over these are taken over the range the term the factor will vanish symmetry thus and refer the mean the gaussian distribution now consider second order moments the gaussian the univariate case considered the second order moment given for the multivariate gaussian there are second order moments given xixj which can group together form the matrix xxt this matrix can written xxt exp xxt exp yjuj the gaussian distribution where again have changed variables using note that the cross terms involving and will again vanish symmetry the term constant and can taken outside the integral which itself unity because the gaussian distribution normalized consider the term involving zzt again can make use the eigenvector expansion the covariance matrix given together with the completeness the set eigenvectors write where which gives uiut exp zzt uiut exp yiyj where have made use the eigenvector equation together with the fact that the integral the right hand side the middle line vanishes symmetry unless and the final line have made use the results and together with thus have xxt for single random variables subtracted the mean before taking second moments order define variance similarly the multivariate case again convenient subtract off the mean giving rise the covariance random vector defined for the specific case gaussian distribution can make use together with the result give cov cov because the parameter matrix governs the covariance under the gaussian distribution called the covariance matrix although the gaussian distribution widely used density model suffers from some significant limitations consider the number free parameters the distribution general symmetric covariance matrix will have independent parameters and there are another independent parameters giving parameters total for large the total number parameters exercise probability distributions figure contours constant probability density for gaussian distribution two dimensions which the covariance matrix general form diagonal which the elliptical contours are aligned with the coordinate axes and proportional the identity matrix which the contours are concentric circles therefore grows quadratically with and the computational task manipulating and inverting large matrices can become prohibitive one way address this problem use restricted forms the covariance matrix consider covariance matrices that are diagonal that diag then have total independent parameters the density model the corresponding contours constant density are given axis aligned ellipsoids could further restrict the covariance matrix proportional the identity matrix known isotropic covariance giving independent parameters the model and spherical surfaces constant density the three possibilities general diagonal and isotropic covariance matrices are illustrated figure unfortunately whereas such approaches limit the number degrees freedom the distribution and make inversion the covariance matrix much faster operation they also greatly restrict the form the probability density and limit its ability capture interesting correlations the data further limitation the gaussian distribution that intrinsically unimodal has single maximum and unable provide good approximation multimodal distributions thus the gaussian distribution can both too flexible the sense having too many parameters while also being too limited the range distributions that can adequately represent will see later that the introduction latent variables also called hidden variables unobserved variables allows both these problems addressed particular rich family multimodal distributions obtained introducing discrete latent variables leading mixtures gaussians discussed section similarly the introduction continuous latent variables described chapter leads models which the number free parameters can controlled independently the dimensionality the data space while still allowing the model capture the dominant correlations the data set indeed these two approaches can combined and further extended derive very rich set hierarchical models that can adapted broad range practical applications for instance the gaussian version the markov random field which widely used probabilistic model images gaussian distribution over the joint space pixel intensities but rendered tractable through the imposition considerable structure reflecting the spatial organization the pixels similarly the linear dynamical system used model time series data for applications such tracking also joint gaussian distribution over potentially large number observed and latent variables and again tractable due the structure imposed the distribution powerful framework for expressing the form and properties section section and the covariance matrix given note that the symmetry the covariance matrix implies that and are symmetric while many situations will convenient work with the inverse the covariance matrix which known the precision matrix fact shall see that some properties gaussian distributions are most naturally expressed terms the covariance whereas others take simpler form when viewed terms the precision therefore also introduce the partitioned form the precision matrix the gaussian distribution such complex distributions that probabilistic graphical models which will form the subject chapter"}, "25": {"Section": "2.3.1", "Title": "Conditional Gaussian distributions", "Content": "important property the multivariate gaussian distribution that two sets variables are jointly gaussian then the conditional distribution one set conditioned the other again gaussian similarly the marginal distribution either set also gaussian consider first the case conditional distributions suppose dimensional vector with gaussian distribution and that partition into two disjoint subsets and without loss generality can take form the first components with comprising the remaining components that also define corresponding partitions the mean vector given exercise corresponding the partitioning the vector because the inverse symmetric matrix also symmetric see that and are symmetric while should stressed this point that for instance not simply given the inverse fact shall shortly examine the relation between the inverse partitioned matrix and the inverses its partitions let begin finding expression for the conditional distribution from the product rule probability see that this conditional distribution can probability distributions evaluated from the joint distribution simply fixing the observed value and normalizing the resulting expression obtain valid probability distribution over instead performing this normalization explicitly can obtain the solution more efficiently considering the quadratic form the exponent the gaussian distribution given and then reinstating the normalization coefficient the end the calculation make use the partitioning and obtain see that function this again quadratic form and hence the corresponding conditional distribution will gaussian because this distribution completely characterized its mean and its covariance our goal will identify expressions for the mean and covariance inspection this example rather common operation associated with gaussian distributions sometimes called completing the square which are given quadratic form defining the exponent terms gaussian distribution and need determine the corresponding mean and covariance such problems can solved straightforwardly noting that the exponent general gaussian distribution can written const where const denotes terms which are independent and have made use the symmetry thus take our general quadratic form and express the form given the right hand side then can immediately equate the matrix coefficients entering the second order term the inverse covariance matrix and the coefficient the linear term from which can obtain now let apply this procedure the conditional gaussian distribution for which the quadratic form the exponent given will denote the mean and covariance this distribution and respectively consider the functional dependence which regarded constant pick out all terms that are second order have aaxa from which can immediately conclude that the covariance inverse precision given the gaussian distribution now consider all the terms that are linear where have used the coefficient this expression must equal from our discussion the general form and hence where have made use the results and are expressed terms the partitioned precision matrix the original joint distribution can also express these results terms the corresponding partitioned covariance matrix this make use the following identity for the inverse partitioned matrix where have defined cmbd mbd the quantity known the schur complement the matrix the left hand side with respect the submatrix using the definition and making use have from these obtain the following expressions for the mean and covariance the conditional distribution comparing and see that the conditional distribution takes simpler form when expressed terms the partitioned precision matrix than when expressed terms the partitioned covariance matrix note that the mean the conditional distribution given linear function and that the covariance given independent this represents example linear gaussian model exercise section probability distributions"}, "26": {"Section": "2.3.2", "Title": "Marginal Gaussian distributions", "Content": "have seen that joint distribution gaussian then the conditional distribution will again gaussian now turn discussion the marginal distribution given dxb which shall see also gaussian once again our strategy for evaluating this distribution efficiently will focus the quadratic form the exponent the joint distribution and thereby identify the mean and covariance the marginal distribution the quadratic form for the joint distribution can expressed using the partitioned precision matrix the form because our goal integrate out this most easily achieved first considering the terms involving and then completing the square order facilitate integration picking out just those terms that involve have bbxb where have defined see that the dependence has been cast into the standard quadratic form gaussian distribution corresponding the first term the right hand side plus term that does not depend but that does depend thus when take the exponential this quadratic form see that the integration over required will take the form exp dxb this integration easily performed noting that the integral over unnormalized gaussian and the result will the reciprocal the normalization coefficient know from the form the normalized gaussian given that this coefficient independent the mean and depends only the determinant the covariance matrix thus completing the square with respect can integrate out and the only term remaining from the contributions the left hand side that depends the last term the right hand side which given combining this term with the remaining terms from the gaussian distribution that depend obtain const aaxa const where const denotes quantities independent again comparison with see that the covariance the marginal distribution given similarly the mean given where have used the covariance expressed terms the partitioned precision matrix given can rewrite this terms the corresponding partitioning the covariance matrix given did for the conditional distribution these partitioned matrices are related making use then have thus obtain the intuitively satisfying result that the marginal distribution has mean and covariance given cov see that for marginal distribution the mean and covariance are most simply expressed terms the partitioned covariance matrix contrast the conditional distribution for which the partitioned precision matrix gives rise simpler expressions our results for the marginal and conditional distributions partitioned gaussian are summarized below partitioned gaussians given joint gaussian distribution with and probability distributions figure the plot the left shows the contours gaussian distribution over two variables and the plot the right shows the marginal distribution blue curve and the conditional distribution for red curve conditional distribution marginal distribution illustrate the idea conditional and marginal distributions associated with multivariate gaussian using example involving two variables figure"}, "27": {"Section": "2.3.3", "Title": "Bayes\u2019 theorem for Gaussian variables", "Content": "sections and considered gaussian which partitioned the vector into two subvectors and then found expressions for the conditional distribution and the marginal distribution noted that the mean the conditional distribution was linear function here shall suppose that are given gaussian marginal distribution and gaussian conditional distribution which has mean that linear function and covariance which independent this example the gaussian distribution linear gaussian model roweis and ghahramani which shall study greater generality section wish find the marginal distribution and the conditional distribution this problem that will arise frequently subsequent chapters and will prove convenient derive the general results here shall take the marginal and conditional distributions where and are parameters governing the means and and are precision matrices has dimensionality and has dimensionality then the matrix has size define first find expression for the joint distribution over and this and then consider the log the joint distribution const where const denotes terms independent and before see that this quadratic function the components and hence gaussian distribution find the precision this gaussian consider the second order terms which can written atla ytly ytlax xtatly atla atl ztrz and the gaussian distribution over has precision inverse covariance matrix given atla atl exercise the covariance matrix found taking the inverse the precision which can done using the matrix inversion formula give cov probability distributions similarly can find the mean the gaussian distribution over identifying the linear terms which are given xtatlb ytlb atlb using our earlier result obtained completing the square over the quadratic form multivariate gaussian find that the mean given exercise making use then obtain atlb section section next find expression for the marginal distribution which have marginalized over recall that the marginal distribution over subset the components gaussian random vector takes particularly simple form when expressed terms the partitioned covariance matrix specifically its mean and covariance are given and respectively making use and see that the mean and covariance the marginal distribution are given cov special case this result when which case reduces the convolution two gaussians for which see that the mean the convolution the sum the mean the two gaussians and the covariance the convolution the sum their covariances finally seek expression for the conditional recall that the results for the conditional distribution are most easily expressed terms the partitioned precision matrix using and applying these results and see that the conditional distribution has mean and covariance given atla cov atla atl the evaluation this conditional can seen example bayes theorem can interpret the distribution prior distribution over the variable observed then the conditional distribution represents the corresponding posterior distribution over having found the marginal and conditional distributions effectively expressed the joint distribution the form these results are summarized below the gaussian distribution marginal and conditional gaussians given marginal gaussian distribution for and conditional gaussian distribution for given the form the marginal distribution and the conditional distribution given are given atl where atla"}, "28": {"Section": "2.3.4", "Title": "Maximum likelihood for the Gaussian", "Content": "given data set which the observations are assumed drawn independently from multivariate gaussian distribution can estimate the parameters the distribution maximum likelihood the log likelihood function given simple rearrangement see that the likelihood function depends the data set only through the two quantities appendix these are known the sufficient statistics for the gaussian distribution using the derivative the log likelihood with respect given xnxt and setting this derivative zero obtain the solution for the maximum likelihood estimate the mean given probability distributions exercise exercise which the mean the observed set data points the maximization with respect rather more involved the simplest approach ignore the symmetry constraint and show that the resulting solution symmetric required alternative derivations this result which impose the symmetry and positive definiteness constraints explicitly can found magnus and neudecker the result expected and takes the form which involves because this the result joint maximization with respect and note that the solution for does not depend and can first evaluate and then use this evaluate evaluate the expectations the maximum likelihood solutions under the true distribution obtain the following results see that the expectation the maximum likelihood estimate for the mean equal the true mean however the maximum likelihood estimate for the covariance has expectation that less than the true value and hence biased can correct this bias defining different estimator given clearly from and the expectation equal"}, "29": {"Section": "2.3.5", "Title": "Sequential estimation", "Content": "our discussion the maximum likelihood solution for the parameters gaussian distribution provides convenient opportunity give more general discussion the topic sequential estimation for maximum likelihood sequential methods allow data points processed one time and then discarded and are important for line applications and also where large data sets are involved that batch processing all data points once infeasible consider the result for the maximum likelihood estimator the mean when based observations which will denote figure schematic illustration two correlated random variables and together with the regression function given the conthe robbinsditional expectation monro algorithm provides general sequential procedure for finding the root such functions the gaussian distribution dissect out the contribution from the final data point obtain this result has nice interpretation follows after observing data points have estimated now observe data point and obtain our revised estimate moving the old estimate small amount proportional the direction the error signal note that increases the contribution from successive data points gets smaller the result will clearly give the same answer the batch result because the two formulae are equivalent however will not always able derive sequential algorithm this route and seek more general formulation sequential learning which leads the robbins monro algorithm consider pair random variables and governed joint distribution the conditional expectation given defines deterministic function that given and illustrated schematically figure functions defined this way are called regression functions our goal find the root which had large data set observations and then could model the regression function directly and then obtain estimate its root suppose however that observe values one time and wish find corresponding sequential estimation scheme for the following general procedure for solving such problems was given probability distributions robbins and monro shall assume that the conditional variance finite that and shall also without loss generality consider the case where for and for the case figure the robbins monro procedure then defines sequence successive estimates the root given where observed value when takes the value the coefficients represent sequence positive numbers that satisfy the conditions lim can then shown robbins and monro fukunaga that the sequence estimates given does indeed converge the root with probability one note that the first condition ensures that the successive corrections decrease magnitude that the process can converge limiting value the second condition required ensure that the algorithm does not converge short the root and the third condition needed ensure that the accumulated noise has finite variance and hence does not spoil convergence now let consider how general maximum likelihood problem can solved sequentially using the robbins monro algorithm definition the maximum likelihood solution stationary point the log likelihood function and hence satisfies exchanging the derivative and the summation and taking the limit have lim and see that finding the maximum likelihood solution corresponds finding the root regression function can therefore apply the robbins monro procedure which now takes the form the gaussian distribution figure the case gaussian distribution with corresponding the mean the regression function illustrated figure takes the form straight line shown red this case the random variable corresponds the derivative the log likelihood function and given and its expectation that defines the regression function straight line given the root the regression function corresponds the maximum likelihood estimator specific example consider once again the sequential estimation the mean gaussian distribution which case the parameter the estimate the mean the gaussian and the random variable given thus the distribution gaussian with mean illustrated figure substituting into obtain the univariate form provided choose the coefficients have the form note that although have focussed the case single variable the same technique together with the same restrictions the coefficients apply equally the multivariate case blum"}, "30": {"Section": "2.3.6", "Title": "Bayesian inference for the Gaussian", "Content": "the maximum likelihood framework gave point estimates for the parameters and now develop bayesian treatment introducing prior distributions over these parameters let begin with simple example which consider single gaussian random variable shall suppose that the variance known and consider the task inferring the mean given set observations the likelihood function that the probability the observed data given viewed function given exp again emphasize that the likelihood function not probability distribution over and not normalized see that the likelihood function takes the form the exponential quadratic form thus choose prior given gaussian will and the posterior distribution given where which the maximum likelihood solution for given the sample mean worth spending moment studying the form the posterior mean and variance first all note that the mean the posterior distribution given compromise between the prior mean and the maximum likelihood solution the number observed data points then reduces the prior mean expected for the posterior mean given the maximum likelihood solution similarly consider the result for the variance the posterior distribution see that this most naturally expressed terms the inverse variance which called the precision furthermore the precisions are additive that the precision the posterior given the precision the prior plus one contribution the data precision from each the observed data points increase the number observed data points the precision steadily increases corresponding posterior distribution with steadily decreasing variance with observed data points have the prior variance whereas the number goes zero and the posterior distribution data points the variance becomes infinitely peaked around the maximum likelihood solution therefore see that the maximum likelihood result point estimate for given recovered precisely from the bayesian formalism the limit infinite number observations note also that for finite take the limit which the prior has infinite variance then the posterior mean reduces the maximum likelihood result while from the posterior variance given probability distributions conjugate distribution for this likelihood function because the corresponding posterior will product two exponentials quadratic functions and hence will also gaussian therefore take our prior distribution exercise simple manipulation involving completing the square the exponent shows that the posterior distribution given the gaussian distribution figure illustration bayesian inference for the mean gaussian distribution which the variance assumed known the curves show the prior distribution over the curve labelled which this case itself gaussian along with the posterior distribution given for increasing numbers data points the data points are generated from gaussian mean and variance and the prior chosen have mean both the prior and the likelihood function the variance set the true value exercise section illustrate our analysis bayesian inference for the mean gaussian distribution figure the generalization this result the case ddimensional gaussian random variable with known covariance and unknown mean straightforward have already seen how the maximum likelihood expression for the mean gaussian can cast sequential update formula which the mean after observing data points was expressed terms the mean after observing data points together with the contribution from data point fact the bayesian paradigm leads very naturally sequential view the inference problem see this the context the inference the mean gaussian write the posterior distribution with the contribution from the final data point separated out that the term square brackets normalization coefficient just the posterior distribution after observing data points see that this can viewed prior distribution which combined using bayes theorem with the likelihood function associated with data point arrive the posterior distribution after observing data points this sequential view bayesian inference very general and applies any problem which the observed data are assumed independent and identically distributed far have assumed that the variance the gaussian distribution over the data known and our goal infer the mean now let suppose that the mean known and wish infer the variance again our calculations will greatly simplified choose conjugate form for the prior distribution turns out most convenient work with the precision the likelihood function for takes the form exp probability distributions figure plot the gamma distribution gam defined for various values the parameters and the corresponding conjugate prior should therefore proportional the product power and the exponential linear function this corresponds the gamma distribution which defined gam exp exercise exercise here the gamma function that defined and that ensures that correctly normalized the gamma distribution has finite integral and the distribution itself finite plotted for various values and figure the mean and variance the gamma distribution are given var consider prior distribution gam multiply the likelihood function then obtain posterior distribution exp which recognize gamma distribution the form gam where the maximum likelihood estimator the variance note that where there need keep track the normalization constants the prior and the likelihood function because required the correct coefficient can found the end using the normalized form for the gamma distribution section the gaussian distribution from see that the effect observing data points increase the value the coefficient thus can interpret the parameter the prior terms effective prior observations similarly from see that the data points contribute the variance and can interpret the parameter the prior arising from the effective prior observations having variance recall that made analogous interpretation for the dirichlet prior these distributions are examples the exponential family and shall see that the interpretation conjugate prior terms effective fictitious data points general one for the exponential family distributions the parameter where instead working with the precision can consider the variance itself the conjugate prior this case called the inverse gamma distribution although shall not discuss this further because will find more convenient work with the precision now suppose that both the mean and the precision are unknown find conjugate prior consider the dependence the likelihood function and exp exp exp now wish identify prior distribution that has the same functional dependence and the likelihood function and that should therefore take the form exp exp exp exp where and are constants since can always write can find and inspection particular see that gaussian whose precision linear function and that gamma distribution that the normalized prior takes the form gam where have defined new constants given the distribution called the normal gamma gaussian gamma distribution and plotted figure note that this not simply the product independent gaussian prior over and gamma prior over because the precision linear function even chose prior which and were independent the posterior distribution would exhibit coupling between the precision and the value probability distributions figure contour plot the normal gamma distribution for parameter values and the case the multivariate gaussian distribution for ddimensional variable the conjugate prior distribution for the mean assuming the precision known again gaussian for known mean and unknown precision matrix the conjugate prior the wishart distribution given exercise exp where called the number degrees freedom the distribution scale matrix and denotes the trace the normalization constant given again also possible define conjugate prior over the covariance matrix itself rather than over the precision matrix which leads the inverse wishart distribution although shall not discuss this further both the mean and the precision are unknown then following similar line reasoning the univariate case the conjugate prior given which known the normal wishart gaussian wishart distribution"}, "31": {"Section": "2.3.7", "Title": "Student\u2019s t-distribution", "Content": "have seen that the conjugate prior for the precision gaussian given gamma distribution have univariate gaussian together with gamma prior gam and integrate out the precision obtain the marginal distribution the form section exercise the gaussian distribution figure plot student distribution for and for various values the limit corresponds gaussian distribution with mean and precision gam exp bae where have made the change variable convention define new parameters given and terms which the distribution takes the form which known student distribution the parameter sometimes called the precision the distribution even though not general equal the inverse the variance the parameter called the degrees freedom and its effect illustrated figure for the particular case the distribution reduces the cauchy distribution while the limit the distribution becomes gaussian with mean and precision from see that student distribution obtained adding infinite number gaussian distributions having the same mean but different precisions this can interpreted infinite mixture gaussians gaussian mixtures will discussed detail section the result distribution that general has longer tails than gaussian was seen figure this gives the tdistribution important property called robustness which means that much less sensitive than the gaussian the presence few data points which are outliers the robustness the distribution illustrated figure which compares the maximum likelihood solutions for gaussian and distribution note that the maximum likelihood solution for the distribution can found using the expectationmaximization algorithm here see that the effect small number exercise exercise probability distributions figure illustration the robustness student distribution compared gaussian histogram distribution data points drawn from gaussian distribution together with the maximum likelihood fit obtained from distribution red curve and gaussian green curve largely hidden the red curve because the distribution contains the gaussian special case gives almost the same solution the gaussian the same data set but with three additional outlying data points showing how the gaussian green curve strongly distorted the outliers whereas the distribution red curve relatively unaffected outliers much less significant for the distribution than for the gaussian outliers can arise practical applications either because the process that generates the data corresponds distribution having heavy tail simply through mislabelled data robustness also important property for regression problems unsurprisingly the least squares approach regression does not exhibit robustness because corresponds maximum likelihood under conditional gaussian distribution basing regression model heavy tailed distribution such distribution obtain more robust model back and substitute the alternative parameters and see that the distribution can written the form gam can then generalize this multivariate gaussian obtain the corresponding multivariate student distribution the form gam exercise using the same technique for the univariate case can evaluate this integral give the gaussian distribution where the dimensionality and the squared mahalanobis distance defined exercise this the multivariate form student distribution and satisfies the following properties cov mode with corresponding results for the univariate case"}, "32": {"Section": "2.3.8", "Title": "Periodic variables", "Content": "although gaussian distributions are great practical significance both their own right and building blocks for more complex probabilistic models there are situations which they are inappropriate density models for continuous variables one important case which arises practical applications that periodic variables example periodic variable would the wind direction particular geographical location might for instance measure values wind direction number days and wish summarize this using parametric distribution another example calendar time where may interested modelling quantities that are believed periodic over hours over annual cycle such quantities can conveniently represented using angular polar coordinate might tempted treat periodic variables choosing some direction the origin and then applying conventional distribution such the gaussian such approach however would give results that were strongly dependent the arbitrary choice origin suppose for instance that have two observations and and model them using standard univariate gaussian distribution choose the origin then the sample mean this data set will with standard deviation whereas choose the origin then the mean will and the standard deviation will clearly need develop special approach for the treatment periodic variables let consider the problem evaluating the mean set observations periodic variable from now shall assume that measured radians have already seen that the simple average will strongly coordinate dependent find invariant measure the mean note that the observations can viewed points the unit circle and can therefore described instead two dimensional unit vectors where for illustrated figure can average the vectors probability distributions figure illustration the representation values periodic variable twodimensional vectors living the unit circle also shown the average those vectors instead give and then find the corresponding angle this average clearly this definition will ensure that the location the mean independent the origin the angular coordinate note that will typically lie inside the unit circle the cartesian coordinates the observations are given cos sin and can write the cartesian coordinates the sample mean the form cos sin substituting into and equating the and components then gives cos cos sin sin taking the ratio and using the identity tan sin cos can solve for give tan sin cos shortly shall see how this result arises naturally the maximum likelihood estimator for appropriately defined distribution over periodic variable now consider periodic generalization the gaussian called the von mises distribution here shall limit our attention univariate distributions although periodic distributions can also found over hyperspheres arbitrary dimension for extensive discussion periodic distributions see mardia and jupp convention will consider distributions that have period any probability density defined over must not only nonnegative and integrate the gaussian distribution figure the von mises distribution can derived considering two dimensional gaussian the form whose density contours are shown blue and conditioning the unit circle shown red one but must also periodic thus must satisfy the three conditions from follows that for any integer can easily obtain gaussian like distribution that satisfies these three properties follows consider gaussian distribution over two variables having mean and covariance matrix where the identity matrix that exp the contours constant are circles illustrated figure now suppose consider the value this distribution along circle fixed radius then construction this distribution will periodic although will not normalized can determine the form this distribution transforming from cartesian coordinates polar coordinates that sin also map the mean into polar coordinates writing cos cos sin next substitute these transformations into the two dimensional gaussian distribution and then condition the unit circle noting that are interested only the dependence focussing the exponent the gaussian distribution have cos cos sin sin cos const cos cos sin sin probability distributions figure the von mises distribution plotted for two different parameter values shown cartesian plot the left and the corresponding polar plot the right exercise exercise where const denotes terms independent and have made use the following trigonometrical identities cos sin now define obtain our final expression for the distribution along the unit circle the form cos cos sin sin cos exp cos which called the von mises distribution the circular normal here the parameter corresponds the mean the distribution while which known the concentration parameter analogous the inverse variance precision for the gaussian the normalization coefficient expressed terms which the zeroth order bessel function the first kind abramowitz and stegun and defined exp cos for large the distribution becomes approximately gaussian the von mises distribution plotted figure and the function plotted figure now consider the maximum likelihood estimators for the parameters and for the von mises distribution the log likelihood function given cos the gaussian distribution figure plot the bessel function defined together with the function defined setting the derivative with respect equal zero gives sin solve for make use the trigonometric identity exercise from which obtain sin cos sin cos sin tan sin cos which recognize the result obtained earlier for the mean the observations viewed two dimensional cartesian space similarly maximizing with respect and making use abramowitz and stegun have where have substituted for the maximum likelihood solution for that are performing joint optimization over and and have defined recalling cos the function plotted figure making use the trigonometric identity can write the form mml cos cos sin sin probability distributions the left figure plots the old faithful data which the blue curves show contours constant probability density single gaussian distribution which has been fitted the data using maximum likelihood note that this distribution fails capture the two clumps the data and indeed places much its probability mass the central region between the clumps where the data are relatively sparse the right the distribution given linear combination two gaussians which has been fitted the data maximum likelihood using techniques discussed chapter and which gives better representation the data the right hand side easily evaluated and the function can inverted numerically for completeness mention briefly some alternative techniques for the construction periodic distributions the simplest approach use histogram observations which the angular coordinate divided into fixed bins this has the virtue simplicity and flexibility but also suffers from significant limitations shall see when discuss histogram methods more detail section another approach starts like the von mises distribution from gaussian distribution over euclidean space but now marginalizes onto the unit circle rather than conditioning mardia and jupp however this leads more complex forms distribution and will not discussed further finally any valid distribution over the real axis such gaussian can turned into periodic distribution mapping successive intervals width onto the periodic variable which corresponds wrapping the real axis around unit circle again the resulting distribution more complex handle than the von mises distribution one limitation the von mises distribution that unimodal forming mixtures von mises distributions obtain flexible framework for modelling periodic variables that can handle multimodality for example machine learning application that makes use von mises distributions see lawrence and for extensions modelling conditional densities for regression problems see bishop and nabney"}, "33": {"Section": "2.3.9", "Title": "Mixtures of Gaussians", "Content": "while the gaussian distribution has some important analytical properties suffers from significant limitations when comes modelling real data sets consider the example shown figure this known the old faithful data set and comprises measurements the eruption the old faithful geyser yellowstone national park the usa each measurement comprises the duration appendix the eruption minutes horizontal axis and the time minutes the next eruption vertical axis see that the data set forms two dominant clumps and that simple gaussian distribution unable capture this structure whereas linear superposition two gaussians gives better characterization the data set such superpositions formed taking linear combinations more basic distributions such gaussians can formulated probabilistic models known mixture distributions mclachlan and basford mclachlan and peel figure see that linear combination gaussians can give rise very complex densities using sufficient number gaussians and adjusting their means and covariances well the coefficients the linear combination almost any continuous density can approximated arbitrary accuracy therefore consider superposition gaussian densities the form which called mixture gaussians each gaussian density called component the mixture and has its own mean and covariance contour and surface plots for gaussian mixture having components are shown figure this section shall consider gaussian components illustrate the framework mixture models more generally mixture models can comprise linear combinations other distributions for instance section shall consider mixtures bernoulli distributions example mixture model for discrete variables the parameters are called mixing coefficients integrate both sides with respect and note that both and the individual gaussian components are normalized obtain also the requirement that together with implies for all combining this with the condition obtain the gaussian distribution figure example gaussian mixture distribution one dimension showing three gaussians each scaled coefficient blue and their sum red section probability distributions figure illustration mixture gaussians two dimensional space contours constant density for each the mixture components which the components are denoted red blue and green and the values the mixing coefficients are shown below each component contours the marginal probability density the mixture distribution surface plot the distribution therefore see that the mixing coefficients satisfy the requirements probabilities from the sum and product rules the marginal density given which equivalent which can view the prior probability picking the kth component and the density the probability conditioned shall see later chapters important role played the posterior probabilities which are also known responsibilities from bayes theorem these are given shall discuss the probabilistic interpretation the mixture distribution greater detail chapter the form the gaussian mixture distribution governed the parameters and where have used the notation and one way set the values these parameters use maximum likelihood from the log the likelihood function given"}, "34": {"Section": "2.4", "Title": "The Exponential Family", "Content": "where immediately see that the situation now much more complex than with single gaussian due the presence the summation over inside the logarithm result the maximum likelihood solution for the parameters longer has closed form analytical solution one approach maximizing the likelihood function use iterative numerical optimization techniques fletcher nocedal and wright bishop and nabney alternatively can employ powerful framework called expectation maximization which will discussed length chapter the exponential family the probability distributions that have studied far this chapter with the exception the gaussian mixture are specific examples broad class distributions called the exponential family duda and hart bernardo and smith members the exponential family have many important properties common and illuminating discuss these properties some generality the exponential family distributions over given parameters defined the set distributions the form exp where may scalar vector and may discrete continuous here are called the natural parameters the distribution and some function the function can interpreted the coefficient that ensures that the distribution normalized and therefore satisfies exp where the integration replaced summation discrete variable begin taking some examples the distributions introduced earlier the chapter and showing that they are indeed members the exponential family consider first the bernoulli distribution bern expressing the right hand side the exponential the logarithm have exp exp comparison with allows identify probability distributions which can solve for give where exp called the logistic sigmoid function thus can write the bernoulli distribution using the standard representation the form exp where have used which easily proved from comparison with shows that next consider the multinomial distribution that for single observation takes the form exp where again can write this the standard representation that where and have defined again comparing with have exp note that the parameters are not independent because the parameters are subject the constraint that given any the parameters the value the remaining parameter fixed some circumstances will convenient remove this constraint expressing the distribution terms only parameters this can achieved using the relationship eliminate expressing terms the remaining where thereby leaving parameters note that these remaining parameters are still subject the constraints exp exp exp the exponential family making use the constraint the multinomial distribution this representation then becomes exp exp exp now identify which can solve for first summing both sides over and then rearranging and back substituting give exp this called the softmax function the normalized exponential this representation the multinomial distribution therefore takes the form exp this the standard form the exponential family with parameter vector which exp finally let consider the gaussian distribution for the univariate gaussian have exp probability distributions which after some simple rearrangement can cast the standard exponential family form with exercise exercise exp"}, "35": {"Section": "2.4.1", "Title": "Maximum likelihood and sufficient statistics", "Content": "let now consider the problem estimating the parameter vector the general exponential family distribution using the technique maximum likelihood taking the gradient both sides with respect have exp exp rearranging and making use again then gives exp where have used therefore obtain the result note that the covariance can expressed terms the second derivatives and similarly for higher order moments thus provided can normalize distribution from the exponential family can always find its moments simple differentiation now consider set independent identically distributed data denoted for which the likelihood function given exp setting the gradient with respect zero get the following condition satisfied the maximum likelihood estimator the exponential family which can principle solved obtain see that the solution for the which maximum likelihood estimator depends the data only through therefore called the sufficient statistic the distribution not need store the entire data set itself but only the value the sufficient statistic for the bernoulli distribution for example the function given just and need only keep the sum the data points whereas for the gaussian and should keep both the sum and the sum consider the limit then the right hand side becomes and comparing with see that this limit will equal the true value fact this sufficiency property holds also for bayesian inference although shall defer discussion this until chapter when have equipped ourselves with the tools graphical models and can thereby gain deeper insight into these important concepts"}, "36": {"Section": "2.4.2", "Title": "Conjugate priors", "Content": "have already encountered the concept conjugate prior several times for example the context the bernoulli distribution for which the conjugate prior the beta distribution the gaussian where the conjugate prior for the mean gaussian and the conjugate prior for the precision the wishart distribution general for given probability distribution can seek prior that conjugate the likelihood function that the posterior distribution has the same functional form the prior for any member the exponential family there exists conjugate prior that can written the form exp where normalization coefficient and the same function appears see that this indeed conjugate let multiply the prior the likelihood function obtain the posterior distribution normalization coefficient the form exp this again takes the same functional form the prior confirming conjugacy furthermore see that the parameter can interpreted effective number pseudo observations the prior each which has value for the sufficient statistic given"}, "37": {"Section": "2.4.3", "Title": "Noninformative priors", "Content": "some applications probabilistic inference may have prior knowledge that can conveniently expressed through the prior distribution for example the prior assigns zero probability some value variable then the posterior distribution will necessarily also assign zero probability that value irrespective probability distributions any subsequent observations data many cases however may have little idea what form the distribution should take may then seek form prior distribution called noninformative prior which intended have little influence the posterior distribution possible jeffries box and tao bernardo and smith this sometimes referred letting the data speak for themselves have distribution governed parameter might tempted propose prior distribution const suitable prior discrete variable with states this simply amounts setting the prior probability each state the case continuous parameters however there are two potential difficulties with this approach the first that the domain unbounded this prior distribution cannot correctly normalized because the integral over diverges such priors are called improper practice improper priors can often used provided the corresponding posterior distribution proper that can correctly normalized for instance put uniform prior distribution over the mean gaussian then the posterior distribution for the mean once have observed least one data point will proper second difficulty arises from the transformation behaviour probability density under nonlinear change variables given function constant and change variables then will also constant however choose the density constant then the density will given from and the density over will not constant this issue does not arise when use maximum likelihood because the likelihood function simple function and are free use any convenient parameterization however are choose prior distribution that constant must take care use appropriate representation for the parameters here consider two simple examples noninformative priors berger first all density takes the form then the parameter known location parameter this family densities exhibits translation invariance because shift constant give then thus the density takes the same form the where have defined new variable the original one and the density independent the choice origin would like choose prior distribution that reflects this translation invariance property and choose prior that assigns equal probability mass the exponential family interval the shifted interval this implies and because this must hold for all choices and have which implies that constant example location parameter would the mean gaussian distribution have seen the conjugate prior distribution for this case gaussian and obtain noninformative prior taking the limit indeed from and see that this gives posterior distribution over which the contributions from the prior vanish second example consider density the form exercise where note that this will normalized density provided correctly normalized the parameter known scale parameter and the density exhibits scale invariance because scale constant give then this transformation corresponds change where have defined scale for example from meters kilometers length and would like choose prior distribution that reflects this scale invariance consider interval and scaled interval then the prior should assign equal probability mass these two intervals thus have and because this must hold for choices and have and hence note that again this improper prior because the integral the distribution over divergent sometimes also convenient think the prior distribution for scale parameter terms the density the log the parameter using the transformation rule for densities see that const thus for this prior there the same probability mass the range the range and section probability distributions example scale parameter would the standard deviation gaussian distribution after have taken account the location parameter because exp discussed earlier often more convenient work terms where the precision rather than itself using the transformation rule for densities see that distribution corresponds distribution over the form have seen that the conjugate prior for was the gamma distribution gam given the noninformative prior obtained the special case again examine the results and for the posterior distribution see that for the posterior depends only terms arising from the data and not from the prior"}, "38": {"Section": "2.5", "Title": "Nonparametric Methods", "Content": "throughout this chapter have focussed the use probability distributions having specific functional forms governed small number parameters whose values are determined from data set this called the parametric approach density modelling important limitation this approach that the chosen density might poor model the distribution that generates the data which can result poor predictive performance for instance the process that generates the data multimodal then this aspect the distribution can never captured gaussian which necessarily unimodal this final section consider some nonparametric approaches density estimation that make few assumptions about the form the distribution here shall focus mainly simple frequentist methods the reader should aware however that nonparametric bayesian methods are attracting increasing interest walker neal uller and quintana teh let start with discussion histogram methods for density estimation which have already encountered the context marginal and conditional distributions figure and the context the central limit theorem figure here explore the properties histogram density models more detail focussing the case single continuous variable standard histograms simply partition into distinct bins width and then count the number observations falling bin order turn this count into normalized probability density simply divide the total number observations and the width the bins obtain probability values for each bin given this gives model for the density for which easily seen that that constant over the width each bin and often the bins are chosen have the same width figure illustration the histogram approach density estimation which data set data points generated from the distribution shown the green curve histogram density estimates based with common bin width are shown for various values nonparametric methods figure show example histogram density estimation here the data drawn from the distribution corresponding the green curve which formed from mixture two gaussians also shown are three examples histogram density estimates corresponding three different choices for the bin width see that when very small top figure the resulting density model very spiky with lot structure that not present the underlying distribution that generated the data set conversely too large bottom figure then the result model that too smooth and that consequently fails capture the bimodal property the green curve the best results are obtained for some intermediate value middle figure principle histogram density model also dependent the choice edge location for the bins though this typically much less significant than the value note that the histogram method has the property unlike the methods discussed shortly that once the histogram has been computed the data set itself can discarded which can advantageous the data set large also the histogram approach easily applied the data points are arriving sequentially practice the histogram technique can useful for obtaining quick visualization data one two dimensions but unsuited most density estimation applications one obvious problem that the estimated density has discontinuities that are due the bin edges rather than any property the underlying distribution that generated the data another major limitation the histogram approach its scaling with dimensionality divide each variable dimensional space into bins then the total number bins will this exponential scaling with example the curse dimensionality space high dimensionality the quantity data needed provide meaningful estimates local probability density would prohibitive the histogram approach density estimation does however teach two important lessons first estimate the probability density particular location should consider the data points that lie within some local neighbourhood that point note that the concept locality requires that assume some form distance measure and here have been assuming euclidean distance for histograms section probability distributions this neighbourhood property was defined the bins and there natural smoothing parameter describing the spatial extent the local region this case the bin width second the value the smoothing parameter should neither too large nor too small order obtain good results this reminiscent the choice model complexity polynomial curve fitting discussed chapter where the degree the polynomial alternatively the value the regularization parameter was optimal for some intermediate value neither too large nor too small armed with these insights turn now discussion two widely used nonparametric techniques for density estimation kernel estimators and nearest neighbours which have better scaling with dimensionality than the simple histogram model"}, "39": {"Section": "2.5.1", "Title": "Kernel density estimators", "Content": "let suppose that observations are being drawn from some unknown probability density some dimensional space which shall take euclidean and wish estimate the value from our earlier discussion locality let consider some small region containing the probability mass associated with this region given section now suppose that have collected data set comprising observations drawn from because each data point has probability falling within the total number points that lie inside will distributed according the binomial distribution bin using see that the mean fraction points falling inside the region and similarly using see that the variance around this mean var for large this distribution will sharply peaked around the mean and however also assume that the region sufficiently small that the probability density roughly constant over the region then have where the volume combining and obtain our density estimate the form note that the validity depends two contradictory assumptions namely that the region sufficiently small that the density approximately constant over the region and yet sufficiently large relation the value that density that the number points falling inside the region sufficient for the binomial distribution sharply peaked nonparametric methods can exploit the result two different ways either can fix and determine the value from the data which gives rise the nearest neighbour technique discussed shortly can fix and determine from the data giving rise the kernel approach can shown that both the nearest neighbour density estimator and the kernel density estimator converge the true probability density the limit provided shrinks suitably with and grows with duda and hart begin discussing the kernel method detail and start with take the region small hypercube centred the point which wish determine the probability density order count the number points falling within this region convenient define the following function otherwise which represents unit cube centred the origin the function example kernel function and this context also called parzen window from the quantity will one the data point lies inside cube side centred and zero otherwise the total number data points lying inside this cube will therefore substituting this expression into then gives the following result for the estimated density where have used for the volume hypercube side dimensions using the symmetry the function can now interpret this equation not single cube centred but the sum over cubes centred the data points stands the kernel density estimator will suffer from one the same problems that the histogram method suffered from namely the presence artificial discontinuities this case the boundaries the cubes can obtain smoother density model choose smoother kernel function and common choice the gaussian which gives rise the following kernel density model exp where represents the standard deviation the gaussian components thus our density model obtained placing gaussian over each data point and then adding the contributions over the whole data set and then dividing that the density correctly normalized figure apply the model the data probability distributions figure illustration the kernel density model applied the same data set used demonstrate the histogram approach figure see that acts smoothing parameter and that set too small top panel the result very noisy density model whereas set too large bottom panel then the bimodal nature the underlying distribution from which the data generated shown the green curve washed out the best density model obtained for some intermediate value middle panel set used earlier demonstrate the histogram technique see that expected the parameter plays the role smoothing parameter and there trade off between sensitivity noise small and over smoothing large again the optimization problem model complexity analogous the choice bin width histogram density estimation the degree the polynomial used curve fitting can choose any other kernel function subject the conditions which ensure that the resulting probability distribution nonnegative everywhere and integrates one the class density model given called kernel density estimator parzen estimator has great merit that there computation involved the training phase because this simply requires storage the training set however this also one its great weaknesses because the computational cost evaluating the density grows linearly with the size the data set"}, "40": {"Section": "2.5.2", "Title": "Nearest-neighbour methods", "Content": "one the difficulties with the kernel approach density estimation that the parameter governing the kernel width fixed for all kernels regions high data density large value may lead over smoothing and washing out structure that might otherwise extracted from the data however reducing may lead noisy estimates elsewhere data space where the density smaller thus the optimal choice for may dependent location within the data space this issue addressed nearest neighbour methods for density estimation therefore return our general result for local density estimation and instead fixing and determining the value from the data consider fixed value and use the data find appropriate value for this consider small sphere centred the point which wish estimate the nonparametric methods figure illustration nearest neighbour density estimation using the same data set figures and see that the parameter governs the degree smoothing that small value leads very noisy density model top panel whereas large value bottom panel smoothes out the bimodal nature the true distribution shown the green curve from which the data set was generated exercise density and allow the radius the sphere grow until contains precisely data points the estimate the density then given with set the volume the resulting sphere this technique known nearest neighbours and illustrated figure for various choices the parameter using the same data set used figure and figure see that the value now governs the degree smoothing and that again there optimum choice for that neither too large nor too small note that the model produced nearest neighbours not true density model because the integral over all space diverges close this chapter showing how the nearest neighbour technique for density estimation can extended the problem classification this apply the nearest neighbour density estimation technique each class separately and then make use bayes theorem let suppose that have data set comk prising points class with points total that wish classify new point draw sphere centred containing precisely points irrespective their class suppose this sphere has volume and contains points from class then provides estimate the density associated with each class nkv similarly the unconditional density given while the class priors are given can now combine and using bayes theorem obtain the posterior probability class membership probability distributions figure the nearestneighbour classifier new point shown the black diamond classified according the majority class membership the closest training data points this case the nearest neighbour approach classification the resulting decision boundary composed hyperplanes that form perpendicular bisectors pairs points from different classes wish minimize the probability misclassification this done assigning the test point the class having the largest posterior probability corresponding the largest value thus classify new point identify the nearest points from the training data set and then assign the new point the class having the largest number representatives amongst this set ties can broken random the particular case called the nearest neighbour rule because test point simply assigned the same class the nearest point from the training set these concepts are illustrated figure figure show the results applying the nearest neighbour algorithm the oil flow data introduced chapter for various values expected see that controls the degree smoothing that small produces many small regions each class whereas large leads fewer larger regions figure plot data points from the oil data set showing values plotted against where the red green and blue points correspond the laminar annular and homogeneous classes respectively also shown are the classifications the input space given the nearest neighbour algorithm for various values exercises interesting property the nearest neighbour classifier that the limit the error rate never more than twice the minimum achievable error rate optimal classifier one that uses the true class distributions cover and hart discussed far both the nearest neighbour method and the kernel density estimator require the entire training data set stored leading expensive computation the data set large this effect can offset the expense some additional one off computation constructing tree based search structures allow approximate near neighbours found efficiently without doing exhaustive search the data set nevertheless these nonparametric methods are still severely limited the other hand have seen that simple parametric models are very restricted terms the forms distribution that they can represent therefore need find density models that are very flexible and yet for which the complexity the models can controlled independently the size the training set and shall see subsequent chapters how achieve this linear models for regression the focus far this book has been unsupervised learning including topics such density estimation and data clustering turn now discussion supervised learning starting with regression the goal regression predict the value one more continuous target variables given the value dimensional vector input variables have already encountered example regression problem when considered polynomial curve fitting chapter the polynomial specific example broad class functions called linear regression models which share the property being linear functions the adjustable parameters and which will form the focus this chapter the simplest form linear regression models are also linear functions the input variables however can obtain much more useful class functions taking linear combinations fixed set nonlinear functions the input variables known basis functions such models are linear functions the parameters which gives them simple analytical properties and yet can nonlinear with respect the input variables"}, "41": {"Section": "3.", "Title": "Linear Models for Regression", "Content": "given training data set comprising observations where together with corresponding target values the goal predict the value for new value the simplest approach this can done directly constructing appropriate function whose values for new inputs constitute the predictions for the corresponding values more generally from probabilistic perspective aim model the predictive distribution because this expresses our uncertainty about the value for each value from this conditional distribution can make predictions for any new value such way minimize the expected value suitably chosen loss function discussed section common choice loss function for real valued variables the squared loss for which the optimal solution given the conditional expectation although linear models have significant limitations practical techniques for pattern recognition particularly for problems involving input spaces high dimensionality they have nice analytical properties and form the foundation for more sophisticated models discussed later chapters"}, "42": {"Section": "3.1", "Title": "Linear Basis Function Models", "Content": "the simplest linear model for regression one that involves linear combination the input variables wdxd where this often simply known linear regression the key property this model that linear function the parameters also however linear function the input variables and this imposes significant limitations the model therefore extend the class models considering linear combinations fixed nonlinear functions the input variables the form where are known basis functions denoting the maximum value the index the total number parameters this model will the parameter allows for any fixed offset the data and sometimes called bias parameter not confused with bias statistical sense often convenient define additional dummy basis function that where and many practical applications pattern recognition will apply some form fixed pre processing linear basis function models feature extraction the original data variables the original variables comprise the vector then the features can expressed terms the basis functions using nonlinear basis functions allow the function nonlinear function the input vector functions the form are called linear models however because this function linear this linearity the parameters that will greatly simplify the analysis this class models however also leads some significant limitations discuss section the example polynomial regression considered chapter particular example this model which there single input variable and the basis functions take the form powers that one limitation polynomial basis functions that they are global functions the input variable that changes one region input space affect all other regions this can resolved dividing the input space into regions and fit different polynomial each region leading spline functions hastie there are many other possible choices for the basis functions for example exp where the govern the locations the basis functions input space and the parameter governs their spatial scale these are usually referred gaussian basis functions although should noted that they are not required have probabilistic interpretation and particular the normalization coefficient unimportant because these basis functions will multiplied adaptive parameters another possibility the sigmoidal basis function the form where the logistic sigmoid function defined exp equivalently can use the tanh function because this related the logistic sigmoid tanh and general linear combination logistic sigmoid functions equivalent general linear combination tanh functions these various choices basis function are illustrated figure yet another possible choice basis function the fourier basis which leads expansion sinusoidal functions each basis function represents specific frequency and has infinite spatial extent contrast basis functions that are localized finite regions input space necessarily comprise spectrum different spatial frequencies many signal processing applications interest consider basis functions that are localized both space and frequency leading class functions known wavelets these are also defined mutually orthogonal simplify their application wavelets are most applicable when the input values live linear models for regression figure examples basis functions showing polynomials the left gaussians the form the centre and sigmoidal the form the right regular lattice such the successive time points temporal sequence the pixels image useful texts wavelets include ogden mallat and vidakovic most the discussion this chapter however independent the particular choice basis function set and for most our discussion shall not specify the particular form the basis functions except for the purposes numerical illustration indeed much our discussion will equally applicable the situation which the vector basis functions simply the identity furthermore order keep the notation simple shall focus the case single target variable however section consider briefly the modifications needed deal with multiple target variables"}, "43": {"Section": "3.1.1", "Title": "Maximum likelihood and least squares", "Content": "chapter fitted polynomial functions data sets minimizing sumof squares error function also showed that this error function could motivated the maximum likelihood solution under assumed gaussian noise model let return this discussion and consider the least squares approach and its relation maximum likelihood more detail before assume that the target variable given deterministic function with additive gaussian noise that where zero mean gaussian random variable with precision inverse variance thus can write section recall that assume squared loss function then the optimal prediction for new value will given the conditional mean the target variable the case gaussian conditional distribution the form the conditional mean linear basis function models will simply note that the gaussian noise assumption implies that the conditional distribution given unimodal which may inappropriate for some applications extension mixtures conditional gaussian distributions which permit multimodal conditional distributions will discussed section now consider data set inputs with corresponding target values group the target variables into column vector that denote where the typeface chosen distinguish from single observation multivariate target which would denoted making the assumption that these data points are drawn independently from the distribution obtain the following expression for the likelihood function which function the adjustable parameters and the form where have used note that supervised learning problems such regression and classification are not seeking model the distribution the input variables thus will always appear the set conditioning variables and from now will drop the explicit from expressions such order keep the notation uncluttered taking the logarithm the likelihood function and making use the standard form for the univariate gaussian have lnn where the sum squares error function defined having written down the likelihood function can use maximum likelihood determine and consider first the maximization with respect observed already section see that maximization the likelihood function under conditional gaussian noise distribution for linear model equivalent minimizing sum squares error function given the gradient the log likelihood function takes the form linear models for regression setting this gradient zero gives solving for obtain which are known the normal equations for the least squares problem here matrix called the design matrix whose elements are given that wml the quantity known the moore penrose pseudo inverse the matrix rao and mitra golub and van loan can regarded generalization the notion matrix inverse nonsquare matrices indeed square and invertible then using the property see that make the bias parameter explicit then the error function becomes this point can gain some insight into the role the bias parameter setting the derivative with respect equal zero and solving for obtain where have defined thus the bias compensates for the difference between the averages over the training set the target values and the weighted sum the averages the basis function values can also maximize the log likelihood function with respect the noise precision parameter giving linear basis function models figure geometrical interpretation the least squares solution dimensional space whose axes are the values the least squares regression function obtained finding the orthogonal projection the data vector onto the subspace spanned the basis functions which each basis function viewed vector length with elements and see that the inverse the noise precision given the residual variance the target values around the regression function"}, "44": {"Section": "3.1.2", "Title": "Geometry of least squares", "Content": "this point instructive consider the geometrical interpretation the least squares solution this consider dimensional space whose axes are given the that vector this space each basis function evaluated the data points can also represented vector the same space denoted illustrated figure note that corresponds the jth column whereas corresponds the nth row the number basis functions smaller than the number data points then the vectors will span linear subspace dimensionality define dimensional vector whose nth element given where because arbitrary linear combination the vectors can live anywhere the dimensional subspace the sum squares error then equal factor the squared euclidean distance between and thus the least squares solution for corresponds that choice that lies subspace and that closest intuitively from figure anticipate that this solution corresponds the orthogonal projection onto the subspace this indeed the case can easily verified noting that the solution for given wml and then confirming that this takes the form orthogonal projection practice direct solution the normal equations can lead numerical difficulties when close singular particular when two more the basis vectors are linear nearly the resulting parameter values can have large magnitudes such near degeneracies will not uncommon when dealing with real data sets the resulting numerical difficulties can addressed using the technique singular value decomposition svd press bishop and nabney note that the addition regularization term ensures that the matrix nonsingular even the presence degeneracies"}, "45": {"Section": "3.1.3", "Title": "Sequential learning", "Content": "batch techniques such the maximum likelihood solution which involve processing the entire training set one can computationally costly for large data sets have discussed chapter the data set sufficiently large may worthwhile use sequential algorithms also known line algorithms exercise linear models for regression which the data points are considered one time and the model parameters updated after each such presentation sequential learning also appropriate for realtime applications which the data observations are arriving continuous stream and predictions must made before all the data points are seen can obtain sequential learning algorithm applying the technique stochastic gradient descent also known sequential gradient descent follows the error function comprises sum over data points then after presentation pattern the stochastic gradient descent algorithm updates the parameter vector using where denotes the iteration number and learning rate parameter shall discuss the choice value for shortly the value initialized some starting vector for the case the sum squares error function this gives where this known least mean squares the lms algorithm the value needs chosen with care ensure that the algorithm converges bishop and nabney"}, "46": {"Section": "3.1.4", "Title": "Regularized least squares", "Content": "section introduced the idea adding regularization term error function order control over fitting that the total error function minimized takes the form where the regularization coefficient that controls the relative importance the data dependent error and the regularization term one the simplest forms regularizer given the sum squares the weight vector elements also consider the sum squares error function given wtw then the total error function becomes wtw this particular choice regularizer known the machine learning literature weight decay because sequential learning algorithms encourages weight values decay towards zero unless supported the data statistics provides example parameter shrinkage method because shrinks parameter values towards linear basis function models figure contours the regularization term for various values the parameter zero has the advantage that the error function remains quadratic function and its exact minimizer can found closed form specifically setting the gradient with respect zero and solving for before obtain this represents simple extension the least squares solution more general regularizer sometimes used for which the regularized error takes the form where corresponds the quadratic regularizer figure shows contours the regularization function for different values the case know the lasso the statistics literature tibshirani has the property that sufficiently large some the coefficients are driven zero leading sparse model which the corresponding basis functions play role see this first note that minimizing equivalent minimizing the unregularized sum squares error subject the constraint for appropriate value the parameter where the two approaches can related using lagrange multipliers the origin the sparsity can seen from figure which shows that the minimum the error function subject the constraint increased increasing number parameters are driven zero regularization allows complex models trained data sets limited size without severe over fitting essentially limiting the effective model complexity however the problem determining the optimal model complexity then shifted from one finding the appropriate number basis functions one determining suitable value the regularization coefficient shall return the issue model complexity later this chapter exercise appendix linear models for regression figure plot the contours the unregularized error function blue along with the constraint region for the quadratic regularizer the left and the lasso regularizer the right which the optimum value for the parameter vector denoted the lasso gives sparse solution which for the remainder this chapter shall focus the quadratic regularizer both for its practical importance and its analytical tractability"}, "47": {"Section": "3.1.5", "Title": "Multiple outputs", "Content": "far have considered the case single target variable some applications may wish predict target variables which denote collectively the target vector this could done introducing different set basis functions for each component leading multiple independent regression problems however more interesting and more common approach use the same set basis functions model all the components the target vector that where dimensional column vector matrix parameters and dimensional column vector with elements with before suppose take the conditional distribution the target vector isotropic gaussian the form have set observations can combine these into matrix size such that the nth row given similarly can combine the input vectors into matrix the log likelihood function then given lnn"}, "48": {"Section": "3.2", "Title": "The Bias-Variance Decomposition", "Content": "before can maximize this function with respect giving wml examine this result for each target variable have ttk exercise where dimensional column vector with components tnk for thus the solution the regression problem decouples between the different target variables and need only compute single pseudo inverse matrix which shared all the vectors the extension general gaussian noise distributions having arbitrary covariance matrices straightforward again this leads decoupling into independent regression problems this result unsurprising because the parameters define only the mean the gaussian noise distribution and know from section that the maximum likelihood solution for the mean multivariate gaussian independent the covariance from now shall therefore consider single target variable for simplicity the bias variance decomposition far our discussion linear models for regression have assumed that the form and number basis functions are both fixed have seen chapter the use maximum likelihood equivalently least squares can lead severe over fitting complex models are trained using data sets limited size however limiting the number basis functions order avoid over fitting has the side effect limiting the flexibility the model capture interesting and important trends the data although the introduction regularization terms can control over fitting for models with many parameters this raises the question how determine suitable value for the regularization coefficient seeking the solution that minimizes the regularized error function with respect both the weight vector and the regularization coefficient clearly not the right approach since this leads the unregularized solution with have seen earlier chapters the phenomenon over fitting really unfortunate property maximum likelihood and does not arise when marginalize over parameters bayesian setting this chapter shall consider the bayesian view model complexity some depth before doing however instructive consider frequentist viewpoint the model complexity issue known the biasvariance trade off although shall introduce this concept the context linear basis function models where easy illustrate the ideas using simple examples the discussion has more general applicability section when discussed decision theory for regression problems considered various loss functions each which leads corresponding optimal prediction once are given the conditional distribution popular choice linear models for regression the squared loss function for which the optimal prediction given the conditional expectation which denote and which given this point worth distinguishing between the squared loss function arising from decision theory and the sum squares error function that arose the maximum likelihood estimation model parameters might use more sophisticated techniques than least squares for example regularization fully bayesian approach determine the conditional distribution these can all combined with the squared loss function for the purpose making predictions showed section that the expected squared loss can written the form recall that the second term which independent arises from the intrinsic noise the data and represents the minimum achievable value the expected loss the first term depends our choice for the function and will seek solution for which makes this term minimum because nonnegative the smallest that can hope make this term zero had unlimited supply data and unlimited computational resources could principle find the regression function any desired degree accuracy and this would represent the optimal choice for however practice have data set containing only finite number data points and consequently not know the regression function exactly model the using parametric function governed parameter vector then from bayesian perspective the uncertainty our model expressed through posterior distribution over frequentist treatment however involves making point estimate based the data set and tries instead interpret the uncertainty this estimate through the following thought experiment suppose had large number data sets each size and each drawn independently from the distribution for any given data set can run our learning algorithm and obtain prediction function different data sets from the ensemble will give different functions and consequently different values the squared loss the performance particular learning algorithm then assessed taking the average over this ensemble data sets consider the integrand the first term which for particular data set takes the form because this quantity will dependent the particular data set take its average over the ensemble data sets add and subtract the quantity the bias variance decomposition inside the braces and then expand obtain now take the expectation this expression with respect and note that the final term will vanish giving bias variance see that the expected squared difference between and the regression function can expressed the sum two terms the first term called the squared bias represents the extent which the average prediction over all data sets differs from the desired regression function the second term called the variance measures the extent which the solutions for individual data sets vary around their average and hence this measures the extent which the function sensitive the particular choice data set shall provide some intuition support these definitions shortly when consider simple example far have considered single input value substitute this expansion back into obtain the following decomposition the expected squared loss expected loss bias variance noise where bias variance noise and the bias and variance terms now refer integrated quantities our goal minimize the expected loss which have decomposed into the sum squared bias variance and constant noise term shall see there trade off between bias and variance with very flexible models having low bias and high variance and relatively rigid models having high bias and low variance the model with the optimal predictive capability the one that leads the best balance between bias and variance this illustrated considering the sinusoidal data set from chapter here generate data sets each containing data points independently from the sinusoidal curve sin the data sets are indexed where and for each data set appendix linear models for regression figure illustration the dependence bias and variance model complexity governed regularization parameter using the sinusoidal data set from chapter there are data sets each having data points and there are gaussian basis functions the model that the total number parameters including the bias parameter the left column shows the result fitting the model the data sets for various values for clarity only the fits are shown the right column shows the corresponding average the fits red along with the sinusoidal function from which the data sets were generated green figure plot squared bias and variance together with their sum corresponding the results shown figure also shown the average test set error for test data set size points the minimum value bias variance occurs around which close the value that gives the minimum error the test data the bias variance decomposition bias variance bias variance test error fit model with gaussian basis functions minimizing the regularized error function give prediction function shown figure the top row corresponds large value the regularization coefficient that gives low variance because the red curves the left plot look similar but high bias because the two curves the right plot are very different conversely the bottom row for which small there large variance shown the high variability between the red curves the left plot but low bias shown the good fit between the average model fit and the original sinusoidal function note that the result averaging many solutions for the complex model with very good fit the regression function which suggests that averaging may beneficial procedure indeed weighted averaging multiple solutions lies the heart bayesian approach although the averaging with respect the posterior distribution parameters not with respect multiple data sets can also examine the bias variance trade off quantitatively for this example the average prediction estimated from and the integrated squared bias and integrated variance are then given bias variance where the integral over weighted the distribution approximated finite sum over data points drawn from that distribution these quantities along with their sum are plotted function figure see that small values allow the model become finely tuned the noise each individual linear models for regression data set leading large variance conversely large value pulls the weight parameters towards zero leading large bias although the bias variance decomposition may provide some interesting insights into the model complexity issue from frequentist perspective limited practical value because the bias variance decomposition based averages with respect ensembles data sets whereas practice have only the single observed data set had large number independent training sets given size would better off combining them into single large training set which course would reduce the level over fitting for given model complexity given these limitations turn the next section bayesian treatment linear basis function models which not only provides powerful insights into the issues over fitting but which also leads practical techniques for addressing the question model complexity"}, "49": {"Section": "3.3", "Title": "Bayesian Linear Regression", "Content": "our discussion maximum likelihood for setting the parameters linear regression model have seen that the effective model complexity governed the number basis functions needs controlled according the size the data set adding regularization term the log likelihood function means the effective model complexity can then controlled the value the regularization coefficient although the choice the number and form the basis functions course still important determining the overall behaviour the model this leaves the issue deciding the appropriate model complexity for the particular problem which cannot decided simply maximizing the likelihood function because this always leads excessively complex models and over fitting independent hold out data can used determine model complexity discussed section but this can both computationally expensive and wasteful valuable data therefore turn bayesian treatment linear regression which will avoid the over fitting problem maximum likelihood and which will also lead automatic methods determining model complexity using the training data alone again for simplicity will focus the case single target variable extension multiple target variables straightforward and follows the discussion section"}, "50": {"Section": "3.3.1", "Title": "Parameter distribution", "Content": "begin our discussion the bayesian treatment linear regression introducing prior probability distribution over the model parameters for the moment shall treat the noise precision parameter known constant first note that the likelihood function defined the exponential quadratic function the corresponding conjugate prior therefore given gaussian distribution the form having mean and covariance bayesian linear regression next compute the posterior distribution which proportional the product the likelihood function and the prior due the choice conjugate gaussian prior distribution the posterior will also gaussian can evaluate this distribution the usual procedure completing the square the exponential and then finding the normalization coefficient using the standard result for normalized gaussian however have already done the necessary work deriving the general result which allows write down the posterior distribution directly the form where note that because the posterior distribution gaussian its mode coincides with its mean thus the maximum posterior weight vector simply given wmap consider infinitely broad prior with the mean the posterior distribution reduces the maximum likelihood value wml given similarly then the posterior distribution reverts the prior furthermore data points arrive sequentially then the posterior distribution any stage acts the prior distribution for the subsequent data point such that the new posterior distribution again given for the remainder this chapter shall consider particular form gaussian prior order simplify the treatment specifically consider zero mean isotropic gaussian governed single precision parameter that and the corresponding posterior distribution over then given with exercise exercise the log the posterior distribution given the sum the log likelihood and the log the prior and function takes the form wtw const maximization this posterior distribution with respect therefore equivalent the minimization the sum squares error function with the addition quadratic regularization term corresponding with can illustrate bayesian learning linear basis function model well the sequential update posterior distribution using simple example involving straight line fitting consider single input variable single target variable and linear models for regression linear model the form because this has just two adaptive parameters can plot the prior and posterior distributions directly parameter space generate synthetic data from the function with parameter values and first choosing values from the uniform distribution then evaluating and finally adding gaussian noise with standard deviation obtain the target values our goal recover the values and from such data and will explore the dependence the size the data set assume here that the noise variance known and hence set the precision parameter its true value similarly fix the parameter shall shortly discuss strategies for determining and from the training data figure shows the results bayesian learning this model the size the data set increased and demonstrates the sequential nature bayesian learning which the current posterior distribution forms the prior when new data point observed worth taking time study this figure detail illustrates several important aspects bayesian inference the first row this figure corresponds the situation before any data points are observed and shows plot the prior distribution space together with six samples the function which the values are drawn from the prior the second row see the situation after observing single data point the location the data point shown blue circle the right hand column the left hand column plot the likelihood function for this data point function note that the likelihood function provides soft constraint that the line must pass close the data point where close determined the noise precision for comparison the true parameter values and used generate the data set are shown white cross the plots the left column figure when multiply this likelihood function the prior from the top row and normalize obtain the posterior distribution shown the middle plot the second row samples the regression function obtained drawing samples from this posterior distribution are shown the right hand plot note that these sample lines all pass close the data point the third row this figure shows the effect observing second data point again shown blue circle the plot the right hand column the corresponding likelihood function for this second data point alone shown the left plot when multiply this likelihood function the posterior distribution from the second row obtain the posterior distribution shown the middle plot the third row note that this exactly the same posterior distribution would obtained combining the original prior with the likelihood function for the two data points this posterior has now been influenced two data points and because two points are sufficient define line this already gives relatively compact posterior distribution samples from this posterior distribution give rise the functions shown red the third column and see that these functions pass close both the data points the fourth row shows the effect observing total data points the left hand plot shows the likelihood function for the data point alone and the middle plot shows the resulting posterior distribution that has now absorbed information from all observations note how the posterior much sharper than the third row the limit infinite number data points the bayesian linear regression figure illustration sequential bayesian learning for simple linear model the form detailed description this figure given the text linear models for regression posterior distribution would become delta function centred the true parameter values shown the white cross other forms prior over the parameters can considered for instance can generalize the gaussian prior give exp which corresponds the gaussian distribution and only this case the prior conjugate the likelihood function finding the maximum the posterior distribution over corresponds minimization the regularized error function the case the gaussian prior the mode the posterior distribution was equal the mean although this will longer hold"}, "51": {"Section": "3.3.2", "Title": "Predictive distribution", "Content": "practice are not usually interested the value itself but rather making predictions for new values this requires that evaluate the predictive distribution defined which the vector target values from the training set and have omitted the corresponding input vectors from the right hand side the conditioning statements simplify the notation the conditional distribution the target variable given and the posterior weight distribution given see that involves the convolution two gaussian distributions and making use the result from section see that the predictive distribution takes the form where the variance the predictive distribution given tsn the first term represents the noise the data whereas the second term reflects the uncertainty associated with the parameters because the noise process and the distribution are independent gaussians their variances are additive note that additional data points are observed the posterior distribution becomes narrower consequence can shown qazaz that the limit the second term goes zero and the variance the predictive distribution arises solely from the additive noise governed the parameter illustration the predictive distribution for bayesian linear regression models let return the synthetic sinusoidal data set section figure exercise exercise bayesian linear regression figure examples the predictive distribution for model consisting gaussian basis functions the form using the synthetic sinusoidal data set section see the text for detailed discussion fit model comprising linear combination gaussian basis functions data sets various sizes and then look the corresponding posterior distributions here the green curves correspond the function sin from which the data points were generated with the addition gaussian noise data sets size and are shown the four plots the blue circles for each plot the red curve shows the mean the corresponding gaussian predictive distribution and the red shaded region spans one standard deviation either side the mean note that the predictive uncertainty depends and smallest the neighbourhood the data points also note that the level uncertainty decreases more data points are observed the plots figure only show the point wise predictive variance function order gain insight into the covariance between the predictions different values can draw samples from the posterior distribution over and then plot the corresponding functions shown figure linear models for regression figure plots the function using samples from the posterior distributions over corresponding the plots figure used localized basis functions such gaussians then regions away from the basis function centres the contribution from the second term the predictive variance will zero leaving only the noise contribution thus the model becomes very confident its predictions when extrapolating outside the region occupied the basis functions which generally undesirable behaviour this problem can avoided adopting alternative bayesian approach regression known gaussian process note that both and are treated unknown then can introduce conjugate prior distribution that from the discussion section will given gaussian gamma distribution denison this case the predictive distribution student distribution section exercise exercise bayesian linear regression figure the equivalent kernel for the gaussian basis functions figure shown plot versus together with three slices through this matrix corresponding three different values the data set used generate this kernel comprised values equally spaced over the interval"}, "52": {"Section": "3.3.3", "Title": "Equivalent kernel", "Content": "the posterior mean solution for the linear basis function model has interesting interpretation that will set the stage for kernel methods including gaussian processes substitute into the expression see that the predictive mean can written the form chapter tsn tsn where defined thus the mean the predictive distribution point given linear combination the training set target variables that can write where the function tsn known the smoother matrix the equivalent kernel regression functions such this which make predictions taking linear combinations the training set target values are known linear smoothers note that the equivalent kernel depends the input values from the data set because these appear the definition the equivalent kernel illustrated for the case gaussian basis functions figure which the kernel functions have been plotted function for three different values see that they are localized around and the mean the predictive distribution given obtained forming weighted combination the target values which data points close are given higher weight than points further removed from intuitively seems reasonable that should weight local evidence more strongly than distant evidence note that this localization property holds not only for the localized gaussian basis functions but also for the nonlocal polynomial and sigmoidal basis functions illustrated figure linear models for regression figure examples equivalent kernels for plotted function corresponding left the polynomial basis functions and right the sigmoidal basis functions shown figure note that these are localized functions even though the corresponding basis functions are nonlocal further insight into the role the equivalent kernel can obtained considering the covariance between and which given cov cov tsn where have made use and from the form the equivalent kernel see that the predictive mean nearby points will highly correlated whereas for more distant pairs points the correlation will smaller the predictive distribution shown figure allows visualize the pointwise uncertainty the predictions governed however drawing samples from the posterior distribution over and plotting the corresponding model functions figure are visualizing the joint uncertainty the posterior distribution between the values two more values governed the equivalent kernel the formulation linear regression terms kernel function suggests alternative approach regression follows instead introducing set basis functions which implicitly determines equivalent kernel can instead define localized kernel directly and use this make predictions for new input vectors given the observed training set this leads practical framework for regression and classification called gaussian processes which will discussed detail section have seen that the effective kernel defines the weights which the training set target values are combined order make prediction new value and can shown that these weights sum one other words exercise for all values this intuitively pleasing result can easily proven informally noting that the summation equivalent considering the predictive mean for set target data which for all provided the basis functions are linearly independent that there are more data points than basis functions and that one the basis functions constant corresponding the bias parameter then clear that can fit the training data exactly and hence that the predictive mean will"}, "53": {"Section": "3.4", "Title": "Bayesian Model Comparison", "Content": "from which obtain note that the kernel function can simply negative well positive although satisfies summation constraint the corresponding predictions are not necessarily convex combinations the training set target variables chapter finally note that the equivalent kernel satisfies important property shared kernel functions general namely that can expressed the form inner product with respect vector nonlinear functions that where bayesian model comparison chapter highlighted the problem over fitting well the use crossvalidation technique for setting the values regularization parameters for choosing between alternative models here consider the problem model selection from bayesian perspective this section our discussion will very general and then section shall see how these ideas can applied the determination regularization parameters linear regression shall see the over fitting associated with maximum likelihood can avoided marginalizing summing integrating over the model parameters instead making point estimates their values models can then compared directly the training data without the need for validation set this allows all available data used for training and avoids the multiple training runs for each model associated with cross validation also allows multiple complexity parameters determined simultaneously part the training process for example chapter shall introduce the relevance vector machine which bayesian model having one complexity parameter for every training data point the bayesian view model comparison simply involves the use probabilities represent uncertainty the choice model along with consistent application the sum and product rules probability suppose wish compare set models where here model refers probability distribution over the observed data the case the polynomial curve fitting problem the distribution defined over the set target values while the set input values assumed known other types model define joint distributions over and shall suppose that the data generated from one these models but are uncertain which one our uncertainty expressed through prior probability distribution given training set then wish evaluate the posterior distribution the prior allows express preference for different models let simply assume that all models are given equal prior probability the interesting term the model evidence which expresses the preference shown the data for section linear models for regression different models and shall examine this term more detail shortly the model evidence sometimes also called the marginal likelihood because can viewed likelihood function over the space models which the parameters have been marginalized out the ratio model evidences for two models known bayes factor kass and raftery once know the posterior distribution over models the predictive distribution given from the sum and product rules this example mixture distribution which the overall predictive distribution obtained averaging the predictive distributions individual models weighted the posterior probabilities those models for instance have two models that are posteriori equally likely and one predicts narrow distribution around while the other predicts narrow distribution around the overall predictive distribution will bimodal distribution with modes and not single model simple approximation model averaging use the single most probable model alone make predictions this known model selection for model governed set parameters the model evidence given from the sum and product rules probability from sampling perspective the marginal likelihood can viewed the probability generating the data set from model whose parameters are sampled random from the prior also interesting note that the evidence precisely the normalizing term that appears the denominator bayes theorem when evaluating the posterior distribution over parameters because can obtain some insight into the model evidence making simple approximation the integral over parameters consider first the case model having single parameter the posterior distribution over parameters proportional where omit the dependence the model keep the notation uncluttered assume that the posterior distribution sharply peaked around the most probable value wmap with width wposterior then can approximate the integral the value the integrand its maximum times the width the peak further assume that the prior flat with width wprior that wprior then have wmap wposterior wprior chapter bayesian model comparison wposterior figure can obtain rough approximation the model evidence assume that the posterior distribution over parameters sharply peaked around its mode wmap and taking logs obtain wmap wmap wprior wposterior wprior this approximation illustrated figure the first term represents the fit the data given the most probable parameter values and for flat prior this would correspond the log likelihood the second term penalizes the model according its complexity because wposterior wprior this term negative and increases magnitude the ratio wposterior wprior gets smaller thus parameters are finely tuned the data the posterior distribution then the penalty term large for model having set parameters can make similar approximation for each parameter turn assuming that all parameters have the same ratio wposterior wprior obtain wmap wposterior wprior thus this very simple approximation the size the complexity penalty increases linearly with the number adaptive parameters the model increase the complexity the model the first term will typically decrease because more complex model better able fit the data whereas the second term will increase due the dependence the optimal model complexity determined the maximum evidence will given trade off between these two competing terms shall later develop more refined version this approximation based gaussian approximation the posterior distribution can gain further insight into bayesian model comparison and understand how the marginal likelihood can favour models intermediate complexity considering figure here the horizontal axis one dimensional representation the space possible data sets that each point this axis corresponds specific data set now consider three models and successively increasing complexity imagine running these models generatively produce example data sets and then looking the distribution data sets that result any given section linear models for regression figure schematic illustration the distribution data sets for three models different comin which the plexity simplest and the most complex note that the distributions are normalized this example for the particular observed data set the model with intermediate complexity has the largest evidence bayes factor the form model can generate variety different data sets since the parameters are governed prior probability distribution and for any choice the parameters there may random noise the target variables generate particular data set from specific model first choose the values the parameters from their prior distribution and then for these parameter values sample the data from simple model for example based first order polynomial has little variability and will generate data sets that are fairly similar each other its distribution therefore confined relatively small region the horizontal axis contrast complex model such ninth order polynomial can generate great variety different data sets and its distribution spread over large region the space data sets because the distributions are normalized see that the particular data set can have the highest value the evidence for the model intermediate complexity essentially the simpler model cannot fit the data well whereas the more complex model spreads its predictive probability over too broad range data sets and assigns relatively small probability any one them implicit the bayesian model comparison framework the assumption that the true distribution from which the data are generated contained within the set models under consideration provided this can show that bayesian model comparison will average favour the correct model see this consider two models and which the truth corresponds for given finite data set possible for the bayes factor larger for the incorrect model however average the bayes factor over the distribution data sets obtain the expected section where the average has been taken with respect the true distribution the data this quantity example the kullback leibler divergence and satisfies the property always being positive unless the two distributions are equal which case zero thus average the bayes factor will always favour the correct model have seen that the bayesian framework avoids the problem over fitting and allows models compared the basis the training data alone however"}, "54": {"Section": "3.5", "Title": "The Evidence Approximation", "Content": "bayesian approach like any approach pattern recognition needs make assumptions about the form the model and these are invalid then the results can misleading particular see from figure that the model evidence can sensitive many aspects the prior such the behaviour the tails indeed the evidence not defined the prior improper can seen noting that improper prior has arbitrary scaling factor other words the normalization coefficient not defined because the distribution cannot normalized consider proper prior and then take suitable limit order obtain improper prior for example gaussian prior which take the limit infinite variance then the evidence will zero can seen from and figure may however possible consider the evidence ratio between two models first and then take limit obtain meaningful answer practical application therefore will wise keep aside independent test set data which evaluate the overall performance the final system the evidence approximation fully bayesian treatment the linear basis function model would introduce prior distributions over the hyperparameters and and make predictions marginalizing with respect these hyperparameters well with respect the parameters however although can integrate analytically over either over the hyperparameters the complete marginalization over all these variables analytically intractable here discuss approximation which set the hyperparameters specific values determined maximizing the marginal likelihood function obtained first integrating over the parameters this framework known the statistics literature empirical bayes bernardo and smith gelman type maximum likelihood berger generalized maximum likelihood wahba and the machine learning literature also called the evidence approximation gull mackay introduce hyperpriors over and the predictive distribution obtained marginalizing over and that where given and given with and defined and respectively here have omitted the dependence the input variable keep the notation uncluttered the posterior distribution sharply peaked around values then the predictive distribution obtained simply marginalizing over which and are fixed the values and that and linear models for regression from bayes theorem the posterior distribution for and given and the prior relatively flat then the evidence framework the values are obtained maximizing the marginal likelihood function shall proceed evaluating the marginal likelihood for the linear basis function model and then finding its maxima this will allow determine values for these hyperparameters from the training data alone without recourse cross validation recall that the ratio analogous regularization parameter aside worth noting that define conjugate gamma prior distributions over and then the marginalization over these hyperparameters can performed analytically give student distribution over see section although the resulting integral over longer analytically tractable might thought that approximating this integral for example using the laplace approximation discussed section which based local gaussian approximation centred the mode the posterior distribution might provide practical alternative the evidence framework buntine and weigend however the integrand function typically has strongly skewed mode that the laplace approximation fails capture the bulk the probability mass leading poorer results than those obtained maximizing the evidence mackay returning the evidence framework note that there are two approaches that can take the maximization the log evidence can evaluate the evidence function analytically and then set its derivative equal zero obtain estimation equations for and which shall section alternatively use technique called the expectation maximization algorithm which will discussed section where shall also show that these two approaches converge the same solution"}, "55": {"Section": "3.5.1", "Title": "Evaluation of the evidence function", "Content": "the marginal likelihood function obtained integrating over the weight parameters that exercise exercise one way evaluate this integral make use once again the result for the conditional distribution linear gaussian model here shall evaluate the integral instead completing the square the exponent and making use the standard form for the normalization coefficient gaussian from and can write the evidence function the form exp exercise exercise the evidence approximation where the dimensionality and have defined wtw recognize being equal constant proportionality the regularized sum squares error function now complete the square over giving where have introduced together with note that corresponds the matrix second derivatives the error function and known the hessian matrix here have also defined given using see that definition and therefore represents the mean the posterior distribution and hence equivalent the previous the integral over can now evaluated simply appealing the standard result for the normalization coefficient multivariate gaussian giving exp exp exp exp using can then write the log the marginal likelihood the form which the required expression for the evidence function returning the polynomial regression problem can plot the model evidence against the order the polynomial shown figure here have assumed prior the form with the parameter fixed the form this plot very instructive referring back figure see that the polynomial has very poor fit the data and consequently gives relatively low value linear models for regression figure plot the model evidence versus the order for the polynomial regression model showing that the evidence favours the model with for the evidence going the polynomial greatly improves the data fit and hence the evidence significantly higher however going the data fit improved only very marginally due the fact that the underlying sinusoidal function from which the data generated odd function and has even terms polynomial expansion indeed figure shows that the residual data error reduced only slightly going from because this richer model suffers greater complexity penalty the evidence actually falls going from when obtain significant further improvement data fit seen figure and the evidence increased again giving the highest overall evidence for any the polynomials further increases the value produce only small improvements the fit the data but suffer increasing complexity penalty leading overall decrease the evidence values looking again figure see that the generalization error roughly constant between and and would difficult choose between these models the basis this plot alone the evidence values however show clear preference for since this the simplest model which gives good explanation for the observed data"}, "56": {"Section": "3.5.2", "Title": "Maximizing the evidence function", "Content": "let first consider the maximization with respect this can done first defining the following eigenvector equation from then follows that has eigenvalues now consider the derivative the term involving with respect have iui thus the stationary points with respect satisfy the evidence approximation multiplying through and rearranging obtain since there are terms the sum over the quantity can written exercise the interpretation the quantity will discussed shortly from see that the value that maximizes the marginal likelihood satisfies note that this implicit solution for not only because depends but also because the mode the posterior distribution itself depends the choice therefore adopt iterative procedure which make initial choice for and use this find which given and also evaluate which given these values are then used estimate using and the process repeated until convergence note that because the matrix fixed can compute its eigenvalues once the start and then simply multiply these obtain the should emphasized that the value has been determined purely looking the training data contrast maximum likelihood methods independent data set required order optimize the model complexity can similarly maximize the log marginal likelihood with respect this note that the eigenvalues defined are proportional and hence giving the stationary point the marginal likelihood therefore satisfies exercise and rearranging obtain again this implicit solution for and can solved choosing initial value for and then using this calculate and and then estimate using repeating until convergence both and are determined from the data then their values can estimated together after each update linear models for regression figure contours the likelihood function red and the prior green which the axes parameter space have been rotated align with the eigenvectors the hessian for the mode the posterior given the maximum likelihood solution wml whereas for nonzero the mode wmap the direction the eigenvalue defined small compared with and the quantity close zero and the corresponding map value also close zero contrast the direction the eigenvalue large compared with and the quantity close unity and the map value close its maximum likelihood value wml wmap"}, "57": {"Section": "3.5.3", "Title": "Effective number of parameters", "Content": "the result has elegant interpretation mackay which provides insight into the bayesian solution for see this consider the contours the likelihood function and the prior illustrated figure here have implicitly transformed rotated set axes parameter space aligned with the eigenvectors defined contours the likelihood function are then axis aligned ellipses the eigenvalues measure the curvature the likelihood function and figure the eigenvalue small compared with because smaller curvature corresponds greater elongation the contours the likelihood function because positive definite matrix will have positive eigenvalues and the ratio will lie between and consequently the quantity defined will lie the range for directions which the corresponding parameter will close its maximum likelihood value and the ratio will close such parameters are called well determined because their values are tightly constrained the data conversely for directions which the corresponding parameters will close zero will the ratios these are directions which the likelihood function relatively insensitive the parameter value and the parameter has been set small value the prior the quantity defined therefore measures the effective total number well determined parameters can obtain some insight into the result for estimating comparing with the corresponding maximum likelihood result given both these formulae express the variance the inverse precision average the squared differences between the targets and the model predictions however they differ that the number data points the denominator the maximum likelihood result replaced the bayesian result recall from that the maximum likelihood estimate the variance for gaussian distribution over the evidence approximation single variable given and that this estimate biased because the maximum likelihood solution for the mean has fitted some the noise the data effect this has used one degree freedom the model the corresponding unbiased estimate given and takes the form map shall see section that this result can obtained from bayesian treatment which marginalize over the unknown mean the factor the denominator the bayesian result takes account the fact that one degree freedom has been used fitting the mean and removes the bias maximum likelihood now consider the corresponding results for the linear regression model the mean the target distribution now given the function which contains parameters however not all these parameters are tuned the data the effective number parameters that are determined the data with the remaining parameters set small values the prior this reflected the bayesian result for the variance that has factor the denominator thereby correcting for the bias the maximum likelihood result can illustrate the evidence framework for setting hyperparameters using the sinusoidal synthetic data set from section together with the gaussian basis function model comprising basis functions that the total number parameters the model given including the bias here for simplicity illustration have set its true value and then used the evidence framework determine shown figure can also see how the parameter controls the magnitude the parameters plotting the individual parameters versus the effective number parameters shown figure consider the limit which the number data points large relation the number parameters then from all the parameters will well determined the data because involves implicit sum over data points and the eigenvalues increase with the size the data set this case and the estimation equations for and become where and are defined and respectively these results can used easy compute approximation the full evidence estimation linear models for regression figure the left plot shows red curve and blue curve versus for the sinusoidal synthetic data set the intersection these two curves that defines the optimum value for given the evidence procedure the right plot shows the corresponding graph log evidence versus red curve showing that the peak coincides with the crossing point the curves the left plot also shown the test set error blue curve showing that the evidence maximum occurs close the point best generalization formulae because they not require evaluation the eigenvalue spectrum the hessian figure plot the parameters from the gaussian basis function model versus the effective number parameters which the hyperparameter varied the range causing vary the range"}, "58": {"Section": "3.6", "Title": "Limitations of Fixed Basis Functions", "Content": "throughout this chapter have focussed models comprising linear combination fixed nonlinear basis functions have seen that the assumption linearity the parameters led range useful properties including closed form solutions the least squares problem well tractable bayesian treatment furthermore for suitable choice basis functions can model arbitrary nonlinearities the exercises mapping from input variables targets the next chapter shall study analogous class models for classification might appear therefore that such linear models constitute general purpose framework for solving problems pattern recognition unfortunately there are some significant shortcomings with linear models which will cause turn later chapters more complex models such support vector machines and neural networks the difficulty stems from the assumption that the basis functions are fixed before the training data set observed and manifestation the curse dimensionality discussed section consequence the number basis functions needs grow rapidly often exponentially with the dimensionality the input space fortunately there are two properties real data sets that can exploit help alleviate this problem first all the data vectors typically lie close nonlinear manifold whose intrinsic dimensionality smaller than that the input space result strong correlations between the input variables will see example this when consider images handwritten digits chapter are using localized basis functions can arrange that they are scattered input space only regions containing data this approach used radial basis function networks and also support vector and relevance vector machines neural network models which use adaptive basis functions having sigmoidal nonlinearities can adapt the parameters that the regions input space over which the basis functions vary corresponds the data manifold the second property that target variables may have significant dependence only small number possible directions within the data manifold neural networks can exploit this property choosing the directions input space which the basis functions respond linear models for classification the previous chapter explored class regression models having particularly simple analytical and computational properties now discuss analogous class models for solving classification problems the goal classification take input vector and assign one discrete classes where the most common scenario the classes are taken disjoint that each input assigned one and only one class the input space thereby divided into decision regions whose boundaries are called decision boundaries decision surfaces this chapter consider linear models for classification which mean that the decision surfaces are linear functions the input vector and hence are defined dimensional hyperplanes within the dimensional input space data sets whose classes can separated exactly linear decision surfaces are said linearly separable for regression problems the target variable was simply the vector real numbers whose values wish predict the case classification there are various"}, "59": {"Section": "4.", "Title": "Linear Models for Classification", "Content": "ways using target values represent class labels for probabilistic models the most convenient the case two class problems the binary representation which there single target variable such that represents class and represents class can interpret the value the probability that the class with the values probability taking only the extreme values and for classes convenient use coding scheme which vector length such that the class then all elements are zero except element which takes the value for instance have classes then pattern from class would given the target vector again can interpret the value the probability that the class for nonprobabilistic models alternative choices target variable representation will sometimes prove convenient chapter identified three distinct approaches the classification problem the simplest involves constructing discriminant function that directly assigns each vector specific class more powerful approach however models the conditional probability distribution inference stage and then subsequently uses this distribution make optimal decisions separating inference and decision gain numerous benefits discussed section there are two different approaches determining the conditional probabilities one technique model them directly for example representing them parametric models and then optimizing the parameters using training set alternatively can adopt generative approach which model the class conditional densities given together with the prior probabilities for the classes and then compute the required posterior probabilities using bayes theorem shall discuss examples all three approaches this chapter the linear regression models considered chapter the model prediction was given linear function the parameters the simplest case the model also linear the input variables and therefore takes the form wtx that real number for classification problems however wish predict discrete class labels more generally posterior probabilities that lie the range achieve this consider generalization this model which transform the linear function using nonlinear function that wtx the machine learning literature known activation function whereas its inverse called link function the statistics literature the decision surfaces correspond constant that wtx constant and hence the decision surfaces are linear functions even the function nonlinear for this reason the class models described are called generalized linear models"}, "60": {"Section": "4.1", "Title": "Discriminant Functions", "Content": "mccullagh and nelder note however that contrast the models used for regression they are longer linear the parameters due the presence the nonlinear function this will lead more complex analytical and computational properties than for linear regression models nevertheless these models are still relatively simple compared the more general nonlinear models that will studied subsequent chapters the algorithms discussed this chapter will equally applicable first make fixed nonlinear transformation the input variables using vector basis functions did for regression models chapter begin considering classification directly the original input space while section shall find convenient switch notation involving basis functions for consistency with later chapters discriminant functions discriminant function that takes input vector and assigns one classes denoted this chapter shall restrict attention linear discriminants namely those for which the decision surfaces are hyperplanes simplify the discussion consider first the case two classes and then investigate the extension classes"}, "61": {"Section": "4.1.1", "Title": "Two classes", "Content": "the simplest representation linear discriminant function obtained taking linear function the input vector that wtx where called weight vector and bias not confused with bias the statistical sense the negative the bias sometimes called threshold input vector assigned class and class otherwise the corresponding decision boundary therefore defined the relation which corresponds dimensional hyperplane within the dimensional input space consider two points and both which lie the decision surface because have and hence the vector orthogonal every vector lying within the decision surface and determines the orientation the decision surface similarly point the decision surface then and the normal distance from the origin the decision surface given therefore see that the bias parameter determines the location the decision surface these properties are illustrated for the case figure furthermore note that the value gives signed measure the perpendicular distance the point from the decision surface see this consider wtx linear models for classification figure illustration the geometry linear discriminant function two dimensions the decision surface shown red perpendicular and its displacement from the origin controlled the bias parameter also the signed orthogonal distance general point from the decision surface given arbitrary point and let its orthogonal projection onto the decision surface that multiplying both sides this result and adding and making use wtx and wtx have this result illustrated figure with the linear regression models chapter sometimes convenient use more compact notation which introduce additional dummy input value and then define that and this case the decision surfaces are dimensional hyperplanes passing through the origin the dimensional expanded input space"}, "62": {"Section": "4.1.2", "Title": "Multiple classes", "Content": "now consider the extension linear discriminants classes might tempted build class discriminant combining number two class discriminant functions however this leads some serious difficulties duda and hart now show consider the use classifiers each which solves two class problem separating points particular class from points not that class this known one versus the rest classifier the left hand example figure shows discriminant functions not not figure attempting construct class discriminant from set two class discriminants leads ambiguous regions shown green the left example involving the use two discriminants designed distinguish points class from points not class the right example involving three discriminant functions each which used separate pair classes and example involving three classes where this approach leads regions input space that are ambiguously classified alternative introduce binary discriminant functions one for every possible pair classes this known one versus one classifier each point then classified according majority vote amongst the discriminant functions however this too runs into the problem ambiguous regions illustrated the right hand diagram figure can avoid these difficulties considering single class discriminant comprising linear functions the form and then assigning point class for all the decision boundary between class and class therefore given and hence corresponds dimensional hyperplane defined this has the same form the decision boundary for the two class case discussed section and analogous geometrical properties apply the decision regions such discriminant are always singly connected and convex see this consider two points and both which lie inside decision that lies the line connecting region illustrated figure any point and can expressed the form linear models for classification figure illustration the decision regions for multiclass linear discriminant with the decision boundaries shown red two points and both lie inside the same decision region then any point that lies the line connecting these two points must also lie and hence the decision region must singly connected and convex where from the linearity the discriminant functions follows that because both and lie inside follows that and for all and hence also lies inside thus singly connected and convex note that for two classes can either employ the formalism discussed here based two discriminant functions and else use the simpler but equivalent formulation described section based single discriminant function and now explore three approaches learning the parameters linear discriminant functions based least squares fisher linear discriminant and the perceptron algorithm"}, "63": {"Section": "4.1.3", "Title": "Least squares for classification", "Content": "chapter considered models that were linear functions the parameters and saw that the minimization sum squares error function led simple closed form solution for the parameter values therefore tempting see can apply the same formalism classification problems consider general classification problem with classes with binary coding scheme for the target vector one justification for using least squares such context that approximates the conditional expectation the target values given the input vector for the binary coding scheme this conditional expectation given the vector posterior class probabilities unfortunately however these probabilities are typically approximated rather poorly indeed the approximations can have values outside the range due the limited flexibility linear model shall see shortly each class described its own linear model that where can conveniently group these together using vector notation that the form discriminant functions and matrix whose kth column comprises the dimensional vector where the corresponding augmented input vector with dummy input this representation was discussed detail section new input then assigned the class for which the output largest minimizing sum squares error function did for regression chapter consider training data set where and define matrix whose nth row the vector the sum squares error function together with matrix can then written now determine the parameter matrix whose nth row setting the derivative with respect solution for zero and rearranging then obtain the xtt the pseudo inverse the matrix where then obtain the discriminant function the form discussed section exercise section interesting property least squares solutions with multiple target variables that every target vector the training set satisfies some linear constraint attn for some constants and then the model prediction for any value will satisfy the same constraint that aty thus use coding scheme for classes then the predictions made the model will have the property that the elements will sum for any value however this summation constraint alone not sufficient allow the model outputs interpreted probabilities because they are not constrained lie within the interval the least squares approach gives exact closed form solution for the discriminant function parameters however even discriminant function where use make decisions directly and dispense with any probabilistic interpretation suffers from some severe problems have already seen that least squares solutions lack robustness outliers and this applies equally the classification application illustrated figure here see that the additional data points the righthand figure produce significant change the location the decision boundary even though these point would correctly classified the original decision boundary the left hand figure the sum squares error function penalizes predictions that are too correct that they lie long way the correct side the decision linear models for classification figure the left plot shows data from two classes denoted red crosses and blue circles together with the decision boundary found least squares magenta curve and also the logistic regression model green curve which discussed later section the right hand plot shows the corresponding results obtained when extra data points are added the bottom left the diagram showing that least squares highly sensitive outliers unlike logistic regression boundary section shall consider several alternative error functions for classification and shall see that they not suffer from this difficulty however problems with least squares can more severe than simply lack robustness illustrated figure this shows synthetic data set drawn from three classes two dimensional input space having the property that linear decision boundaries can give excellent separation between the classes indeed the technique logistic regression described later this chapter gives satisfactory solution seen the right hand plot however the least squares solution gives poor results with only small region the input space assigned the green class the failure least squares should not surprise when recall that corresponds maximum likelihood under the assumption gaussian conditional distribution whereas binary target vectors clearly have distribution that far from gaussian adopting more appropriate probabilistic models shall obtain classification techniques with much better properties than least squares for the moment however continue explore alternative nonprobabilistic methods for setting the parameters the linear classification models"}, "64": {"Section": "4.1.4", "Title": "Fisher\u2019s linear discriminant", "Content": "one way view linear classification model terms dimensionality reduction consider first the case two classes and suppose take the discriminant functions figure example synthetic data set comprising three classes with training data points denoted red green and blue lines denote the decision boundaries and the background colours denote the respective classes the decision regions the left the result using least squares discriminant see that the region input space assigned the green class too small and most the points from this class are misclassified the right the result using logistic regressions described section showing correct classification the training data dimensional input vector and project down one dimension using wtx place threshold and classify class and otherwise class then obtain our standard linear classifier discussed the previous section general the projection onto one dimension leads considerable loss information and classes that are well separated the original dimensional space may become strongly overlapping one dimension however adjusting the components the weight vector can select projection that maximizes the class separation begin with consider two class problem which there are points class and points class that the mean vectors the two classes are given the simplest measure the separation the classes when projected onto the separation the projected class means this suggests that might choose maximize where wtmk linear models for classification appendix exercise figure the left plot shows samples from two classes depicted red and blue along with the histograms resulting from projection onto the line joining the class means note that there considerable class overlap the projected space the right plot shows the corresponding projection based the fisher linear discriminant showing the greatly improved class separation the mean the projected data from class however this expression can made arbitrarily large simply increasing the magnitude solve this using problem could constrain have unit length that lagrange multiplier perform the constrained maximization then find that there still problem with this approach however illustrated figure this shows two classes that are well separated the original twodimensional space but that have considerable overlap when projected onto the line joining their means this difficulty arises from the strongly nondiagonal covariances the class distributions the idea proposed fisher maximize function that will give large separation between the projected class means while also giving small variance within each class thereby minimizing the class overlap the projection formula transforms the set labelled data points into labelled set the one dimensional space the within class variance the transformed data from class therefore given where wtxn can define the total within class variance for the whole the fisher criterion defined the ratio the data set simply between class variance the within class variance and given exercise can make the dependence explicit using and rewrite the fisher criterion the form discriminant functions wtsbw wtsww where the between class covariance matrix and given and the total within class covariance matrix given differentiating with respect find that maximized when wtsbw sww wtsww sbw from see that sbw always the direction furthermore not care about the magnitude only its direction and can drop the scalar factors wtsbw and wtsww multiplying both sides then obtain note that the within class covariance isotropic that proportional the unit matrix find that proportional the difference the class means discussed above the result known fisher linear discriminant although strictly not discriminant but rather specific choice direction for projection the data down one dimension however the projected data can subsequently used construct discriminant choosing threshold that classify new point belonging and classify belonging otherwise for example can model the class conditional densities using gaussian distributions and then use the techniques section find the parameters the gaussian distributions maximum likelihood having found gaussian approximations the projected classes the formalism section then gives expression for the optimal threshold some justification for the gaussian assumption comes from the central limit theorem noting that wtx the sum set random variables"}, "65": {"Section": "4.1.5", "Title": "Relation to least squares", "Content": "the least squares approach the determination linear discriminant was based the goal making the model predictions close possible set target values contrast the fisher criterion was derived requiring maximum class separation the output space interesting see the relationship between these two approaches particular shall show that for the two class problem the fisher criterion can obtained special case least squares far have considered coding for the target values however adopt slightly different target coding scheme then the least squares solution for linear models for classification the weights becomes equivalent the fisher solution duda and hart particular shall take the targets for class where the number patterns class and the total number patterns this target value approximates the reciprocal the prior probability for class for class shall take the targets where the number patterns class the sum squares error function can written wtxn setting the derivatives with respect and zero obtain respectively wtxn wtxn from and making use our choice target coding scheme for the obtain expression for the bias the form where have used wtm and where the mean the total data set and given exercise after some straightforward algebra and again making use the choice the second equation becomes where defined defined and have substituted for the bias using using note that sbw always the direction thus can write where have ignored irrelevant scale factors thus the weight vector coincides with that found from the fisher criterion addition have also found expression for the bias value given this tells that new vector should classified belonging class and class otherwise discriminant functions"}, "66": {"Section": "4.1.6", "Title": "Fisher\u2019s discriminant for multiple classes", "Content": "now consider the generalization the fisher discriminant classes and shall assume that the dimensionality the input space greater than the number classes next introduce linear features where these feature values can conveniently grouped together form vector similarly the weight vectors can considered the columns matrix that note that again are not including any bias parameters the definition the generalization the within class covariance matrix the case classes follows from give wtx where and the number patterns class order find generalization the between class covariance matrix follow duda and hart and consider first the total covariance matrix where the mean the total data set nkmk and the total number data points the total covariance matrix can decomposed into the sum the within class covariance matrix given and plus additional matrix which identify measure the between class covariance where linear models for classification these covariance matrices have been defined the original space can now define similar matrices the projected dimensional space and where again wish construct scalar that large when the between class covariance large and when the within class covariance small there are now many possible choices criterion fukunaga one example given this criterion can then rewritten explicit function the projection matrix the form wswwt wsbwt maximization such criteria straightforward though somewhat involved and discussed length fukunaga the weight values are determined those eigenvectors that correspond the largest eigenvalues there one important result that common all such criteria which worth emphasizing first note from that composed the sum matrices each which outer product two vectors and therefore rank addition only these matrices are independent result the constraint thus has rank most equal and there are most nonzero eigenvalues this shows that the projection onto the dimensional subspace spanned the eigenvectors does not alter the value and are therefore unable find more than linear features this means fukunaga"}, "67": {"Section": "4.1.7", "Title": "The perceptron algorithm", "Content": "another example linear discriminant model the perceptron rosenblatt which occupies important place the history pattern recognition algorithms corresponds two class model which the input vector first transformed using fixed nonlinear transformation give feature vector and this then used construct generalized linear model the form discriminant functions where the nonlinear activation function given step function the form the vector will typically include bias component earlier discussions two class classification problems have focussed target coding scheme which which appropriate the context probabilistic models for the perceptron however more convenient use target values for class and for class which matches the choice activation function the algorithm used determine the parameters the perceptron can most easily motivated error function minimization natural choice error function would the total number misclassified patterns however this does not lead simple learning algorithm because the error piecewise constant function with discontinuities wherever change causes the decision boundary move across one the data points methods based changing using the gradient the error function cannot then applied because the gradient zero almost everywhere therefore consider alternative error function known the perceptron criterion derive this note that are seeking weight vector such that patterns class will have whereas patterns class have using the target coding scheme follows that would like all patterns satisfy the perceptron criterion associates zero error with any pattern that correctly classified whereas for misclassified pattern tries minimize the quantity the perceptron criterion therefore given ntn frank rosenblatt rosenblatt perceptron played important role the history machine learning initially rosenblatt simulated the perceptron ibm computer cornell but the early had built special purpose hardware that provided direct parallel implementation perceptron learning many his ideas were encapsulated principles neurodynamics perceptrons and the theory brain mechanisms published rosenblatt work was criticized marvin minksy whose objections were published the book perceptrons authored with seymour papert this book was widely misinterpreted the time showing that neural networks were fatally flawed and could only learn solutions for linearly separable problems fact only proved such limitations the case single layer networks such the perceptron and merely conjectured incorrectly that they applied more general network models unfortunately however this book contributed the substantial decline research funding for neural computing situation that was not reversed until the mid today there are many hundreds not thousands applications neural networks widespread use with examples areas such handwriting recognition and information retrieval being used routinely millions people linear models for classification section where denotes the set all misclassified patterns the contribution the error associated with particular misclassified pattern linear function regions space where the pattern misclassified and zero regions where correctly classified the total error function therefore piecewise linear now apply the stochastic gradient descent algorithm this error function the change the weight vector then given ntn where the learning rate parameter and integer that indexes the steps the algorithm because the perceptron function unchanged multiply constant can set the learning rate parameter equal without generality note that the weight vector evolves during training the set patterns that are misclassified will change the perceptron learning algorithm has simple interpretation follows cycle through the training patterns turn and for each pattern evaluate the perceptron function the pattern correctly classified then the weight vector remains unchanged whereas incorrectly classified then for class add the vector onto the current estimate weight vector while for class subtract the vector from the perceptron learning algorithm illustrated figure consider the effect single update the perceptron learning algorithm see that the contribution the error from misclassified pattern will reduced because from have ntn ntn ntn ntn ntn where have set and made use ntn course this does not imply that the contribution the error function from the other misclassified patterns will have been reduced furthermore the change weight vector may have caused some previously correctly classified patterns become misclassified thus the perceptron learning rule not guaranteed reduce the total error function each stage however the perceptron convergence theorem states that there exists exact solution other words the training data set linearly separable then the perceptron learning algorithm guaranteed find exact solution finite number steps proofs this theorem can found for example rosenblatt block nilsson minsky and papert hertz and bishop note however that the number steps required achieve convergence could still substantial and practice until convergence achieved will not able distinguish between nonseparable problem and one that simply slow converge even when the data set linearly separable there may many solutions and which one found will depend the initialization the parameters and the order presentation the data points furthermore for data sets that are not linearly separable the perceptron learning algorithm will never converge discriminant functions figure illustration the convergence the perceptron learning algorithm showing data points from two classes red and blue two dimensional feature space the top left plot shows the initial parameter vector shown black arrow together with the corresponding decision boundary black line which the arrow points towards the decision region which classified belonging the red class the data point circled green misclassified and its feature vector added the current weight vector giving the new decision boundary shown the top right plot the bottom left plot shows the next misclassified point considered indicated the green circle and its feature vector again added the weight vector giving the decision boundary shown the bottom right plot for which all data points are correctly classified linear models for classification figure illustration the mark perceptron hardware the photograph the left shows how the inputs were obtained using simple camera system which input scene this case printed character was illuminated powerful lights and image focussed onto array cadmium sulphide photocells giving primitive pixel image the perceptron also had patch board shown the middle photograph which allowed different configurations input features tried often these were wired random demonstrate the ability the perceptron learn without the need for precise wiring contrast modern digital computer the photograph the right shows one the racks adaptive weights each weight was implemented using rotary variable resistor also called potentiometer driven electric motor thereby allowing the value the weight adjusted automatically the learning algorithm aside from difficulties with the learning algorithm the perceptron does not provide probabilistic outputs nor does generalize readily classes the most important limitation however arises from the fact that common with all the models discussed this chapter and the previous one based linear combinations fixed basis functions more detailed discussions the limitations perceptrons can found minsky and papert and bishop analogue hardware implementations the perceptron were built rosenblatt based motor driven variable resistors implement the adaptive parameters these are illustrated figure the inputs were obtained from simple camera system based array photo sensors while the basis functions could chosen variety ways for example based simple fixed functions randomly chosen subsets pixels from the input image typical applications involved learning discriminate simple shapes characters the same time that the perceptron was being developed closely related system called the adaline which short for adaptive linear element was being explored widrow and workers the functional form the model was the same for the perceptron but different approach training was adopted widrow and hoff widrow and lehr"}, "68": {"Section": "4.2", "Title": "Probabilistic Generative Models", "Content": "turn next probabilistic view classification and show how models with linear decision boundaries arise from simple assumptions about the distribution the data section discussed the distinction between the discriminative and the generative approaches classification here shall adopt generative probabilistic generative models figure plot the logistic sigmoid function defined shown red together with the scaled probit function for shown dashed blue where defined the scaling factor chosen that the derivatives the two curves are equal for approach which model the class conditional densities well the class priors and then use these compute posterior probabilities through bayes theorem consider first all the case two classes the posterior probability for class can written exp where have defined and the logistic sigmoid function defined exp which plotted figure the term sigmoid means shaped this type function sometimes also called squashing function because maps the whole real axis into finite interval the logistic sigmoid has been encountered already earlier chapters and plays important role many classification algorithms satisfies the following symmetry property easily verified the inverse the logistic sigmoid given and known the logit function represents the log the ratio probabilities for the two classes also known the log odds linear models for classification note that have simply rewritten the posterior probabilities equivalent form and the appearance the logistic sigmoid may seem rather vacuous however will have significance provided takes simple functional form shall shortly consider situations which linear function which case the posterior probability governed generalized linear model for the case classes have exp exp which known the normalized exponential and can regarded multiclass generalization the logistic sigmoid here the quantities are defined the normalized exponential also known the softmax function represents smoothed version the max function because for all then and now investigate the consequences choosing specific forms for the classconditional densities looking first continuous input variables and then discussing briefly the case discrete inputs"}, "69": {"Section": "4.2.1", "Title": "Continuous inputs", "Content": "let assume that the class conditional densities are gaussian and then explore the resulting form for the posterior probabilities start with shall assume that all classes share the same covariance matrix thus the density for class given consider first the case two classes from and have exp where have defined wtx see that the quadratic terms from the exponents the gaussian densities have cancelled due the assumption common covariance matrices leading linear function the argument the logistic sigmoid this result illustrated for the case two dimensional input space figure the resulting probabilistic generative models figure the left hand plot shows the class conditional densities for two classes denoted red and blue the right the corresponding posterior probability which given logistic sigmoid linear function the surface the right hand plot coloured using proportion red ink given and proportion blue ink given decision boundaries correspond surfaces along which the posterior probabilities are constant and will given linear functions and therefore the decision boundaries are linear input space the prior probabilities enter only through the bias parameter that changes the priors have the effect making parallel shifts the decision boundary and more generally the parallel contours constant posterior probability for the general case classes have from and where have defined see that the are again linear functions consequence the cancellation the quadratic terms due the shared covariances the resulting decision boundaries corresponding the minimum misclassification rate will occur when two the posterior probabilities the two largest are equal and will defined linear functions and again have generalized linear model relax the assumption shared covariance matrix and allow each classconditional density have its own covariance matrix then the earlier cancellations will longer occur and will obtain quadratic functions giving rise quadratic discriminant the linear and quadratic decision boundaries are illustrated figure linear models for classification figure the left hand plot shows the class conditional densities for three classes each having gaussian distribution coloured red green and blue which the red and green classes have the same covariance matrix the right hand plot shows the corresponding posterior probabilities which the rgb colour vector represents the posterior probabilities for the respective three classes the decision boundaries are also shown notice that the boundary between the red and green classes which have the same covariance matrix linear whereas those between the other pairs classes are quadratic"}, "70": {"Section": "4.2.2", "Title": "Maximum likelihood solution", "Content": "once have specified parametric functional form for the class conditional densities can then determine the values the parameters together with the prior class probabilities using maximum likelihood this requires data set comprising observations along with their corresponding class labels consider first the case two classes each having gaussian class conditional density with shared covariance matrix and suppose have data set where here denotes class and denotes class denote the prior class probability that for data point from class have and hence similarly for class have and hence thus the likelihood function given where usual convenient maximize the log the likelihood function consider first the maximization with respect the terms exercise probabilistic generative models the log likelihood function that depend are setting the derivative with respect equal zero and rearranging obtain where denotes the total number data points class and denotes the total number data points class thus the maximum likelihood estimate for simply the fraction points class expected this result easily generalized the multiclass case where again the maximum likelihood estimate the prior probability associated with class given the fraction the training set points assigned that class now consider the maximization with respect again can pick out the log likelihood function those terms that depend giving lnn const setting the derivative with respect zero and rearranging obtain which simply the mean all the input vectors assigned class similar argument the corresponding result for given tnxn which again the mean all the input vectors assigned class finally consider the maximum likelihood solution for the shared covariance matrix picking out the terms the log likelihood function that depend have linear models for classification where have defined exercise section section exercise using the standard result for the maximum likelihood solution for gaussian distribution see that which represents weighted average the covariance matrices associated with each the two classes separately this result easily extended the class problem obtain the corresponding maximum likelihood solutions for the parameters which each class conditional density gaussian with shared covariance matrix note that the approach fitting gaussian distributions the classes not robust outliers because the maximum likelihood estimation gaussian not robust"}, "71": {"Section": "4.2.3", "Title": "Discrete features", "Content": "let now consider the case discrete feature values for simplicity begin looking binary feature values and discuss the extension more general discrete features shortly there are inputs then general distribution would correspond table numbers for each class containing independent variables due the summation constraint because this grows exponentially with the number features might seek more restricted representation here will make the naive bayes assumption which the feature values are treated independent conditioned the class thus have class conditional distributions the form which contain independent parameters for each class substituting into then gives which again are linear functions the input values for the case classes can alternatively consider the logistic sigmoid formulation given analogous results are obtained for discrete variables each which can take states"}, "72": {"Section": "4.2.4", "Title": "Exponential family", "Content": "have seen for both gaussian distributed and discrete inputs the posterior class probabilities are given generalized linear models with logistic sigmoid"}, "73": {"Section": "4.3", "Title": "Probabilistic Discriminative Models", "Content": "classes softmax classes activation functions these are particular cases more general result obtained assuming that the class conditional densities are members the exponential family distributions distribution can written the form using the form for members the exponential family see that the exp now restrict attention the subclass such distributions for which then make use introduce scaling parameter that obtain the restricted set exponential family class conditional densities the form exp note that are allowing each class have its own parameter vector but are assuming that the classes share the same scale parameter for the two class problem substitute this expression for the class conditional densities into and see that the posterior class probability again given logistic sigmoid acting linear function which given similarly for the class problem substitute the class conditional density expression into give and again linear function probabilistic discriminative models for the two class classification problem have seen that the posterior probability class can written logistic sigmoid acting linear function for wide choice class conditional distributions similarly for the multiclass case the posterior probability class given softmax transformation linear function for specific choices the class conditional densities have used maximum likelihood determine the parameters the densities well the class priors and then used bayes theorem find the posterior class probabilities however alternative approach use the functional form the generalized linear model explicitly and determine its parameters directly using maximum likelihood shall see that there efficient algorithm finding such solutions known iterative reweighted least squares irls the indirect approach finding the parameters generalized linear model fitting class conditional densities and class priors separately and then applying linear models for classification figure illustration the role nonlinear basis functions linear classification models the left plot shows the original input space together with data points from two classes labelled red and blue two gaussian basis functions and are defined this space with centres shown the green crosses and with contours shown the green circles the right hand plot shows the corresponding feature space together with the linear decision boundary obtained given logistic regression model the form discussed section this corresponds nonlinear decision boundary the original input space shown the black curve the left hand plot bayes theorem represents example generative modelling because could take such model and generate synthetic data drawing values from the marginal distribution the direct approach are maximizing likelihood function defined through the conditional distribution which represents form discriminative training one advantage the discriminative approach that there will typically fewer adaptive parameters determined shall see shortly may also lead improved predictive performance particularly when the class conditional density assumptions give poor approximation the true distributions"}, "74": {"Section": "4.3.1", "Title": "Fixed basis functions", "Content": "far this chapter have considered classification models that work directly with the original input vector however all the algorithms are equally applicable first make fixed nonlinear transformation the inputs using vector basis functions the resulting decision boundaries will linear the feature space and these correspond nonlinear decision boundaries the original space illustrated figure classes that are linearly separable the feature space need not linearly separable the original observation space note that our discussion linear models for regression one the probabilistic discriminative models basis functions typically set constant say that the corresponding parameter plays the role bias for the remainder this chapter shall include fixed basis function transformation this will highlight some useful similarities the regression models discussed chapter for many problems practical interest there significant overlap between the class conditional densities this corresponds posterior probabilities which for least some values are not such cases the optimal solution obtained modelling the posterior probabilities accurately and then applying standard decision theory discussed chapter note that nonlinear transformations cannot remove such class overlap indeed they can increase the level overlap create overlap where none existed the original observation space however suitable choices nonlinearity can make the process modelling the posterior probabilities easier such fixed basis function models have important limitations and these will resolved later chapters allowing the basis functions themselves adapt the data notwithstanding these limitations models with fixed nonlinear basis functions play important role applications and discussion such models will introduce many the key concepts needed for understanding their more complex counterparts"}, "75": {"Section": "4.3.2", "Title": "Logistic regression", "Content": "begin our treatment generalized linear models considering the problem two class classification our discussion generative approaches section saw that under rather general assumptions the posterior probability class can written logistic sigmoid acting linear function the feature vector that with here the logistic sigmoid function defined the terminology statistics this model known logistic regression although should emphasized that this model for classification rather than regression for dimensional feature space this model has adjustable parameters contrast had fitted gaussian class conditional densities using maximum likelihood would have used parameters for the means and parameters for the shared covariance matrix together with the class prior this gives total parameters which grows quadratically with contrast the linear dependence the number parameters logistic regression for large values there clear advantage working with the logistic regression model directly now use maximum likelihood determine the parameters the logistic regression model this shall make use the derivative the logistic sigmoid function which can conveniently expressed terms the sigmoid function itself section exercise exercise section exercise where and taking the gradient the error function with respect obtain where have made use see that the factor involving the derivative the logistic sigmoid has cancelled leading simplified form for the gradient the log likelihood particular the contribution the gradient from data point given the error between the target value and the prediction the model times the basis function vector furthermore comparison with shows that this takes precisely the same form the gradient the sum squares error function for the linear regression model desired could make use the result give sequential algorithm which patterns are presented one time which each the weight vectors updated using which the nth term worth noting that maximum likelihood can exhibit severe over fitting for data sets that are linearly separable this arises because the maximum likelihood solution occurs when the hyperplane corresponding equivalent separates the two classes and the magnitude goes infinity this case the logistic sigmoid function becomes infinitely steep feature space corresponding heaviside step function that every training point from each class assigned posterior probability furthermore there typically continuum such solutions because any separating hyperplane will give rise the same posterior probabilities the training data points will seen later figure maximum likelihood provides way favour one such solution over another and which solution found practice will depend the choice optimization algorithm and the parameter initialization note that the problem will arise even the number data points large compared with the number parameters the model long the training data set linearly separable the singularity can avoided inclusion prior and finding map solution for equivalently adding regularization term the error function linear models for classification for data set where and with the likelihood function can written ytn where and usual can define error function taking the negative logarithm the likelihood which gives the crossentropy error function the form probabilistic discriminative models"}, "76": {"Section": "4.3.3", "Title": "Iterative reweighted least squares", "Content": "the case the linear regression models discussed chapter the maximum likelihood solution the assumption gaussian noise model leads closed form solution this was consequence the quadratic dependence the log likelihood function the parameter vector for logistic regression there longer closed form solution due the nonlinearity the logistic sigmoid function however the departure from quadratic form not substantial precise the error function concave shall see shortly and hence has unique minimum furthermore the error function can minimized efficient iterative technique based the newton raphson iterative optimization scheme which uses local quadratic approximation the log likelihood function the newton raphson update for minimizing function takes the form fletcher bishop and nabney where the hessian matrix whose elements comprise the second derivatives with respect the components new old let first all apply the newton raphson method the linear regression model with the sum squares error function the gradient and hessian this error function are given new old old which recognize the standard least squares solution note that the error function this case quadratic and hence the newton raphson formula gives the exact solution one step now let apply the newton raphson update the cross entropy error function for the logistic regression model from see that the gradient and hessian this error function are given section where the design matrix whose nth row given raphson update then takes the form the newton linear models for classification rnn where have made use also have introduced the diagonal matrix with elements see that the hessian longer constant but depends through the weighting matrix corresponding the fact that the error function longer quadratic using the property which follows from the form the logistic sigmoid function see that uthu for arbitrary vector and the hessian matrix positive definite follows that the error function concave function and hence has unique minimum the newton raphson update formula for the logistic regression model then beexercise comes new old trz old where dimensional vector with elements old see that the update formula takes the form set normal equations for weighted least squares problem because the weighing matrix not constant but depends the parameter vector must apply the normal equations iteratively each time using the new weight vector compute revised weighing matrix for this reason the algorithm known iterative reweighted least squares irls rubin the weighted least squares problem the elements the diagonal weighting matrix can interpreted variances because the mean and variance the logistic regression model are given var where have used the property for fact can interpret irls the solution linearized problem the space the variable the quantity which corresponds the nth element can then given simple interpretation effective target value this space obtained making local linear approximation the logistic sigmoid function around the current operating point old old old old dan dyn probabilistic discriminative models"}, "77": {"Section": "4.3.4", "Title": "Multiclass logistic regression", "Content": "our discussion generative models for multiclass classification have seen that for large class distributions the posterior probabilities are given softmax transformation linear functions the feature variables that exp exp where the activations are given there used maximum likelihood determine separately the class conditional densities and the class priors and then found the corresponding posterior probabilities using bayes theorem thereby implicitly determining the parameters here consider the use maximum likelihood determine the parameters this model directly this will require the derivatives with respect all the activations these are given ikj where ikj are the elements the identity matrix next write down the likelihood function this most easily done using the coding scheme which the target vector for feature vector belonging class binary vector with all elements zero except for element which equals one the likelihood function then given tnk ytnk where ynk and matrix target variables with elements tnk taking the negative logarithm then gives tnk ynk which known the cross entropy error function for the multiclass classification problem now take the gradient the error function with respect one the parameter vectors making use the result for the derivatives the softmax function obtain ynj tnj section exercise exercise linear models for classification tnk once again see the same form arising where have made use for the gradient was found for the sum squares error function with the linear model and the cross entropy error for the logistic regression model namely the product the error ynj tnj times the basis function again could use this formulate sequential algorithm which patterns are presented one time which each the weight vectors updated using have seen that the derivative the log likelihood function for linear regression model with respect the parameter vector for data point took the form the error times the feature vector similarly for the combination logistic sigmoid activation function and cross entropy error function and for the softmax activation function with the multiclass cross entropy error function again obtain this same simple form this example more general result shall see section find batch algorithm again appeal the newton raphson update obtain the corresponding irls algorithm for the multiclass problem this requires evaluation the hessian matrix that comprises blocks size which block given exercise ynk ikj ynj with the two class problem the hessian matrix for the multiclass logistic regression model positive definite and the error function again has unique minimum practical details irls for the multiclass case can found bishop and nabney"}, "78": {"Section": "4.3.5", "Title": "Probit regression", "Content": "have seen that for broad range class conditional distributions described the exponential family the resulting posterior class probabilities are given logistic softmax transformation acting linear function the feature variables however not all choices class conditional density give rise such simple form for the posterior probabilities for instance the class conditional densities are modelled using gaussian mixtures this suggests that might worth exploring other types discriminative probabilistic model for the purposes this chapter however shall return the two class case and again remain within the framework generalized linear models that where and the activation function one way motivate alternative choice for the link function consider noisy threshold model follows for each input evaluate and then set the target value according otherwise probabilistic discriminative models figure schematic example probability density shown the blue curve given this example mixture two gaussians along with its cumulative distribution function shown the red curve note that the value the blue curve any point such that indicated the vertical green line corresponds the slope the red curve the same point conversely the value the red curve this point corresponds the area under the blue curve indicated the shaded green region the stochastic threshold model the class label takes the value the value exceeds threshold otherwise takes the value this equivalent activation function given the cumulative distribution function the value drawn from probability density then the corresponding activation function will given the cumulative distribution function illustrated figure specific example suppose that the density given zero mean unit variance gaussian the corresponding cumulative distribution function given which known the probit function has sigmoidal shape and compared with the logistic sigmoid function figure note that the use more general gaussian distribution does not change the model because this equivalent scaling the linear coefficients many numerical packages provide for the evaluation closely related function defined erf exp exercise and known the erf function error function not confused with the error function machine learning model related the probit function erf the generalized linear model based probit activation function known probit regression can determine the parameters this model using maximum likelihood straightforward extension the ideas discussed earlier practice the results found using probit regression tend similar those logistic regression shall linear models for classification however find another use for the probit model when discuss bayesian treatments logistic regression section one issue that can occur practical applications that outliers which can arise for instance through errors measuring the input vector through mislabelling the target value because such points can lie long way the wrong side the ideal decision boundary they can seriously distort the classifier note that the logistic and probit regression models behave differently this respect because the tails the logistic sigmoid decay asymptotically like exp for whereas for the probit activation function they decay like exp and the probit model can significantly more sensitive outliers however both the logistic and the probit models assume the data correctly labelled the effect mislabelling easily incorporated into probabilistic model introducing probability that the target value has been flipped the wrong value opper and winther leading target value distribution for data point the form where the activation function with input vector here may set advance may treated hyperparameter whose value inferred from the data"}, "79": {"Section": "4.3.6", "Title": "Canonical link functions", "Content": "for the linear regression model with gaussian noise distribution the error function corresponding the negative log likelihood given take the derivative with respect the parameter vector the contribution the error function from data point this takes the form the error times the feature vector where similarly for the combination the logistic sigmoid activation function and the cross entropy error function and for the softmax activation function with the multiclass cross entropy error function again obtain this same simple form now show that this general result assuming conditional distribution for the target variable from the exponential family along with corresponding choice for the activation function known the canonical link function again make use the restricted form exponential family distributions note that here are applying the assumption exponential family distribution the target variable contrast section where applied the input vector therefore consider conditional distributions the target variable the form exp using the same line argument led the derivation the result see that the conditional mean which denote given"}, "80": {"Section": "4.4", "Title": "The Laplace Approximation", "Content": "thus and must related and denote this relation through following nelder and wedderburn define generalized linear model one for which nonlinear function linear combination the input feature variables that where known the activation function the machine learning literature and known the link function statistics now consider the log likelihood function for this model which function given ntn const where are assuming that all observations share common scale parameter which corresponds the noise variance for gaussian distribution for instance and independent the derivative the log likelihood with respect the model parameters then given dyn dyn dan where and have used together with the result for now see that there considerable simplification choose particular form for the link function given which gives and hence also because have and hence this case the gradient the error function reduces for the gaussian whereas for the logistic model the laplace approximation section shall discuss the bayesian treatment logistic regression shall see this more complex than the bayesian treatment linear regression models discussed sections and particular cannot integrate exactly chapter chapter linear models for classification over the parameter vector since the posterior distribution longer gaussian therefore necessary introduce some form approximation later the book shall consider range techniques based analytical approximations and numerical sampling here introduce simple but widely used framework called the laplace approximation that aims find gaussian approximation probability density defined over set continuous variables consider first the case single continuous variable and suppose the distribution defined the normalization coefficient shall suppose that the where value unknown the laplace method the goal find gaussian approximation which centred mode the distribution the first step find mode other words point such that equivalently gaussian distribution has the property that its logarithm quadratic function the variables therefore consider taylor expansion centred the mode that where note that the first order term the taylor expansion does not appear since local maximum the distribution taking the exponential obtain exp can then obtain normalized distribution making use the standard result for the normalization gaussian that exp the laplace approximation illustrated figure note that the gaussian approximation will only well defined its precision other words the stationary point must local maximum that the second derivative the point negative the laplace approximation figure illustration the laplace approximation applied the distribution exp where the logistic sigmoid function defined the left plot shows the normalized distribution yellow together with the laplace approximation centred the mode red the right plot shows the negative logarithms the corresponding curves can extend the laplace method approximate distribution defined over dimensional space stationary point the gradient will vanish expanding around this stationary point have where the hessian matrix defined and the gradient operator taking the exponential both sides obtain exp the distribution proportional and the appropriate normalization coefficient can found inspection using the standard result for normalized multivariate gaussian giving exp where denotes the determinant this gaussian distribution will well defined provided its precision matrix given positive definite which implies that the stationary point must local maximum not minimum saddle point order apply the laplace approximation first need find the mode and then evaluate the hessian matrix that mode practice mode will typically found running some form numerical optimization algorithm bishop linear models for classification and nabney many the distributions encountered practice will multimodal and there will different laplace approximations according which mode being considered note that the normalization constant the true distribution does not need known order apply the laplace method result the central limit theorem the posterior distribution for model expected become increasingly better approximated gaussian the number observed data points increased and would expect the laplace approximation most useful situations where the number data points relatively large one major weakness the laplace approximation that since based gaussian distribution only directly applicable real variables other cases may possible apply the laplace approximation transformation the variable for instance then can consider laplace approximation the most serious limitation the laplace framework however that based purely the aspects the true distribution specific value the variable and can fail capture important global properties chapter shall consider alternative approaches which adopt more global perspective"}, "81": {"Section": "4.4.1", "Title": "Model comparison and BIC", "Content": "well approximating the distribution can also obtain approximation the normalization constant using the approximation have exp where have noted that the integrand gaussian and made use the standard result for normalized gaussian distribution can use the result obtain approximation the model evidence which discussed section plays central role bayesian model comparison consider data set and set models having parameters for each model define likelihood function introduce prior over the parameters then are interested computing the model evidence for the various models from now omit the conditioning keep the notation uncluttered from bayes theorem the model evidence given identifying and and applying the result obtain map map occam factor exercise exercise section"}, "82": {"Section": "4.5", "Title": "Bayesian Logistic Regression", "Content": "where map the value the mode the posterior distribution and the hessian matrix second derivatives the negative log posterior map map map the first term the right hand side represents the log likelihood evaluated using the optimized parameters while the remaining three terms comprise the occam factor which penalizes model complexity assume that the gaussian prior distribution over parameters broad and that the hessian has full rank then can approximate very roughly using map where the number data points the number parameters and have omitted additive constants this known the bayesian information criterion bic the schwarz criterion schwarz note that compared aic given this penalizes model complexity more heavily complexity measures such aic and bic have the virtue being easy evaluate but can also give misleading results particular the assumption that the hessian matrix has full rank often not valid since many the parameters are not well determined can use the result obtain more accurate estimate the model evidence starting from the laplace approximation illustrate the context neural networks section bayesian logistic regression now turn bayesian treatment logistic regression exact bayesian inference for logistic regression intractable particular evaluation the posterior distribution would require normalization the product prior distribution and likelihood function that itself comprises product logistic sigmoid functions one for every data point evaluation the predictive distribution similarly intractable here consider the application the laplace approximation the problem bayesian logistic regression spiegelhalter and lauritzen mackay"}, "83": {"Section": "4.5.1", "Title": "Laplace approximation", "Content": "recall from section that the laplace approximation obtained finding the mode the posterior distribution and then fitting gaussian centred that mode this requires evaluation the second derivatives the log posterior which equivalent finding the hessian matrix because seek gaussian representation for the posterior distribution natural begin with gaussian prior which write the general form linear models for classification where and are fixed hyperparameters the posterior distribution over given where taking the log both sides and substituting for the prior distribution using and for the likelihood function using obtain const where obtain gaussian approximation the posterior distribution first maximize the posterior distribution give the map maximum posterior solution wmap which defines the mean the gaussian the covariance then given the inverse the matrix second derivatives the negative log likelihood which takes the form the gaussian approximation the posterior distribution therefore takes the form wmap having obtained gaussian approximation the posterior distribution there remains the task marginalizing with respect this distribution order make predictions"}, "84": {"Section": "4.5.2", "Title": "Predictive distribution", "Content": "the predictive distribution for class given new feature vector obtained marginalizing with respect the posterior distribution which itself approximated gaussian distribution that with the corresponding probability for class given evaluate the predictive distribution first note that the function depends only through its projection onto denoting have where the dirac delta function from this obtain bayesian logistic regression where can evaluate noting that the delta function imposes linear constraint and forms marginal distribution from the joint distribution integrating out all directions orthogonal because gaussian know from section that the marginal distribution will also gaussian can evaluate the mean and covariance this distribution taking moments and interchanging the order integration over and that map where have used the result for the variational posterior distribution similarly var tsn note that the distribution takes the same form the predictive distribution for the linear regression model with the noise variance set zero thus our variational approximation the predictive distribution becomes this result can also derived directly making use the results for the marginal gaussian distribution given section the integral over represents the convolution gaussian with logistic sigmoid and cannot evaluated analytically can however obtain good approximation spiegelhalter and lauritzen mackay barber and bishop making use the close similarity between the logistic sigmoid function defined and the probit function defined order obtain the best approximation the logistic function need scale the horizontal axis that approximate can find suitable value requiring that the two functions have the same slope the origin which gives the similarity the logistic sigmoid and the probit function for this choice illustrated figure the advantage using probit function that its convolution with gaussian can expressed analytically terms another probit function specifically can show that exercise exercise exercise linear models for classification now apply the approximation the probit functions appearing both sides this equation leading the following approximation for the convolution logistic sigmoid with gaussian where have defined applying this result obtain the approximate predictive distribution the form where and fined are defined and respectively and denote that the decision boundary corresponding given which the same the decision boundary obtained using the map value for thus the decision criterion based minimizing misclassification rate with equal prior probabilities then the marginalization over has effect however for more complex decision criteria will play important role marginalization the logistic sigmoid model under gaussian approximation the posterior distribution will illustrated the context variational inference figure"}, "85": {"Section": "5", "Title": "Neural Networks", "Content": "sparser models unlike the svm also produces probabilistic outputs although this the expense nonconvex optimization during training alternative approach fix the number basis functions advance but allow them adaptive other words use parametric forms for the basis functions which the parameter values are adapted during training the most successful model this type the context pattern recognition the feed forward neural network also known the multilayer perceptron discussed this chapter fact multilayer perceptron really misnomer because the model comprises multiple layers logistic regression models with continuous nonlinearities rather than multiple perceptrons with discontinuous nonlinearities for many applications the resulting model can significantly more compact and hence faster evaluate than support vector machine having the same generalization performance the price paid for this compactness with the relevance vector machine that the likelihood function which forms the basis for network training longer convex function the model parameters practice however often worth investing substantial computational resources during the training phase order obtain compact model that fast processing new data the term neural network has its origins attempts find mathematical representations information processing biological systems mcculloch and pitts widrow and hoff rosenblatt rumelhart indeed has been used very broadly cover wide range different models many which have been the subject exaggerated claims regarding their biological plausibility from the perspective practical applications pattern recognition however biological realism would impose entirely unnecessary constraints our focus this chapter therefore neural networks efficient models for statistical pattern recognition particular shall restrict our attention the specific class neural networks that have proven greatest practical value namely the multilayer perceptron begin considering the functional form the network model including the specific parameterization the basis functions and then discuss the problem determining the network parameters within maximum likelihood framework which involves the solution nonlinear optimization problem this requires the evaluation derivatives the log likelihood function with respect the network parameters and shall see how these can obtained efficiently using the technique error backpropagation shall also show how the backpropagation framework can extended allow other derivatives evaluated such the jacobian and hessian matrices next discuss various approaches regularization neural network training and the relationships between them also consider some extensions the neural network model and particular describe general framework for modelling conditional probability distributions known mixture density networks finally discuss the use bayesian treatments neural networks additional background neural network models can found bishop"}, "86": {"Section": "5.1", "Title": "Feed-forward Network Functions", "Content": "feed forward network functions the linear models for regression and classification discussed chapters and respectively are based linear combinations fixed nonlinear basis functions and take the form where nonlinear activation function the case classification and the identity the case regression our goal extend this model making the basis functions depend parameters and then allow these parameters adjusted along with the coefficients during training there are course many ways construct parametric nonlinear basis functions neural networks use basis functions that follow the same form that each basis function itself nonlinear function linear combination the inputs where the coefficients the linear combination are adaptive parameters this leads the basic neural network model which can described series functional transformations first construct linear combinations the input variables the form where and the superscript indicates that the corresponding parameters are the first layer the network shall refer the parameters weights and the parameters biases following the nomenclature chapter the quantities are known activations each them then transformed using differentiable nonlinear activation function give these quantities correspond the outputs the basis functions that the context neural networks are called hidden units the nonlinear functions are generally chosen sigmoidal functions such the logistic sigmoid the tanh function following these values are again linearly combined give output unit activations where and the total number outputs this transformation corresponds the second layer the network and again the are bias parameters finally the output unit activations are transformed using appropriate activation function give set network outputs the choice activation function determined the nature the data and the assumed distribution target variables exercise neural networks figure network diagram for the twolayer neural network corresponding the input hidden and output variables are represented nodes and the weight parameters are represented links between the nodes which the bias parameters are denoted links coming from additional input and hidden variables and arrows denote the direction information flow through the network during forward propagation inputs hidden units outputs and follows the same considerations for linear models discussed chapters and thus for standard regression problems the activation function the identity that similarly for multiple binary classification problems each output unit activation transformed using logistic sigmoid function that where exp finally for multiclass problems softmax activation function the form used the choice output unit activation function discussed detail section can combine these various stages give the overall network function that for sigmoidal output unit activation functions takes the form where the set all weight and bias parameters have been grouped together into vector thus the neural network model simply nonlinear function from set input variables set output variables controlled vector adjustable parameters this function can represented the form network diagram shown figure the process evaluating can then interpreted forward propagation information through the network should emphasized that these diagrams not represent probabilistic graphical models the kind considered chapter because the internal nodes represent deterministic variables rather than stochastic ones for this reason have adopted slightly different graphical feed forward network functions notation for the two kinds model shall see later how give probabilistic interpretation neural network discussed section the bias parameters can absorbed into the set weight parameters defining additional input variable whose value clamped that takes the form can similarly absorb the second layer biases into the second layer weights that the overall network function becomes can seen from figure the neural network model comprises two stages processing each which resembles the perceptron model section and for this reason the neural network also known the multilayer perceptron mlp key difference compared the perceptron however that the neural network uses continuous sigmoidal nonlinearities the hidden units whereas the perceptron uses step function nonlinearities this means that the neural network function differentiable with respect the network parameters and this property will play central role network training the activation functions all the hidden units network are taken linear then for any such network can always find equivalent network without hidden units this follows from the fact that the composition successive linear transformations itself linear transformation however the number hidden units smaller than either the number input output units then the transformations that the network can generate are not the most general possible linear transformations from inputs outputs because information lost the dimensionality reduction the hidden units section show that networks linear units give rise principal component analysis general however there little interest multilayer networks linear units the network architecture shown figure the most commonly used one practice however easily generalized for instance considering additional layers processing each consisting weighted linear combination the form followed element wise transformation using nonlinear activation function note that there some confusion the literature regarding the terminology for counting the number layers such networks thus the network figure may described layer network which counts the number layers units and treats the inputs units sometimes single hidden layer network which counts the number layers hidden units recommend terminology which figure called two layer network because the number layers adaptive weights that important for determining the network properties another generalization the network architecture include skip layer connections each which associated with corresponding adaptive parameter for inputs neural networks figure example neural network having general feed forward topology note that each hidden and output unit has associated bias parameter omitted for clarity outputs instance two layer network these would directly from inputs outputs principle network with sigmoidal hidden units can always mimic skip layer connections for bounded input values using sufficiently small first layer weight that over its operating range the hidden unit effectively linear and then compensating with large weight value from the hidden unit the output practice however may advantageous include skip layer connections explicitly furthermore the network can sparse with not all possible connections within layer being present shall see example sparse network architecture when consider convolutional neural networks section because there direct correspondence between network diagram and its mathematical function can develop more general network mappings considering more complex network diagrams however these must restricted feed forward architecture other words one having closed directed cycles ensure that the outputs are deterministic functions the inputs this illustrated with simple example figure each hidden output unit such network computes function given wkjzj where the sum runs over all units that send connections unit and bias parameter included the summation for given set values applied the inputs the network successive application allows the activations all units the network evaluated including those the output units the approximation properties feed forward networks have been widely studied funahashi cybenko hornik stinchecombe and white cotter ito hornik kreinovich ripley and found very general neural networks are therefore said universal approximators for example two layer network with linear outputs can uniformly approximate any continuous function compact input domain arbitrary accuracy provided the network has sufficiently large number hidden units this result holds for wide range hidden unit activation functions but excluding polynomials although such theorems are reassuring the key problem how find suitable parameter values given set training data and later sections this chapter figure illustration the capability multilayer perceptron approximate four different functions comprising sin and where the heaviside step function each case data points shown blue dots have been sampled uniformly over the interval and the corresponding values evaluated these data points are then used train twolayer network having hidden units with tanh activation functions and linear output units the resulting network functions are shown the red curves and the outputs the three hidden units are shown the three dashed curves feed forward network functions will show that there exist effective solutions this problem based both maximum likelihood and bayesian approaches the capability two layer network model broad range functions illustrated figure this figure also shows how individual hidden units work collaboratively approximate the final function the role hidden units simple classification problem illustrated figure using the synthetic classification data set described appendix"}, "87": {"Section": "5.1.1", "Title": "Weight-space symmetries", "Content": "one property feed forward networks which will play role when consider bayesian model comparison that multiple distinct choices for the weight vector can all give rise the same mapping function from inputs outputs chen consider two layer network the form shown figure with hidden units having tanh activation functions and full connectivity both layers change the sign all the weights and the bias feeding into particular hidden unit then for given input pattern the sign the activation the hidden unit will reversed because tanh odd function that tanh tanh this transformation can exactly compensated changing the sign all the weights leading out that hidden unit thus changing the signs particular group weights and bias the input output mapping function represented the network unchanged and have found two different weight vectors that give rise the same mapping function for hidden units there will such sign flip neural networks figure example the solution simple twoclass classification problem involving synthetic data using neural network having two inputs two hidden units with tanh activation functions and single output having logistic sigmoid activation function the dashed blue lines show the contours for each the hidden units and the red line shows the decision surface for the network for comparison the green line denotes the optimal decision boundary computed from the distributions used generate the data symmetries and thus any given weight vector will one set equivalent weight vectors similarly imagine that interchange the values all the weights and the bias leading both into and out particular hidden unit with the corresponding values the weights and bias associated with different hidden unit again this clearly leaves the network input output mapping function unchanged but corresponds different choice weight vector for hidden units any given weight vector will belong set equivalent weight vectors associated with this interchange symmetry corresponding the different orderings the hidden units the network will therefore have overall weight space symmetry factor for networks with more than two layers weights the total level symmetry will given the product such factors one for each layer hidden units turns out that these factors account for all the symmetries weight space except for possible accidental symmetries due specific choices for the weight values furthermore the existence these symmetries not particular property the tanh function but applies wide range activation functions urkov and kainen many cases these symmetries weight space are little practical consequence although section shall encounter situation which need take them into account"}, "88": {"Section": "5.2", "Title": "Network Training", "Content": "far have viewed neural networks general class parametric nonlinear functions from vector input variables vector output variables simple approach the problem determining the network parameters make analogy with the discussion polynomial curve fitting section and therefore minimize sum squares error function given training set comprising set input vectors where together with corresponding set target vectors minimize the error function network training however can provide much more general view network training first giving probabilistic interpretation the network outputs have already seen many advantages using probabilistic predictions section here will also provide with clearer motivation both for the choice output unit nonlinearity and the choice error function start discussing regression problems and for the moment consider single target variable that can take any real value following the discussions section and assume that has gaussian distribution with xdependent mean which given the output the neural network that where the precision inverse variance the gaussian noise course this somewhat restrictive assumption and section shall see how extend this approach allow for more general conditional distributions for the conditional distribution given sufficient take the output unit activation function the identity because such network can approximate any continuous function from given data set independent identically distributed observations along with corresponding target values can construct the corresponding likelihood function taking the negative logarithm obtain the error function which can used learn the parameters and section shall discuss the bayesian treatment neural networks while here consider maximum likelihood approach note that the neural networks literature usual consider the minimization error function rather than the maximization the log likelihood and here shall follow this convention consider first the determination maximizing the likelihood function equivalent minimizing the sum squares error function given neural networks where have discarded additive and multiplicative constants the value found minimizing will denoted wml because corresponds the maximum likelihood solution practice the nonlinearity the network function causes the error nonconvex and practice local maxima the likelihood may found corresponding local minima the error function discussed section having found wml the value can found minimizing the negative log likelihood give wml note that this can evaluated once the iterative optimization required find wml completed have multiple target variables and assume that they are independent conditional and with shared noise precision then the conditional distribution the target values given following the same argument for single target variable see that the maximum likelihood weights are determined minimizing the sum squares error function the noise precision then given exercise exercise wml where the number target variables the assumption independence can dropped the expense slightly more complex optimization problem recall from section that there natural pairing the error function given the negative log likelihood and the output unit activation function the regression case can view the network having output activation function that the identity that the corresponding sum squares error function has the property which shall make use when discussing error backpropagation section now consider the case binary classification which have single target variable such that denotes class and denotes class following the discussion canonical link functions section consider network having single output whose activation function logistic sigmoid exp that can interpret the conditional probability with given the conditional distribution targets given inputs then bernoulli distribution the form consider training set independent observations then the error function which given the negative log likelihood then cross entropy error function the form where denotes note that there analogue the noise precision because the target values are assumed correctly labelled however the model easily extended allow for labelling errors simard found that using the cross entropy error function instead the sum squares for classification problem leads faster training well improved generalization have separate binary classifications perform then can use network having outputs each which has logistic sigmoid activation function associated with each output binary class label where assume that the class labels are independent given the input vector then the conditional distribution the targets taking the negative logarithm the corresponding likelihood function then gives the following error function tnk ynk tnk ynk where ynk denotes again the derivative the error function with respect the activation for particular output unit takes the form just the regression case interesting contrast the neural network solution this problem with the corresponding approach based linear classification model the kind discussed chapter suppose that are using standard two layer network the kind shown figure see that the weight parameters the first layer the network are shared between the various outputs whereas the linear model each classification problem solved independently the first layer the network can viewed performing nonlinear feature extraction and the sharing features between the different outputs can save computation and can also lead improved generalization finally consider the standard multiclass classification problem which each input assigned one mutually exclusive classes the binary target variables have coding scheme indicating the class and the network outputs are interpreted leading the following error function tkn network training exercise exercise exercise neural networks figure geometrical view the error function surface sitting over weight space point local minimum and the global minimum any point the local gradient the error surface given the vector following the discussion section see that the output unit activation function which corresponds the canonical link given the softmax function exp exp which satisfies and note that the are unchanged constant added all the causing the error function constant for some directions weight space this degeneracy removed appropriate regularization term section added the error function once again the derivative the error function with respect the activation for exercise particular output unit takes the familiar form summary there natural choice both output unit activation function and matching error function according the type problem being solved for regression use linear outputs and sum squares error for multiple independent binary classifications use logistic sigmoid outputs and cross entropy error function and for multiclass classification use softmax outputs with the corresponding multiclass cross entropy error function for classification problems involving two classes can use single logistic sigmoid output alternatively can use network with two outputs having softmax output activation function"}, "89": {"Section": "5.2.1", "Title": "Parameter optimization", "Content": "turn next the task finding weight vector which minimizes the chosen function this point useful have geometrical picture the error function which can view surface sitting over weight space shown figure first note that make small step weight space from then the change the error function where the vector points the direction greatest rate increase the error function because the error smooth continuous function its smallest value will occur section network training point weight space such that the gradient the error function vanishes that otherwise could make small step the direction and thereby further reduce the error points which the gradient vanishes are called stationary points and may further classified into minima maxima and saddle points our goal find vector such that takes its smallest value however the error function typically has highly nonlinear dependence the weights and bias parameters and there will many points weight space which the gradient vanishes numerically very small indeed from the discussion section see that for any point that local minimum there will other points weight space that are equivalent minima for instance two layer network the kind shown figure with hidden units each point weight space member family equivalent points furthermore there will typically multiple inequivalent stationary points and particular multiple inequivalent minima minimum that corresponds the smallest value the error function for any weight vector said global minimum any other minima corresponding higher values the error function are said local minima for successful application neural networks may not necessary find the global minimum and general will not known whether the global minimum has been found but may necessary compare several local minima order find sufficiently good solution because there clearly hope finding analytical solution the equation resort iterative numerical procedures the optimization continuous nonlinear functions widely studied problem and there exists extensive literature how solve efficiently most techniques involve choosing some initial value for the weight vector and then moving through weight space succession steps the form where labels the iteration step different algorithms involve different choices for the weight vector update many algorithms make use gradient information and therefore require that after each update the value evaluated the new weight vector order understand the importance gradient information useful consider local approximation the error function based taylor expansion"}, "90": {"Section": "5.2.2", "Title": "Local quadratic approximation", "Content": "insight into the optimization problem and into the various techniques for solving can obtained considering local quadratic approximation the error function consider the taylor expansion around some point weight space neural networks where cubic and higher terms have been omitted here defined the gradient evaluated and the hessian matrix has elements from the corresponding local approximation the gradient given for points that are sufficiently close approximations for the error and its gradient these expressions will give reasonable consider the particular case local quadratic approximation around point that minimum the error function this case there linear term because and becomes where the hessian evaluated order interpret this geometrically consider the eigenvalue equation for the hessian matrix hui iui where the eigenvectors form complete orthonormal set appendix that now expand linear combination the eigenvectors the form iui this can regarded transformation the coordinate system which the origin translated the point and the axes are rotated align with the eigenvectors through the orthogonal matrix whose columns are the and discussed more detail appendix substituting into and using and allows the error function written the form matrix said positive definite and only vthv for all the error figure the neighbourhood minimum function can approximated quadratic contours constant error are then ellipses whose axes are aligned with the eigenvectors the hessian matrix with lengths that are inversely proportional the square roots the corresponding eigenvectors network training because the eigenvectors form complete set arbitrary vector can written the form ciui from and then have vthv exercise exercise and will positive definite and only all its eigenvalues are positive the new coordinate system whose basis vectors are given the eigenvectors the contours constant are ellipses centred the origin illustrated figure for one dimensional weight space stationary point will minimum exercise the corresponding result dimensions that the hessian matrix evaluated should positive definite"}, "91": {"Section": "5.2.3", "Title": "Use of gradient information", "Content": "shall see section possible evaluate the gradient error function efficiently means the backpropagation procedure the use this gradient information can lead significant improvements the speed with which the minima the error function can located can see why this follows the quadratic approximation the error function given the error surface specified the quantities and which contain total independent elements because the matrix symmetric where the dimensionality the total number adaptive parameters the network the location the minimum this quadratic approximation therefore depends parameters and should not expect able locate the minimum until have gathered independent pieces information not make use gradient information would expect have perform function exercise neural networks evaluations each which would require steps thus the computational effort needed find the minimum using such approach would now compare this with algorithm that makes use the gradient information because each evaluation brings items information might hope find the minimum the function gradient evaluations shall see using error backpropagation each such evaluation takes only steps and the minimum can now found steps for this reason the use gradient information forms the basis practical algorithms for training neural networks"}, "92": {"Section": "5.2.4", "Title": "Gradient descent optimization", "Content": "the simplest approach using gradient information choose the weight update comprise small step the direction the negative gradient that where the parameter known the learning rate after each such update the gradient evaluated for the new weight vector and the process repeated note that the error function defined with respect training set and each step requires that the entire training set processed order evaluate techniques that use the whole data set once are called batch methods each step the weight vector moved the direction the greatest rate decrease the error function and this approach known gradient descent steepest descent although such approach might intuitively seem reasonable fact turns out poor algorithm for reasons discussed bishop and nabney for batch optimization there are more efficient methods such conjugate gradients and quasi newton methods which are much more robust and much faster than simple gradient descent gill fletcher nocedal and wright unlike gradient descent these algorithms have the property that the error function always decreases each iteration unless the weight vector has arrived local global minimum order find sufficiently good minimum may necessary run gradient based algorithm multiple times each time using different randomly chosen starting point and comparing the resulting performance independent validation set there however line version gradient descent that has proved useful practice for training neural networks large data sets cun error functions based maximum likelihood for set independent observations comprise sum terms one for each data point line gradient descent also known sequential gradient descent stochastic gradient descent makes update the weight vector based one data point time that"}, "93": {"Section": "5.3", "Title": "Error Backpropagation", "Content": "this update repeated cycling through the data either sequence selecting points random with replacement there are course intermediate scenarios which the updates are based batches data points one advantage line methods compared batch methods that the former handle redundancy the data much more efficiently see this consider extreme example which take data set and double its size duplicating every data point note that this simply multiplies the error function factor and equivalent using the original error function batch methods will require double the computational effort evaluate the batch error function gradient whereas online methods will unaffected another property line gradient descent the possibility escaping from local minima since stationary point with respect the error function for the whole data set will generally not stationary point for each data point individually nonlinear optimization algorithms and their practical application neural network training are discussed detail bishop and nabney error backpropagation our goal this section find efficient technique for evaluating the gradient error function for feed forward neural network shall see that this can achieved using local message passing scheme which information sent alternately forwards and backwards through the network and known error backpropagation sometimes simply backprop should noted that the term backpropagation used the neural computing literature mean variety different things for instance the multilayer perceptron architecture sometimes called backpropagation network the term backpropagation also used describe the training multilayer perceptron using gradient descent applied sum squares error function order clarify the terminology useful consider the nature the training process more carefully most training algorithms involve iterative procedure for minimization error function with adjustments the weights being made sequence steps each such step can distinguish between two distinct stages the first stage the derivatives the error function with respect the weights must evaluated shall see the important contribution the backpropagation technique providing computationally efficient method for evaluating such derivatives because this stage that errors are propagated backwards through the network shall use the term backpropagation specifically describe the evaluation derivatives the second stage the derivatives are then used compute the adjustments made the weights the simplest such technique and the one originally considered rumelhart involves gradient descent important recognize that the two stages are distinct thus the first stage namely the propagation errors backwards through the network order evaluate derivatives can applied many other kinds network and not just the multilayer perceptron can also applied error functions other that just the simple sum squares and the evaln wkixi ynk tnk wji ynj tnj xni where ynk the gradient this error function with respect weight wji given which can interpreted local computation involving the product error signal ynj tnj associated with the output end the link wji and the variable xni associated with the input end the link section saw how similar formula arises with the logistic sigmoid activation function together with the cross entropy error function and similarly for the softmax activation function together with its matching cross entropy error function shall now see how this simple result extends the more complex setting multilayer feed forward networks general feed forward network each unit computes weighted sum its inputs the form wjizi neural networks uation other derivatives such the jacobian and hessian matrices shall see later this chapter similarly the second stage weight adjustment using the calculated derivatives can tackled using variety optimization schemes many which are substantially more powerful than simple gradient descent"}, "94": {"Section": "5.3.1", "Title": "Evaluation of error-function derivatives", "Content": "now derive the backpropagation algorithm for general network having arbitrary feed forward topology arbitrary differentiable nonlinear activation functions and broad class error function the resulting formulae will then illustrated using simple layered network structure having single layer sigmoidal hidden units together with sum squares error many error functions practical interest for instance those defined maximum likelihood for set data comprise sum terms one for each data point the training set that here shall consider the problem evaluating for one such term the error function this may used directly for sequential optimization the results can accumulated over the training set the case batch methods consider first simple linear model which the outputs are linear combinations the input variables that together with error function that for particular input pattern takes the form error backpropagation where the activation unit input that sends connection unit and wji the weight associated with that connection section saw that biases can included this sum introducing extra unit input with activation fixed therefore not need deal with biases explicitly the sum transformed nonlinear activation function give the activation unit the form note that one more the variables the sum could input and similarly the unit could output for each pattern the training set shall suppose that have supplied the corresponding input vector the network and calculated the activations all the hidden and output units the network successive application and this process often called forward propagation because can regarded forward flow information through the network now consider the evaluation the derivative with respect weight wji the outputs the various units will depend the particular input pattern however order keep the notation uncluttered shall omit the subscript from the network variables first note that depends the weight wji only via the summed input unit can therefore apply the chain rule for partial derivatives give wji wji now introduce useful notation where the are often referred errors for reasons shall see shortly using can write wji substituting and into then obtain wji jzi equation tells that the required derivative obtained simply multiplying the value for the unit the output end the weight the value for the unit the input end the weight where the case bias note that this takes the same form for the simple linear model considered the start this section thus order evaluate the derivatives need only calculate the value for each hidden and output unit the network and then apply have seen already for the output units have where the sum runs over all units which unit sends connections the arrangement units and weights illustrated figure note that the units labelled could include other hidden units and output units writing down are making use the fact that variations give rise variations the error function only through variations the variables now substitute the definition given into and make use and obtain the following backpropagation formula wkj which tells that the value for particular hidden unit can obtained propagating the backwards from units higher the network illustrated figure note that the summation taken over the first index wkj corresponding backward propagation information through the network whereas the forward propagation equation taken over the second index because already know the values the for the output units follows that recursively applying can evaluate the for all the hidden units feed forward network regardless its topology the backpropagation procedure can therefore summarized follows error backpropagation apply input vector the network and forward propagate through the network using and find the activations all the hidden and output units evaluate the for all the output units using backpropagate the using obtain for each hidden unit the network use evaluate the required derivatives neural networks figure illustration the calculation for hidden unit backpropagation the from those units which unit sends connections the blue arrow denotes the direction information flow during forward propagation and the red arrows indicate the backward propagation error information wji wkj provided are using the canonical link the output unit activation function evaluate the for hidden units again make use the chain rule for partial derivatives error backpropagation for batch methods the derivative the total error can then obtained repeating the above steps for each pattern the training set and then summing over all patterns wji wji the above derivation have implicitly assumed that each hidden output unit the network has the same activation function the derivation easily generalized however allow different units have individual activation functions simply keeping track which form goes with which unit"}, "95": {"Section": "5.3.2", "Title": "A simple example", "Content": "the above derivation the backpropagation procedure allowed for general forms for the error function the activation functions and the network topology order illustrate the application this algorithm shall consider particular example this chosen both for its simplicity and for its practical importance because many applications neural networks reported the literature make use this type network specifically shall consider two layer network the form illustrated figure together with sum squares error which the output units have linear activation functions that while the hidden units have logistic sigmoid activation functions given where useful feature this function that its derivative can expressed particularly simple form also consider standard sum squares error function that for pattern the error given tanh tanh tanh where the activation output unit and the corresponding target for particular input pattern for each pattern the training set turn first perform forward propagation using next compute the for each output unit using then backpropagate these obtain for the hidden units using neural networks wkj finally the derivatives with respect the first layer and second layer weights are given jxi kzj"}, "96": {"Section": "5.3.3", "Title": "Efficiency of backpropagation", "Content": "one the most important aspects backpropagation its computational efficiency understand this let examine how the number computer operations required evaluate the derivatives the error function scales with the total number weights and biases the network single evaluation the error function for given input pattern would require operations for sufficiently large this follows from the fact that except for network with very sparse connections the number weights typically much greater than the number units and the bulk the computational effort forward propagation concerned with evaluating the sums with the evaluation the activation functions representing small overhead each term the sum requires one multiplication and one addition leading overall computational cost that alternative approach backpropagation for computing the derivatives the error function use finite differences this can done perturbing each weight turn and approximating the derivatives the expression wji wji wji where software simulation the accuracy the approximation the derivatives can improved making smaller until numerical roundoff problems arise the accuracy the finite differences method can improved significantly using symmetrical central differences the form wji wji wji exercise this case the corrections cancel can verified taylor expansion the right hand side and the residual corrections are the number computational steps however roughly doubled compared with the main problem with numerical differentiation that the highly desirable scaling has been lost each forward propagation requires steps and figure illustration modular pattern recognition system which the jacobian matrix can used backpropagate error signals from the outputs through earlier modules the system error backpropagation there are weights the network each which must perturbed individually that the overall scaling however numerical differentiation plays important role practice because comparison the derivatives calculated backpropagation with those obtained using central differences provides powerful check the correctness any software implementation the backpropagation algorithm when training networks practice derivatives should evaluated using backpropagation because this gives the greatest accuracy and numerical efficiency however the results should compared with numerical differentiation using for some test cases order check the correctness the implementation"}, "97": {"Section": "5.3.4", "Title": "The Jacobian matrix", "Content": "have seen how the derivatives error function with respect the weights can obtained the propagation errors backwards through the network the technique backpropagation can also applied the calculation other derivatives here consider the evaluation the jacobian matrix whose elements are given the derivatives the network outputs with respect the inputs jki where each such derivative evaluated with all other inputs held fixed jacobian matrices play useful role systems built from number distinct modules illustrated figure each module can comprise fixed adaptive function which can linear nonlinear long differentiable suppose wish minimize error function with respect the parameter figure the derivative the error function given which the jacobian matrix for the red module figure appears the middle term because the jacobian matrix provides measure the local sensitivity the outputs changes each the input variables also allows any known errors associated with the inputs propagated through the trained network order estimate their contribution the errors the outputs through the relation which valid provided the are small general the network mapping represented trained neural network will nonlinear and the elements the jacobian matrix will not constants but will depend the particular input vector used thus valid only for small perturbations the inputs and the jacobian itself must evaluated for each new input vector the jacobian matrix can evaluated using backpropagation procedure that similar the one derived earlier for evaluating the derivatives error function with respect the weights start writing the element jki the form jki wji where have made use the sum runs over all units which the input unit sends connections for example over all units the first hidden layer the layered topology considered earlier now write down recursive backpropagation formula determine the derivatives neural networks wlj where the sum runs over all units which unit sends connections corresponding the first index wlj again have made use and this backpropagation starts the output units for which the required derivatives can found directly from the functional form the output unit activation function for instance have individual sigmoidal activation functions each output unit then whereas for softmax outputs have kjyk ykyj can summarize the procedure for evaluating the jacobian matrix follows apply the input vector corresponding the point input space which the jacobian matrix found and forward propagate the usual way obtain the"}, "98": {"Section": "5.4", "Title": "The Hessian Matrix", "Content": "activations all the hidden and output units the network next for each row the jacobian matrix corresponding the output unit backpropagate using the recursive relation starting with for all the hidden units the network finally use the backpropagation the inputs the jacobian can also evaluated using alternative forward propagation formalism which can derived analogous way the backpropagation approach given here again the implementation such algorithms can checked using numerical differentiation the form which involves forward propagations for network having inputs exercise the hessian matrix have shown how the technique backpropagation can used obtain the first derivatives error function with respect the weights the network backpropagation can also used evaluate the second derivatives the error given wji wlk note that sometimes convenient consider all the weight and bias parameters elements single vector denoted which case the second derivatives form the elements hij the hessian matrix where and the total number weights and biases the hessian plays important role many aspects neural computing including the following several nonlinear optimization algorithms used for training neural networks are based considerations the second order properties the error surface which are controlled the hessian matrix bishop and nabney the hessian forms the basis fast procedure for training feed forward network following small change the training data bishop the inverse the hessian has been used identify the least significant weights network part network pruning algorithms cun the hessian plays central role the laplace approximation for bayesian neural network see section its inverse used determine the predictive distribution for trained network its eigenvalues determine the values hyperparameters and its determinant used evaluate the model evidence various approximation schemes have been used evaluate the hessian matrix for neural network however the hessian can also calculated exactly using extension the backpropagation technique neural networks important consideration for many applications the hessian the efficiency with which can evaluated there are parameters weights and biases the network then the hessian matrix has dimensions and the computational effort needed evaluate the hessian will scale like for each pattern the data set shall see there are efficient methods for evaluating the hessian whose scaling indeed"}, "99": {"Section": "5.4.1", "Title": "Diagonal approximation", "Content": "some the applications for the hessian matrix discussed above require the inverse the hessian rather than the hessian itself for this reason there has been some interest using diagonal approximation the hessian other words one that simply replaces the off diagonal elements with zeros because its inverse trivial evaluate again shall consider error function that consists sum terms one for each pattern the data set that the hessian can then obtained considering one pattern time and then summing the results over all patterns from the diagonal elements the hessian for pattern can written using and the second derivatives the right hand side can found recursively using the chain rule differential calculus give backpropagation equation the form wkjwk wkj now neglect off diagonal elements the second derivative terms obtain becker and cun cun wkj note that the number computational steps required evaluate this approximation where the total number weight and bias parameters the network compared with for the full hessian ricotti also used the diagonal approximation the hessian but they retained all terms the evaluation and obtained exact expressions for the diagonal terms note that this longer has scaling the major problem with diagonal approximations however that practice the hessian typically found strongly nondiagonal and these approximations which are driven mainly computational convenience must treated with care the hessian matrix"}, "100": {"Section": "5.4.2", "Title": "Outer product approximation", "Content": "when neural networks are applied regression problems common use sum squares error function the form where have considered the case single output order keep the notation simple the extension several outputs straightforward can then write the hessian matrix the form the network has been trained the data set and its outputs happen very close the target values then the second term will small and can neglected more generally however may appropriate neglect this term the following argument recall from section that the optimal function that minimizes sum squares loss the conditional average the target data the quantity then random variable with zero mean assume that its value uncorrelated with the value the second derivative term the right hand side then the whole term will average zero the summation over neglecting the second term arrive the levenberg marquardt approximation outer product approximation because the hessian matrix built from sum outer products vectors given bnbt where because the activation function for the output units simply the identity evaluation the outer product approximation for the hessian straightforward only involves first derivatives the error function which can evaluated efficiently steps using standard backpropagation the elements the matrix can then found steps simple multiplication important emphasize that this approximation only likely valid for network that has been trained appropriately and that for general network mapping the second derivative terms the right hand side will typically not negligible the case the cross entropy error function for network with logistic sigmoid output unit activation functions the corresponding approximation given exercise exercise exercise exercise analogous result can obtained for multiclass networks having softmax outputunit activation functions bnbt neural networks"}, "101": {"Section": "5.4.3", "Title": "Inverse Hessian", "Content": "can use the outer product approximation develop computationally efficient procedure for approximating the inverse the hessian hassibi and stork first write the outer product approximation matrix notation bnbt where wan the contribution the gradient the output unit activation arising from data point now derive sequential procedure for building the hessian including data points one time suppose have already obtained the inverse hessian using the first data points separating off the contribution from data point obtain order evaluate the inverse the hessian now consider the matrix identity vvt vtm vtm where the unit matrix which simply special case the woodbury identity now identify with and with obtain exercise this way data points are sequentially absorbed until and the whole data set has been processed this result therefore represents procedure for evaluating the inverse the hessian using single pass through the data set the initial matrix chosen where small quantity that the algorithm actually finds the inverse the results are not particularly sensitive the precise value extension this algorithm networks having more than one output straightforward note here that the hessian matrix can sometimes calculated indirectly part the network training algorithm particular quasi newton nonlinear optimization algorithms gradually build approximation the inverse the hessian during training such algorithms are discussed detail bishop and nabney"}, "102": {"Section": "5.4.4", "Title": "Finite differences", "Content": "the case the first derivatives the error function can find the second derivatives using finite differences with accuracy limited numerical precision perturb each possible pair weights turn obtain wji wlk wji wlk wji wlk wji wlk wji wlk the hessian matrix again using symmetrical central differences formulation ensure that the residual errors are rather than because there are elements the hessian matrix and because the evaluation each element requires four forward propagations each needing operations per pattern see that this approach will require operations evaluate the complete hessian therefore has poor scaling properties although practice very useful check the software implementation backpropagation methods more efficient version numerical differentiation can found applying central differences the first derivatives the error function which are themselves calculated using backpropagation this gives wji wlk wji wlk wji wlk because there are now only weights perturbed and because the gradients can evaluated steps see that this method gives the hessian operations"}, "103": {"Section": "5.4.5", "Title": "Exact evaluation of the Hessian", "Content": "far have considered various approximation schemes for evaluating the hessian matrix its inverse the hessian can also evaluated exactly for network arbitrary feed forward topology using extension the technique backpropagation used evaluate first derivatives which shares many its desirable features including computational efficiency bishop bishop can applied any differentiable error function that can expressed function the network outputs and networks having arbitrary differentiable activation functions the number computational steps needed evaluate the hessian scales like similar algorithms have also been considered buntine and weigend here consider the specific case network having two layers weights for which the required equations are easily derived shall use indices and denote inputs indices and denoted hidden units and indices and denote outputs first define mkk where the contribution the error from data point the hessian matrix for this network can then considered three separate blocks follows both weights the second layer zjzj mkk exercise neural networks both weights the first layer xixi ijj xixi mkk one weight each layer xih kijj hkk exercise here ijj the element the identity matrix one both the weights bias term then the corresponding expressions are obtained simply setting the appropriate activation inclusion skip layer connections straightforward"}, "104": {"Section": "5.4.6", "Title": "Fast multiplication by the Hessian", "Content": "for many applications the hessian the quantity interest not the hessian matrix itself but the product with some vector have seen that the evaluation the hessian takes operations and also requires storage that the vector vth that wish calculate however has only elements instead computing the hessian intermediate step can instead try find efficient approach evaluating vth directly way that requires only operations this first note that vth where denotes the gradient operator weight space can then write down the standard forward propagation and backpropagation equations for the evaluation and apply these equations give set forward propagation and backpropagation equations for the evaluation vth ller pearlmutter this corresponds acting the original forward propagation and backpropagation equations with differential operator pearlmutter used the notation denote the operator and shall follow this convention the analysis straightforward and makes use the usual rules differential calculus together with the result the technique best illustrated with simple example and again choose two layer network the form shown figure with linear output units and sum squares error function before consider the contribution the error function from one pattern the data set the required vector then obtained the hessian matrix usual summing over the contributions from each the patterns separately for the two layer network the forward propagation equations are given now act these equations using the operator obtain set forward propagation equations the form wjixi wkjzj vjixi wkjr vkjzj where vji the element the vector that corresponds the weight wji quantities the form and are regarded new variables whose values are found using the above equations because are considering sum squares error function have the following standard backpropagation expressions wkj again act these equations with the operator obtain set backpropagation equations the form wkj vkj wkjr finally have the usual equations for the first derivatives the error wkj wji kzj jxi and acting these with the operator obtain expressions for the elements the vector vth xir wkj wji neural networks the implementation this algorithm involves the introduction additional variables and for the hidden units and and for the output units for each input pattern the values these quantities can found using the above results and the elements vth are then given and elegant aspect this technique that the equations for evaluating vth mirror closely those for standard forward and backward propagation and the extension existing software compute this product typically straightforward desired the technique can used evaluate the full hessian matrix choosing the vector given successively series unit vectors the form each which picks out one column the hessian this leads formalism that analytically equivalent the backpropagation procedure bishop described section though with some loss efficiency due redundant calculations"}, "105": {"Section": "5.5", "Title": "Regularization in Neural Networks", "Content": "the number input and outputs units neural network generally determined the dimensionality the data set whereas the number hidden units free parameter that can adjusted give the best predictive performance note that controls the number parameters weights and biases the network and might expect that maximum likelihood setting there will optimum value that gives the best generalization performance corresponding the optimum balance between under fitting and over fitting figure shows example the effect different values for the sinusoidal regression problem the generalization error however not simple function due the presence local minima the error function illustrated figure here see the effect choosing multiple random initializations for the weight vector for range values the overall best validation set performance this case occurred for particular solution having practice one approach choosing fact plot graph the kind shown figure and then choose the specific solution having the smallest validation set error there are however other ways control the complexity neural network model order avoid over fitting from our discussion polynomial curve fitting chapter see that alternative approach choose relatively large value for and then control complexity the addition regularization term the error function the simplest regularizer the quadratic giving regularized error the form wtw this regularizer also known weight decay and has been discussed length chapter the effective model complexity then determined the choice the regularization coefficient have seen previously this regularizer can interpreted the negative logarithm zero mean gaussian prior distribution over the weight vector"}, "106": {"Section": "5.5.1", "Title": "Consistent Gaussian priors", "Content": "one the limitations simple weight decay the form that inconsistent with certain scaling properties network mappings illustrate this consider multilayer perceptron network having two layers weights and linear output units which performs mapping from set input variables set output variables the activations the hidden units the first hidden layer figure plot the sum squares test set error for the polynomial data set versus the number hidden units the network with random starts for each network size showing the effect local minima for each new start the weight vector was initialized sampling from isotropic gaussian distribution having mean zero and variance regularization neural networks figure examples two layer networks trained data points drawn from the sinusoidal data set the graphs show the result fitting networks having and hidden units respectively minimizing sum squares error function using scaled conjugate gradient algorithm wji neural networks take the form while the activations the output units are given wjixi wkjzj suppose perform linear transformation the input data the form axi exercise then can arrange for the mapping performed the network unchanged making corresponding linear transformation the weights and biases from the inputs the units the hidden layer the form similarly linear transformation the output variables the network the form can achieved making transformation the second layer weights and biases using wji wji wji cyk wkj wkj cwkj cwk train one network using the original data and one network using data for which the input and target variables are transformed one the above linear transformations then consistency requires that should obtain equivalent networks that differ only the linear transformation the weights given any regularizer should consistent with this property otherwise arbitrarily favours one solution over another equivalent one clearly simple weight decay that treats all weights and biases equal footing does not satisfy this property therefore look for regularizer which invariant under the linear transformations and these require that the regularizer should invariant scaling the weights and shifts the biases such regularizer given where denotes the set weights the first layer denotes the set weights the second layer and biases are excluded from the summations this regularizer regularization neural networks will remain unchanged under the weight transformations provided the regularization parameters are scaled using and the regularizer corresponds prior the form exp note that priors this form are improper they cannot normalized because the bias parameters are unconstrained the use improper priors can lead difficulties selecting regularization coefficients and model comparison within the bayesian framework because the corresponding evidence zero therefore common include separate priors for the biases which then break shift invariance having their own hyperparameters can illustrate the effect the resulting four hyperparameters drawing samples from the prior and plotting the corresponding network functions shown figure more generally can consider priors which the weights are divided into any number groups that exp where special case this prior choose the groups correspond the sets weights associated with each the input units and optimize the marginal likelihood with respect the corresponding parameters obtain automatic relevance determination discussed section"}, "107": {"Section": "5.5.2", "Title": "Early stopping", "Content": "alternative regularization way controlling the effective complexity network the procedure early stopping the training nonlinear network models corresponds iterative reduction the error function defined with respect set training data for many the optimization algorithms used for network training such conjugate gradients the error nonincreasing function the iteration index however the error measured with respect independent data generally called validation set often shows decrease first followed increase the network starts over fit training can therefore stopped the point smallest error with respect the validation data set indicated figure order obtain network having good generalization performance the behaviour the network this case sometimes explained qualitatively terms the effective number degrees freedom the network which this number starts out small and then grows during the training process corresponding steady increase the effective complexity the model halting training before neural networks figure illustration the effect the hyperparameters governing the prior distribution over weights and biases two layer network having single input single linear output and hidden units having tanh which represent activation functions the priors are governed four hyperparameters the precisions the gaussian distributions the first layer biases first layer weights second layer biases and governs the vertical scale functions note second layer weights respectively see that the parameter the different vertical axis ranges the top two diagrams governs the horizontal scale variations the whose function values and effect not illustrated here governs the range vertical offsets the functions governs the horizontal range over which variations occur the parameter and minimum the training error has been reached then represents way limiting the effective network complexity the case quadratic error function can verify this insight and show that early stopping should exhibit similar behaviour regularization using simple weight decay term this can understood from figure which the axes weight space have been rotated parallel the eigenvectors the hessian matrix the absence weight decay the weight vector starts the origin and proceeds during training along path that follows the local negative gradient vector then the weight vector will move initially parallel the axis through point and then move towards the minimum the error funccorresponding roughly tion wml this follows from the shape the error surface and the widely differing therefore similar weight eigenvalues the hessian stopping point near decay the relationship between early stopping and weight decay can made quantitative thereby showing that the quantity where the iteration index and the learning rate parameter plays the role the reciprocal the regularization exercise regularization neural networks figure illustration the behaviour training set error left and validation set error right during typical training session function the iteration step for the sinusoidal data set the goal achieving the best generalization performance suggests that training should stopped the point shown the vertical dashed lines corresponding the minimum the validation set error parameter the effective number parameters the network therefore grows during the course training"}, "108": {"Section": "5.5.3", "Title": "Invariances", "Content": "many applications pattern recognition known that predictions should unchanged invariant under one more transformations the input variables for example the classification objects two dimensional images such handwritten digits particular object should assigned the same classification irrespective its position within the image translation invariance its size scale invariance such transformations produce significant changes the raw data expressed terms the intensities each the pixels the image and yet should give rise the same output from the classification system similarly speech recognition small levels nonlinear warping along the time axis which preserve temporal ordering should not change the interpretation the signal sufficiently large numbers training patterns are available then adaptive model such neural network can learn the invariance least approximately this involves including within the training set sufficiently large number examples the effects the various transformations thus for translation invariance image the training set should include examples objects many different positions this approach may impractical however the number training examples limited there are several invariants because the number combinations transformations grows exponentially with the number such transformations therefore seek alternative approaches for encouraging adaptive model exhibit the required invariances these can broadly divided into four categories the training set augmented using replicas the training patterns transformed according the desired invariances for instance our digit recognition example could make multiple copies each example which the wml neural networks figure schematic illustration why early stopping can give similar results weight decay the case quadratic error function the ellipse shows contour constant error and wml denotes the minimum the error function the weight vector starts the origin and moves according the local negative gradient direction then will follow the path shown the curve stopping training early weight vector found that qualitatively similar that obtained with simple weight decay regularizer and training the minimum the regularized error can seen comparing with figure digit shifted different position each image regularization term added the error function that penalizes changes the model output when the input transformed this leads the technique"}, "109": {"Section": "5.5.4", "Title": "Tangent propagation", "Content": "invariance built into the pre processing extracting features that are invariant under the required transformations any subsequent regression classification system that uses such features inputs will necessarily also respect these invariances the final option build the invariance properties into the structure neural network into the definition kernel function the case techniques such the relevance vector machine one way achieve this through the use local receptive fields and shared weights discussed the context convolutional neural networks section approach often relatively easy implement and can used encourage complex invariances such those illustrated figure for sequential training algorithms this can done transforming each input pattern before presented the model that the patterns are being recycled different transformation drawn from appropriate distribution added each time for batch methods similar effect can achieved replicating each data point number times and transforming each copy independently the use such augmented data can lead significant improvements generalization simard although can also computationally costly approach leaves the data set unchanged but modifies the error function through the addition regularizer section shall show that this approach closely related approach regularization neural networks figure illustration the synthetic warping handwritten digit the original image shown the left the right the top row shows three examples warped digits with the corresponding displacement fields shown the bottom row these displacement fields are generated sampling random displacements each pixel and then smoothing convolution with gaussians width and respectively one advantage approach that can correctly extrapolate well beyond the range transformations included the training set however can difficult find hand crafted features with the required invariances that not also discard information that can useful for discrimination tangent propagation can use regularization encourage models invariant transformations the input through the technique tangent propagation simard consider the effect transformation particular input vector provided the transformation continuous such translation rotation but not mirror reflection for instance then the transformed pattern will sweep out manifold within the dimensional input space this illustrated figure for the case for simplicity suppose the transformation governed single parameter which might rotation angle for instance then the subspace swept out figure illustration two dimensional input space showing the effect continuous transformation particular input vector onedimensional transformation parameterized the continuous variable applied causes sweep out one dimensional manifold locally the effect the transformation can approximated the tangent vector neural networks will one dimensional and will parameterized let the vector that results from acting this transformation denoted which defined that then the tangent the curve given the directional derivative and the tangent vector the point given under transformation the input vector the network output vector will general change the derivative output with respect given jki where jki the element the jacobian matrix discussed section the result can used modify the standard error function encourage local invariance the neighbourhood the data points the addition the original error function regularization function give total error function the form where regularization coefficient and ynk jnki exercise the regularization function will zero when the network mapping function invariant under the transformation the neighbourhood each pattern vector and the value the parameter determines the balance between fitting the training data and learning the invariance property practical implementation the tangent vector can approximated using finite differences subtracting the original vector from the corresponding vector after transformation using small value and then dividing this illustrated figure the regularization function depends the network weights through the jacobian backpropagation formalism for computing the derivatives the regularizer with respect the network weights easily obtained extension the techniques introduced section the transformation governed parameters for the case translations combined with plane rotations two dimensional image then the manifold will have dimensionality and the corresponding regularizer given the sum terms the form one for each transformation several transformations are considered the same time and the network mapping made invariant each separately then will locally invariant combinations the transformations simard regularization neural networks figure illustration showing the original image handwritten digit the tangent vector corresponding infinitesimal clockwise rotation the result adding small contribution from the tangent vector the original image giving with degrees and the true image rotated for comparison related technique called tangent distance can used build invariance properties into distance based methods such nearest neighbour classifiers simard"}, "110": {"Section": "5.5.5", "Title": "Training with transformed data", "Content": "have seen that one way encourage invariance model set transformations expand the training set using transformed versions the original input patterns here show that this approach closely related the technique tangent propagation bishop leen section shall consider transformation governed single parameter and described the function with shall also consider sum squares error function the error function for untransformed inputs can written the infinite data set limit the form discussed section here have considered network having single output order keep the notation uncluttered now consider infinite number copies each data point each which perturbed the transformation neural networks which the parameter drawn from distribution then the error function defined over this expanded data set can written now assume that the distribution has zero mean with small variance that are only considering small transformations the original input vectors can then expand the transformation function taylor series powers give where denotes the second derivative with respect evaluated this allows expand the model function give substituting into the mean error function and expanding then have because the distribution transformations has zero mean have also shall denote omitting terms the average error function then becomes where the original sum squares error and the regularization term takes the form which have performed the integration over can further simplify this regularization term follows section saw that the function that minimizes the sum squares error given the conditional average the target values from see that the regularized error will equal the unregularized sum squares plus terms which are and the network function that minimizes the total error will have the form thus leading order the first term the regularizer vanishes and are left with which equivalent the tangent propagation regularizer consider the special case which the transformation the inputs simply consists the addition random noise that then the regularizer takes the form which known tikhonov regularization tikhonov and arsenin bishop derivatives this regularizer with respect the network weights can found using extended backpropagation algorithm bishop see that for small noise amplitudes tikhonov regularization related the addition random noise the inputs which has been shown improve generalization appropriate circumstances sietsma and dow"}, "111": {"Section": "5.5.6", "Title": "Convolutional networks", "Content": "another approach creating models that are invariant certain transformation the inputs build the invariance properties into the structure neural network this the basis for the convolutional neural network cun lecun which has been widely applied image data consider the specific task recognizing handwritten digits each input image comprises set pixel intensity values and the desired output posterior probability distribution over the ten digit classes know that the identity the digit invariant under translations and scaling well small rotations furthermore the network must also exhibit invariance more subtle transformations such elastic deformations the kind illustrated figure one simple approach would treat the image the input fully connected network such the kind shown figure given sufficiently large training set such network could principle yield good solution this problem and would learn the appropriate invariances example however this approach ignores key property images which that nearby pixels are more strongly correlated than more distant pixels many the modern approaches computer vision exploit this property extracting local features that depend only small subregions the image information from such features can then merged later stages processing order detect higher order features regularization neural networks exercise neural networks input image convolutional layer sub sampling layer figure diagram illustrating part convolutional neural network showing layer convolutional units followed layer subsampling units several successive pairs such layers may used and ultimately yield information about the image whole also local features that are useful one region the image are likely useful other regions the image for instance the object interest translated these notions are incorporated into convolutional neural networks through three mechanisms local receptive fields weight sharing and iii subsampling the structure convolutional network illustrated figure the convolutional layer the units are organized into planes each which called feature map units feature map each take inputs only from small subregion the image and all the units feature map are constrained share the same weight values for instance feature map might consist units arranged grid with each unit taking inputs from pixel patch the image the whole feature map therefore has adjustable weight parameters plus one adjustable bias parameter input values from patch are linearly combined using the weights and the bias and the result transformed sigmoidal nonlinearity using think the units feature detectors then all the units feature map detect the same pattern but different locations the input image due the weight sharing the evaluation the activations these units equivalent convolution the image pixel intensities with kernel comprising the weight parameters the input image shifted the activations the feature map will shifted the same amount but will otherwise unchanged this provides the basis for the approximate invariance regularization neural networks the network outputs translations and distortions the input image because will typically need detect multiple features order build effective model there will generally multiple feature maps the convolutional layer each having its own set weight and bias parameters the outputs the convolutional units form the inputs the subsampling layer the network for each feature map the convolutional layer there plane units the subsampling layer and each unit takes inputs from small receptive field the corresponding feature map the convolutional layer these units perform subsampling for instance each subsampling unit might take inputs from unit region the corresponding feature map and would compute the average those inputs multiplied adaptive weight with the addition adaptive bias parameter and then transformed using sigmoidal nonlinear activation function the receptive fields are chosen contiguous and nonoverlapping that there are half the number rows and columns the subsampling layer compared with the convolutional layer this way the response unit the subsampling layer will relatively insensitive small shifts the image the corresponding regions the input space practical architecture there may several pairs convolutional and subsampling layers each stage there larger degree invariance input transformations compared the previous layer there may several feature maps given convolutional layer for each plane units the previous subsampling layer that the gradual reduction spatial resolution then compensated increasing number features the final layer the network would typically fully connected fully adaptive layer with softmax output nonlinearity the case multiclass classification the whole network can trained error minimization using backpropagation evaluate the gradient the error function this involves slight modification the usual backpropagation algorithm ensure that the shared weight constraints are satisfied due the use local receptive fields the number weights the network smaller than the network were fully connected furthermore the number independent parameters learned from the data much smaller still due the substantial numbers constraints the weights"}, "112": {"Section": "5.5.7", "Title": "Soft weight sharing", "Content": "one way reduce the effective complexity network with large number weights constrain weights within certain groups equal this the technique weight sharing that was discussed section way building translation invariance into networks used for image interpretation only applicable however particular problems which the form the constraints can specified advance here consider form soft weight sharing nowlan and hinton which the hard constraint equal weights replaced form regularization which groups weights are encouraged have similar values furthermore the division weights into groups the mean weight value for each group and the spread values within the groups are all determined part the learning process exercise neural networks section exercise recall that the simple weight decay regularizer given can viewed the negative log gaussian prior distribution over the weights can encourage the weight values form several groups rather than just one group considering instead probability distribution that mixture gaussians the centres and variances the gaussian components well the mixing coefficients will considered adjustable parameters determined part the learning process thus have probability density the form where and are the mixing coefficients taking the negative logarithm then leads regularization function the form the total error function then given where the regularization coefficient this error minimized both with respect the weights and with respect the parameters the mixture model the weights were constant then the parameters the mixture model could determined using the algorithm discussed chapter however the distribution weights itself evolving during the learning process and avoid numerical instability joint optimization performed simultaneously over the weights and the mixture model parameters this can done using standard optimization algorithm such conjugate gradients quasi newton methods order minimize the total error function necessary able evaluate its derivatives with respect the various adjustable parameters this convenient regard the prior probabilities and introduce the corresponding posterior probabilities which following are given bayes theorem the form the derivatives the total error function with respect the weights are then given account the constraints easily computed give exercise exercise regularization neural networks the effect the regularization term therefore pull each weight towards the centre the jth gaussian with force proportional the posterior probability that gaussian for the given weight this precisely the kind effect that are seeking derivatives the error with respect the centres the gaussians are also which has simple intuitive interpretation because pushes towards average the weight values weighted the posterior probabilities that the respective weight parameters were generated component similarly the derivatives with respect the variances are given which drives towards the weighted average the squared deviations the weights around the corresponding centre where the weighting coefficients are again given the posterior probability that each weight generated component note that practical implementation new variables defined exp are introduced and the minimization performed with respect the this ensures that the parameters remain positive also has the effect discouraging pathological solutions which one more the goes zero corresponding gaussian component collapsing onto one the weight parameter values such solutions are discussed more detail the context gaussian mixture models section for the derivatives with respect the mixing coefficients need take which follow from the interpretation the prior probabilities this can done expressing the mixing coefficients terms set auxiliary variables using the softmax function given exp exp exercise the derivatives the regularized error function with respect the then take the form neural networks figure the left figure shows two link robot arm which the cartesian coordinates the end effector are determined uniquely the two joint angles and and the fixed lengths and the arms this know the forward kinematics the arm practice have find the joint angles that will give rise desired end effector position and shown the right figure this inversekinematicshas two solutions corresponding elbow and elbow down exercise see that therefore driven towards the average posterior probability for component"}, "113": {"Section": "5.6", "Title": "Mixture Density Networks", "Content": "the goal supervised learning model conditional distribution which for many simple regression problems chosen gaussian however practical machine learning problems can often have significantly non gaussian distributions these can arise for example with inverse problems which the distribution can multimodal which case the gaussian assumption can lead very poor predictions simple example inverse problem consider the kinematics robot arm illustrated figure the forward problem involves finding the end effector position given the joint angles and has unique solution however practice wish move the end effector the robot specific position and this must set appropriate joint angles therefore need solve the inverse problem which has two solutions seen figure forward problems often corresponds causality physical system and generally have unique solution for instance specific pattern symptoms the human body may caused the presence particular disease pattern recognition however typically have solve inverse problem such trying predict the presence disease given set symptoms the forward problem involves many one mapping then the inverse problem will have multiple solutions for instance several different diseases may result the same symptoms the robotics example the kinematics defined geometrical equations and the multimodality readily apparent however many machine learning problems the presence multimodality particularly problems involving spaces high dimensionality can less obvious for tutorial purposes however shall consider simple toy problem for which can easily visualize the multimodality data for this problem generated sampling variable uniformly over the interval give set values and the corresponding target values are obtained elbow elbow down mixture density networks figure the left the data set for simple forward problem which the red curve shows the result fitting two layer neural network minimizing the sum squares error function the corresponding inverse problem shown the right obtained exchanging the roles and here the same network trained again minimizing the sum squares error function gives very poor fit the data due the multimodality the data set computing the function sin and then adding uniform noise over the interval the inverse problem then obtained keeping the same data points but exchanging the roles and figure shows the data sets for the forward and inverse problems along with the results fitting two layer neural networks having hidden units and single linear output unit minimizing sumof squares error function least squares corresponds maximum likelihood under gaussian assumption see that this leads very poor model for the highly non gaussian inverse problem therefore seek general framework for modelling conditional probability distributions this can achieved using mixture model for which both the mixing coefficients well the component densities are flexible functions the input vector giving rise the mixture density network for any given value the mixture model provides general formalism for modelling arbitrary conditional density function provided consider sufficiently flexible network then have framework for approximating arbitrary conditional distributions here shall develop the model explicitly for gaussian components that this example heteroscedastic model since the noise variance the data function the input vector instead gaussians can use other distributions for the components such bernoulli distributions the target variables are binary rather than continuous have also specialized the case isotropic covariances for the components although the mixture density network can readily extended allow for general covariance matrices representing the covariances using cholesky factorization williams even with isotropic components the conditional distribution does not assume factorization with respect the components contrast the standard sum squares regression model consequence the mixture distribution now take the various parameters the mixture model namely the mixing governed coefficients the means and the variances neural networks figure the mixturedensitynetwork can represent general conditional probability densities considering parametric mixture model for the distribution whose parameters are determined the outputs neural network that takes its input vector the outputs conventional neural network that takes its input the structure this mixture density network illustrated figure the mixture density network closely related the mixture experts discussed section the principle difference that the mixture density network the same function used predict the parameters all the component densities well the mixing coefficients and the nonlinear hidden units are shared amongst the input dependent functions the neural network figure can for example two layer network having sigmoidal tanh hidden units there are components the mixture model and has components then the network will have output unit that determine the mixing coefficients outputs activations denoted that determine the kernel widths and outputs denoted denoted that determine the components the kernel centres the total number network outputs given compared with the usual outputs for network which simply predicts the conditional means the target variables the mixing coefficients must satisfy the constraints which can achieved using set softmax outputs exp exp similarly the variances must satisfy the exponentials the corresponding network activations using and can represented terms finally because the means have real components they can represented exp mixture density networks directly the network output activations the adaptive parameters the mixture density network comprise the vector weights and biases the neural network that can set maximum likelihood equivalently minimizing error function defined the negative logarithm the likelihood for independent data this error function takes the form where have made the dependencies explicit order minimize the error function need calculate the derivatives the error with respect the components these can evaluated using the standard backpropagation procedure provided obtain suitable expressions for the derivatives the error with respect the output unit activations these represent error signals for each pattern and for each output unit and can backpropagated the hidden units and the error function derivatives evaluated the usual way because the error function composed sum terms one for each training data point can consider the derivatives for particular pattern and then find the derivatives summing over all patterns because are dealing with mixture distributions convenient view the mixing coefficients dependent prior probabilities and introduce the corresponding posterior probabilities given where nnk denotes ing coefficients are given the derivatives with respect the network output activations governing the mixsimilarly the derivatives with respect the output activations controlling the component means are given knnk lnnl finally the derivatives with respect the output activations controlling the component variances are given exercise exercise exercise neural networks figure plot the mixing coefficients function for the three kernel functions mixture density network trained the data shown figure the model has three gaussian components and uses two layer multilayer perceptron with five tanh sigmoidal units the hidden layer and nine outputs corresponding the means and variances the gaussian components and the mixing coefficients both small and large values where the conditional probability density the target data unimodal only one the kernels has high value for its prior probability while intermediate values where the conditional density trimodal the three mixing coefficients have comparable values plots the means using the same colour coding for the mixing coefficients plot the contours the corresponding conditional probability density the target data for the same mixture density network the approximate conditional mode shown the red points the conditional density plot illustrate the use mixture density network returning the toy example inverse problem shown figure plots the mixing coefficients the means and the conditional density contours corresponding are shown figure the outputs the neural network and hence the parameters the mixture model are necessarily continuous single valued functions the input variables however see from figure that the model able produce conditional density that unimodal for some values and trimodal for other values modulating the amplitudes the mixing components once mixture density network has been trained can predict the conditional density function the target data for any given value the input vector this conditional density represents complete description the generator the data far the problem predicting the value the output vector concerned from this density function can calculate more specific quantities that may interest different applications one the simplest these the mean corresponding the conditional average the target data and given"}, "114": {"Section": "5.7", "Title": "Bayesian Neural Networks", "Content": "where have used because standard network trained least squares approximating the conditional mean see that mixture density network can reproduce the conventional least squares result special case course have already noted for multimodal distribution the conditional mean limited value can similarly evaluate the variance the density function about the condiexercise tional average give where have used and this more general than the corresponding least squares result because the variance function have seen that for multimodal distributions the conditional mean can give poor representation the data for instance controlling the simple robot arm shown figure need pick one the two possible joint angle settings order achieve the desired end effector location whereas the average the two solutions not itself solution such cases the conditional mode may more value because the conditional mode for the mixture density network does not have simple analytical solution this would require numerical iteration simple alternative take the mean the most probable component the one with the largest mixing coefficient each value this shown for the toy data set figure bayesian neural networks far our discussion neural networks has focussed the use maximum likelihood determine the network parameters weights and biases regularized maximum likelihood can interpreted map maximum posterior approach which the regularizer can viewed the logarithm prior parameter distribution however bayesian treatment need marginalize over the distribution parameters order make predictions section developed bayesian solution for simple linear regression model under the assumption gaussian noise saw that the posterior distribution which gaussian could evaluated exactly and that the predictive distribution could also found closed form the case multilayered network the highly nonlinear dependence the network function the parameter values means that exact bayesian treatment can longer found fact the log the posterior distribution will nonconvex corresponding the multiple local minima the error function the technique variational inference discussed chapter has been applied bayesian neural networks using factorized gaussian approximation neural networks the posterior distribution hinton and van camp and also using fullcovariance gaussian barber and bishop barber and bishop the most complete treatment however has been based the laplace approximation mackay mackay and forms the basis for the discussion given here will approximate the posterior distribution gaussian centred mode the true posterior furthermore shall assume that the covariance this gaussian small that the network function approximately linear with respect the parameters over the region parameter space for which the posterior probability significantly nonzero with these two approximations will obtain models that are analogous the linear regression and classification models discussed earlier chapters and can exploit the results obtained there can then make use the evidence framework provide point estimates for the hyperparameters and compare alternative models for example networks having different numbers hidden units start with shall discuss the regression case and then later consider the modifications needed for solving classification tasks"}, "115": {"Section": "5.7.1", "Title": "Posterior parameter distribution", "Content": "consider the problem predicting single continuous target variable from vector inputs the extension multiple targets straightforward shall suppose that the conditional distribution gaussian with dependent mean given the output neural network model and with precision inverse variance similarly shall choose prior distribution over the weights that gaussian the form for data set observations with corresponding set target values the likelihood function given and the resulting posterior distribution then which consequence the nonlinear dependence will nongaussian can find gaussian approximation the posterior distribution using the laplace approximation this must first find local maximum the posterior and this must done using iterative numerical optimization usual convenient maximize the logarithm the posterior which can written the exercise can therefore make use the general result for the marginal give wmap wmap wmap bayesian neural networks form wtw const which corresponds regularized sum squares error function assuming for the moment that and are fixed can find maximum the posterior which denote wmap standard nonlinear optimization algorithms such conjugate gradients using error backpropagation evaluate the required derivatives having found mode wmap can then build local gaussian approximation evaluating the matrix second derivatives the negative log posterior distribution from this given where the hessian matrix comprising the second derivatives the sum ofsquares error function with respect the components algorithms for computing and approximating the hessian were discussed section the corresponding gaussian approximation the posterior then given from wmap similarly the predictive distribution obtained marginalizing with respect this posterior distribution however even with the gaussian approximation the posterior this integration still analytically intractable due the nonlinearity the network function function make progress now assume that the posterior distribution has small variance compared with the characteristic scales over which varying this allows make taylor series expansion the network function around wmap and retain only the linear terms wmap wmap where have defined with this approximation now have linear gaussian model with gaussian distribution for and gaussian for whose mean linear function the form wmap neural networks exercise this easily evaluated making use the laplace approximation result taking logarithms then gives where the input dependent variance given gta see that the predictive distribution gaussian whose mean given the network function wmap with the parameter set their map value the variance has two terms the first which arises from the intrinsic noise the target variable whereas the second dependent term that expresses the uncertainty the interpolant due the uncertainty the model parameters this should compared with the corresponding predictive distribution for the linear regression model given and"}, "116": {"Section": "5.7.2", "Title": "Hyperparameter optimization", "Content": "far have assumed that the hyperparameters and are fixed and known can make use the evidence framework discussed section together with the gaussian approximation the posterior obtained using the laplace approximation obtain practical procedure for choosing the values such hyperparameters the marginal likelihood evidence for the hyperparameters obtained integrating over the network weights wmap where the total number parameters and the regularized error function defined wmap wmap mapwmap see that this takes the same form the corresponding result for the linear regression model the evidence framework make point estimates for and maximizing consider first the maximization with respect which can done analogy with the linear regression case discussed section first define the eigenvalue equation where the hessian matrix comprising the second derivatives the sum ofsquares error function evaluated wmap analogy with obtain hui iui mapwmap bayesian neural networks where represents the effective number parameters and defined section section note that this result was exact for the linear regression case for the nonlinear neural network however ignores the fact that changes will cause changes the hessian which turn will change the eigenvalues have therefore implicitly ignored terms involving the derivatives with respect similarly from see that maximizing the evidence with respect gives the estimation formula wmap with the linear model need alternate between estimation the hyperparameters and and updating the posterior distribution the situation with neural network model more complex however due the multimodality the posterior distribution consequence the solution for wmap found maximizing the log posterior will depend the initialization solutions that differ only consequence the interchange and sign reversal symmetries the hidden units are identical far predictions are concerned and irrelevant which the equivalent solutions found however there may inequivalent solutions well and these will generally yield different values for the optimized hyperparameters order compare different models for example neural networks having different numbers hidden units need evaluate the model evidence this can approximated taking and substituting the values and obtained from the iterative optimization these hyperparameters more careful evaluation obtained marginalizing over and again making gaussian approximation mackay bishop either case necessary evaluate the determinant the hessian matrix this can problematic practice because the determinant unlike the trace sensitive the small eigenvalues that are often difficult determine accurately the laplace approximation based local quadratic expansion around mode the posterior distribution over weights have seen section that any given mode two layer network member set equivalent modes that differ interchange and sign change symmetries where the number hidden units when comparing networks having different numbers hidden units this can taken into account multiplying the evidence factor"}, "117": {"Section": "5.7.3", "Title": "Bayesian neural networks for classification", "Content": "far have used the laplace approximation develop bayesian treatment neural network regression models now discuss the modifications this framework that arise when applied classification here shall consider network having single logistic sigmoid output corresponding two class classification problem the extension networks with multiclass softmax outputs straightforward shall build extensively the analogous results for linear classification models discussed section and encourage the reader familiarize themselves with that material before studying this section the log likelihood function for this model given where are the target values and note that there hyperparameter because the data points are assumed correctly labelled before the prior taken isotropic gaussian the form the first stage applying the laplace framework this model initialize the hyperparameter and then determine the parameter vector maximizing the log posterior distribution this equivalent minimizing the regularized error function wtw and can achieved using error backpropagation combined with standard optimization algorithms discussed section having found solution wmap for the weight vector the next step evaluate the hessian matrix comprising the second derivatives the negative log likelihood function this can done for instance using the exact method section using the outer product approximation given the second derivatives the negative log posterior can again written the form and the gaussian approximation the posterior then given optimize the hyperparameter again maximize the marginal likelihood which easily shown take the form wmap const where the regularized error function defined wmap mapwmap which wmap maximizing this evidence function with respect again leads the estimation equation given the use the evidence procedure determine illustrated figure for the synthetic two dimensional data discussed appendix finally need the predictive distribution which defined again this integration intractable due the nonlinearity the network function the neural networks exercise exercise bayesian neural networks figure illustration the evidence framework applied synthetic two class data set the green curve shows the optimal decision boundary the black curve shows the result fitting two layer network with hidden units maximum likelihood and the red curve shows the result including regularizer which optimized using the evidence procedure starting from the initial value note that the evidence procedure greatly reduces the over fitting the network simplest approximation assume that the posterior distribution very narrow and hence make the approximation wmap can improve this however taking account the variance the posterior distribution this case linear approximation for the network outputs was used the case regression would inappropriate due the logistic sigmoid outputunit activation function that constrains the output lie the range instead make linear approximation for the output unit activation the form amap wmap where amap wmap and the vector wmap can found backpropagation because now have gaussian approximation for the posterior distribution over and model for that linear function can now appeal the results section the distribution output unit activation values induced the distribution over network weights given amap wmap where the gaussian approximation the posterior distribution given from section see that this distribution gaussian with mean amap wmap and variance finally obtain the predictive distribution must marginalize over using neural networks figure illustration the laplace approximation for bayesian neural network having hidden units with tanh activation functions and single logistic sigmoid output unit the weight parameters were found using scaled conjugate gradients and the hyperparameter was optimized using the evidence framework the left the result using the simple approximation based point estimate wmap the parameters which the green curve shows the decision boundary and the other contours correspond output probabilities and the right the corresponding result obtained using note that the effect marginalization spread out the contours and make the predictions less confident that each input point the posterior probabilities are shifted towards while the contour itself unaffected the convolution gaussian with logistic sigmoid intractable therefore apply the approximation giving btwmap where defined recall that both fication data set described appendix figure shows example this framework applied the synthetic classia and are functions"}, "118": {"Section": "6", "Title": "Kernel Methods", "Content": "chapters and considered linear parametric models for regression and classification which the form the mapping from input output governed vector adaptive parameters during the learning phase set training data used either obtain point estimate the parameter vector determine posterior distribution over this vector the training data then discarded and predictions for new inputs are based purely the learned parameter vector this approach also used nonlinear parametric models such neural networks however there class pattern recognition techniques which the training data points subset them are kept and used also during the prediction phase for instance the parzen probability density model comprised linear combination kernel functions each one centred one the training data points similarly section introduced simple technique for classification called nearest neighbours which involved assigning each new test vector the same label the chapter section kernel methods closest example from the training set these are examples memory based methods that involve storing the entire training set order make predictions for future data points they typically require metric defined that measures the similarity any two vectors input space and are generally fast train but slow making predictions for test data points many linear parametric models can cast into equivalent dual representation which the predictions are also based linear combinations kernel function evaluated the training data points shall see for models which are based fixed nonlinear feature space mapping the kernel function given the relation from this definition see that the kernel symmetric function its arguments that the kernel concept was introduced into the field pattern recognition aizerman the context the method potential functions called because analogy with electrostatics although neglected for many years was introduced into machine learning the context largemargin classifiers boser giving rise the technique support vector machines since then there has been considerable interest this topic both terms theory and applications one the most significant developments has been the extension kernels handle symbolic objects thereby greatly expanding the range problems that can addressed the simplest example kernel function obtained considering the identity mapping for the feature space that which case xtx shall refer this the linear kernel the concept kernel formulated inner product feature space allows build interesting extensions many well known algorithms making use the kernel trick also known kernel substitution the general idea that have algorithm formulated such way that the input vector enters only the form scalar products then can replace that scalar product with some other choice kernel for instance the technique kernel substitution can applied principal component analysis order develop nonlinear variant pca sch olkopf other examples kernel substitution include nearest neighbour classifiers and the kernel fisher discriminant mika roth and steinhage baudat and anouar there are numerous forms kernel functions common use and shall encounter several examples this chapter many have the property being function only the difference between the arguments that which are known stationary kernels because they are invariant translations input space further specialization involves homogeneous kernels also known radial basis functions which depend only the magnitude the distance typically euclidean between the arguments that brich and shawe taylor and cristianini for recent textbooks kernel methods see sch olkopf and smola herchapter section section"}, "119": {"Section": "6.1", "Title": "Dual Representations", "Content": "dual representations many linear models for regression and classification can reformulated terms dual representation which the kernel function arises naturally this concept will play important role when consider support vector machines the next chapter here consider linear regression model whose parameters are determined minimizing regularized sum squares error function given wtw where set the gradient with respect equal zero see that the solution for takes the form linear combination the vectors with coefficients that are functions the form where the design matrix whose nth row given here the vector and have defined instead working with the parameter vector can now reformulate the leastsquares algorithm terms the parameter vector giving rise dual representation substitute into obtain ttt where now define the gram matrix which symmetric matrix with elements knm where have introduced the kernel function defined terms the gram matrix the sum squares error function can written atkka atkt ttt atka setting the gradient with respect zero obtain the following solution substitute this back into the linear regression model obtain the following prediction for new input where have defined the vector with elements thus see that the dual formulation allows the solution the least squares problem expressed entirely terms the kernel function this known dual formulation because noting that the solution for can expressed linear combination the elements recover the original formulation terms the parameter vector note that the prediction given linear combination the target values from the training set fact have already obtained this result using slightly different notation section the dual formulation determine the parameter vector inverting matrix whereas the original parameter space formulation had invert matrix order determine because typically much larger than the dual formulation does not seem particularly useful however the advantage the dual formulation shall see that expressed entirely terms the kernel function can therefore work directly terms kernels and avoid the explicit introduction the feature vector which allows implicitly use feature spaces high even infinite dimensionality the existence dual representation based the gram matrix property many linear models including the perceptron section will develop duality between probabilistic linear models for regression and the technique gaussian processes duality will also play important role when discuss support vector machines chapter exercise exercise kernel methods"}, "120": {"Section": "6.2", "Title": "Constructing Kernels", "Content": "order exploit kernel substitution need able construct valid kernel functions one approach choose feature space mapping and then use this find the corresponding kernel illustrated figure here the kernel function defined for one dimensional input space where are the basis functions alternative approach construct kernel functions directly this case must ensure that the function choose valid kernel other words that corresponds scalar product some perhaps infinite dimensional feature space simple example consider kernel function given xtz constructing kernels figure illustration the construction kernel functions starting from corresponding set basis functions each column the lower plot shows the kernel function defined plotted function for while the upper plot shows the corresponding basis functions given polynomials left column gaussians centre column and logistic sigmoids right column take the particular case two dimensional input space can expand out the terms and thereby identify the corresponding nonlinear feature mapping xtz and see that the feature mapping takes the form therefore comprises all possible second order terms with specific weighting between them more generally however need simple way test whether function constitutes valid kernel without having construct the function explicitly necessary and sufficient condition for function valid kernel shawetaylor and cristianini that the gram matrix whose elements are given should positive semidefinite for all possible choices the set note that positive semidefinite matrix not the same thing matrix whose elements are nonnegative one powerful technique for constructing new kernels build them out simpler kernels building blocks this can done using the following properties appendix kernel methods techniques for constructing new kernels given valid kernels and the following new kernels will also valid exp xtax where constant any function polynomial with nonnegative coefficients function from valid kernel symmetric positive semidefinite matrix and are variables not necessarily disjoint with and and are valid kernel functions over their respective spaces equipped with these properties can now embark the construction more complex kernels appropriate specific applications require that the kernel symmetric and positive semidefinite and that expresses the appropriate form similarity between and according the intended application here consider few common examples kernel functions for more extensive discussion kernel engineering see shawe taylor and cristianini saw that the simple polynomial kernel xtx contains only consider the slightly generalized kernel with then the corresponding feature mapping contains conm terms degree two xtx stant and linear terms well terms order two similarly contains all monomials order for instance and are two images then the kernel represents particular weighted sum all possible products pixels the first image with pixels the second image this can similarly generm alized include all terms degree considering with using the results and for combining kernels see that these will all valid kernel functions xtx xtx another commonly used kernel takes the form exp and often called gaussian kernel note however that this context not interpreted probability density and hence the normalization coefficient exercise exercise constructing kernels omitted can see that this valid kernel expanding the square xtx xtx give exp xtx exp xtx exp and then making use and together with the validity the linear kernel xtx note that the feature vector that corresponds the gaussian kernel has infinite dimensionality the gaussian kernel not restricted the use euclidean distance use kernel substitution replace xtx with nonlinear kernel obtain exp important contribution arise from the kernel viewpoint has been the extension inputs that are symbolic rather than simply vectors real numbers kernel functions can defined over objects diverse graphs sets strings and text documents consider for instance fixed set and define nonvectorial space consisting all possible subsets this set and are two such subsets then one simple choice kernel would where denotes the intersection sets and and denotes the number subsets this valid kernel function because can shown correspond inner product feature space one powerful approach the construction kernels starts from probabilistic generative model haussler which allows apply generative models discriminative setting generative models can deal naturally with missing data and the case hidden markov models can handle sequences varying length contrast discriminative models generally give better performance discriminative tasks than generative models therefore some interest combine these two approaches lasserre one way combine them use generative model define kernel and then use this kernel discriminative approach given generative model can define kernel this clearly valid kernel function because can interpret inner product the one dimensional feature space defined the mapping says that two inputs and are similar they both have high probabilities can use and extend this class kernels considering sums over products different probability distributions with positive weighting coefficients the form kernel methods this equivalent overall multiplicative constant mixture distribution which the components factorize with the index playing the role latent variable two inputs and will give large value for the kernel function and hence appear similar they have significant probability under range different components taking the limit infinite sum can also consider kernels the form where continuous latent variable now suppose that our data consists ordered sequences length that observation given popular generative model for sequences the hidden markov model which expresses the distribution marginalization over corresponding sequence hidden states can use this approach define kernel function measuring the similarity two sequences and extending the mixture representation give that both observed sequences are generated the same hidden sequence this model can easily extended allow sequences differing length compared alternative technique for using generative models define kernel functions known the fisher kernel jaakkola and haussler consider parametric generative model where denotes the vector parameters the goal find kernel that measures the similarity two input vectors and induced the generative model jaakkola and haussler consider the gradient with respect which defines vector feature space having the same dimensionality particular they consider the fisher score section section exercise from which the fisher kernel defined here the fisher information matrix given where the expectation with respect under the distribution this can motivated from the perspective information geometry amari which considers the differential geometry the space model parameters here simply note that the presence the fisher information matrix causes this kernel invariant under nonlinear parameterization the density model practice often infeasible evaluate the fisher information matrix one approach simply replace the expectation the definition the fisher information with the sample average giving"}, "121": {"Section": "6.3", "Title": "Radial Basis Function Networks", "Content": "section section this the covariance matrix the fisher scores and the fisher kernel corresponds whitening these scores more simply can just omit the fisher information matrix altogether and use the noninvariant kernel application fisher kernels document retrieval given hofmann final example kernel function the sigmoidal kernel given tanh axtx whose gram matrix general not positive semidefinite this form kernel has however been used practice vapnik possibly because gives kernel expansions such the support vector machine superficial resemblance neural network models shall see the limit infinite number basis functions bayesian neural network with appropriate prior reduces gaussian process thereby providing deeper link between neural networks and kernel methods radial basis function networks chapter discussed regression models based linear combinations fixed basis functions although did not discuss detail what form those basis functions might take one choice that has been widely used that radial basis functions which have the property that each basis function depends only the radial distance typically euclidean from centre that historically radial basis functions were introduced for the purpose exact function interpolation powell given set input vectors along with corresponding target values the goal find smooth function that fits every target value exactly that for this achieved expressing linear combination radial basis functions one centred every data point wnh the values the coefficients are found least squares and because there are the same number coefficients there are constraints the result function that fits every target value exactly pattern recognition applications however the target values are generally noisy and exact interpolation undesirable because this corresponds over fitted solution expansions radial basis functions also arise from regularization theory poggio and girosi bishop for sum squares error function with regularizer defined terms differential operator the optimal solution given expansion the green functions the operator which are analogous the eigenvectors discrete matrix again with one basis function centred each data kernel methods point the differential operator isotropic then the green functions depend only the radial distance from the corresponding data point due the presence the regularizer the solution longer interpolates the training data exactly another motivation for radial basis functions comes from consideration the interpolation problem when the input rather than the target variables are noisy the noise the input variable described webb bishop variable having distribution then the sum squares error function becomes appendix exercise using the calculus variations can optimize with respect the function give where the basis functions are given tnh see that there one basis function centred every data point this known the nadaraya watson model and will derived again from different perspective section the noise distribution isotropic that function only then the basis functions will radial for any value the effect such normalization shown figure normalization sometimes used practice avoids having regions input space where all the basis functions take small values which would necessarily lead predictions such regions that are either small controlled purely the bias parameter note that the basis functions are normalized that another situation which expansions normalized radial basis functions arise the application kernel density estimation the problem regression shall discuss section because there one basis function associated with every data point the corresponding model can computationally costly evaluate when making predictions for new data points models have therefore been proposed broomhead and lowe moody and darken poggio and girosi which retain the expansion radial basis functions but where the number basis functions smaller than the number data points typically the number basis functions and the locations their centres are determined based the input data alone the basis functions are then kept fixed and the coefficients are determined least squares solving the usual set linear equations discussed section radial basis function networks figure plot set gaussian basis functions the left together with the corresponding normalized basis functions the right section section one the simplest ways choosing basis function centres use randomly chosen subset the data points more systematic approach called orthogonal least squares chen this sequential selection process which each step the next data point chosen basis function centre corresponds the one that gives the greatest reduction the sum squares error values for the expansion coefficients are determined part the algorithm clustering algorithms such means have also been used which give set basis function centres that longer coincide with training data points"}, "122": {"Section": "6.3.1", "Title": "Nadaraya-Watson model", "Content": "section saw that the prediction linear regression model for new input takes the form linear combination the training set target values with coefficients given the equivalent kernel where the equivalent kernel satisfies the summation constraint can motivate the kernel regression model from different perspective starting with kernel density estimation suppose have training set and use parzen density estimator model the joint distribution that where the component density function and there one such component centred each data point now find expression for the regression function corresponding the conditional average the target variable conditioned where and the kernel function given and have defined the result known the nadaraya watson model kernel regression nadaraya watson for localized kernel function has the property giving more weight the data points that are close note that the kernel satisfies the summation constraint kernel methods the input variable which given now assume for simplicity that the component density functions have zero mean that for all values using simple change variable then obtain figure illustration the nadaraya watson kernel regression model using isotropic gaussian kernels for the sinusoidal data set the original sine function shown the green curve the data points are shown blue and each the centre isotropic gaussian kernel the resulting regression function given the conditional mean shown the red line along with the twostandard deviation region for the conditional distribution shown the red shading the blue ellipse around each data point shows one standard deviation contour for the corresponding kernel these appear noncircular due the different scales the horizontal and vertical axes"}, "123": {"Section": "6.4", "Title": "Gaussian Processes", "Content": "fact this model defines not only conditional expectation but also full conditional distribution given exercise from which other expectations can evaluated illustration consider the case single input variable which given zero mean isotropic gaussian over the variable with variance the corresponding conditional distribution given gaussian mixture and shown together with the conditional mean for the sinusoidal synthetic data set figure obvious extension this model allow for more flexible forms gaussian components for instance having different variance parameters for the input and target variables more generally could model the joint distribution using gaussian mixture model trained using techniques discussed chapter ghahramani and jordan and then find the corresponding conditional distribution this latter case longer have representation terms kernel functions evaluated the training set data points however the number components the mixture model can smaller than the number training set points resulting model that faster evaluate for test data points have thereby accepted increased computational cost during the training phase order have model that faster making predictions gaussian processes section introduced kernels applying the concept duality nonprobabilistic model for regression here extend the role kernels probabilis kernel methods tic discriminative models leading the framework gaussian processes shall thereby see how kernels arise naturally bayesian setting chapter considered linear regression models the form which vector parameters and vector fixed nonlinear basis functions that depend the input vector showed that prior distribution over induced corresponding prior distribution over functions given training data set then evaluated the posterior distribution over and thereby obtained the corresponding posterior distribution over regression functions which turn with the addition noise implies predictive distribution for new input vectors the gaussian process viewpoint dispense with the parametric model and instead define prior probability distribution over functions directly first sight might seem difficult work with distribution over the uncountably infinite space functions however shall see for finite training set only need consider the values the function the discrete set input values corresponding the training set and test set data points and practice can work finite space models equivalent gaussian processes have been widely studied many different fields for instance the geostatistics literature gaussian process regression known kriging cressie similarly arma autoregressive moving average models kalman filters and radial basis function networks can all viewed forms gaussian process models reviews gaussian processes from machine learning perspective can found mackay williams and mackay and comparison gaussian process models with alternative approaches given rasmussen see also rasmussen and williams for recent textbook gaussian processes"}, "124": {"Section": "6.4.1", "Title": "Linear regression revisited", "Content": "order motivate the gaussian process viewpoint let return the linear regression example and derive the predictive distribution working terms distributions over functions this will provide specific example gaussian process consider model defined terms linear combination fixed basis functions given the elements the vector that where the input vector and the dimensional weight vector now consider prior distribution over given isotropic gaussian the form governed the hyperparameter which represents the precision inverse variance the distribution for any given value the definition defines particular function the probability distribution over defined therefore induces probability distribution over functions practice wish evaluate this function specific values for example the training data points gaussian processes are therefore interested the joint distribution the function values which denote the vector with elements for from this vector given where the design matrix with elements can find the probability distribution follows first all note that linear combination gaussian distributed variables given the elements and hence itself gaussian therefore need only find its mean and covariance which are given from cov where the gram matrix with elements yyt wwt knm and the kernel function this model provides with particular example gaussian process general gaussian process defined probability distribution over functions such that the set values evaluated arbitrary set points jointly have gaussian distribution cases where the input vector two dimensional this may also known gaussian random field more generally stochastic process specified giving the joint probability distribution for any finite set values consistent manner key point about gaussian stochastic processes that the joint distribution over variables specified completely the second order statistics namely the mean and the covariance most applications will not have any prior knowledge about the mean and symmetry take zero this equivalent choosing the mean the prior over weight values zero the basis function viewpoint the specification the gaussian process then completed giving the covariance evaluated any two values which given the kernel function for the specific case gaussian process defined the linear regression model with weight prior the kernel function given can also define the kernel function directly rather than indirectly through choice basis function figure shows samples functions drawn from gaussian processes for two different choices kernel function the first these gaussian kernel the form and the second the exponential kernel given exp which corresponds the ornstein uhlenbeck process originally introduced uhlenbeck and ornstein describe brownian motion exercise kernel methods figure samples from gaussian processes for gaussian kernel left and exponential kernel right"}, "125": {"Section": "6.4.2", "Title": "Gaussian processes for regression", "Content": "order apply gaussian process models the problem regression need take account the noise the observed target values which are given where and random noise variable whose value chosen independently for each observation here shall consider noise processes that have gaussian distribution that where hyperparameter representing the precision the noise because the noise independent for each data point the joint distribution the target values conditioned the values given isotropic gaussian the form where denotes the unit matrix from the definition gaussian process the marginal distribution given gaussian whose mean zero and whose covariance defined gram matrix that the kernel function that determines typically chosen express the property that for points and that are similar the corresponding values and will more strongly correlated than for dissimilar points here the notion similarity will depend the application order find the marginal distribution conditioned the input values need integrate over this can done making use the results from section for the linear gaussian model using see that the marginal distribution given gaussian processes where the covariance matrix has elements this result reflects the fact that the two gaussian sources randomness namely that associated with and that associated with are independent and their covariances simply add one widely used kernel function for gaussian process regression given the exponential quadratic form with the addition constant and linear terms give exp nxm note that the term involving corresponds parametric model that linear function the input variables samples from this prior are plotted for various values the parameters figure and figure shows set points sampled from the joint distribution along with the corresponding values defined far have used the gaussian process viewpoint build model the joint distribution over sets data points our goal regression however make predictions the target variables for new inputs given set training data let suppose that corresponding input values comprise the observed training set and our goal predict the target variable for new input vector this requires that evaluate the predictive distribution note that this distribution conditioned also the variables and however keep the notation simple will not show these conditioning variables explicitly find the conditional distribution begin writing down the joint distribution where denotes the vector then apply the results from section obtain the required conditional distribution illustrated figure from the joint distribution over will given where covariance matrix with elements given because this joint distribution gaussian can apply the results from section find the conditional gaussian distribution this partition the covariance matrix follows where the covariance matrix with elements given for the vector has elements for and the scalar kernel methods figure samples from gaussian process prior defined the covariance function the title above each plot denotes using the results and see that the conditional distribution gaussian distribution with mean and covariance given ktc ktc these are the key results that define gaussian process regression because the vector function the test point input value see that the predictive distribution gaussian whose mean and variance both depend example gaussian process regression shown figure the only restriction the kernel function that the covariance matrix given must positive definite eigenvalue then the corresponding eigenvalue will therefore sufficient that the kernel matrix positive semidefinite for any pair points and that because any eigenvalue that zero will still give rise positive eigenvalue for because this the same restriction the kernel function discussed earlier and can again exploit all the techniques section construct figure illustration the sampling data points from gaussian process the blue curve shows sample function from the gaussian process prior over functions and the red points show the values obtained evaluating the function set input values the corresponding values shown green are obtained adding independent gaussian noise each the gaussian processes suitable kernels tion the form note that the mean the predictive distribution can written funcexercise ank where the nth component thus the kernel function depends only the distance then obtain expansion radial basis functions the results and define the predictive distribution for gaussian process regression with arbitrary kernel function the particular case which the kernel function defined terms finite set basis functions can derive the results obtained previously section for linear regression starting from the gaussian process viewpoint for such models can therefore obtain the predictive distribution either taking parameter space viewpoint and using the linear regression result taking function space viewpoint and using the gaussian process result the central computational operation using gaussian processes will involve the inversion matrix size for which standard methods require computations contrast the basis function model have invert matrix size which has computational complexity note that for both viewpoints the matrix inversion must performed once for the given training set for each new test point both methods require vector matrix multiply which has cost the gaussian process case and for the linear basis function model the number basis functions smaller than the number data points will computationally more efficient work the basis function kernel methods figure illustration the mechanism gaussian process regression for the case one training point and one test point which the red ellipses show contours the joint distribution here the training data point and conditioning the value corresponding the vertical blue line obtain shown function the green curve framework however advantage gaussian processes viewpoint that can consider covariance functions that can only expressed terms infinite number basis functions for large training data sets however the direct application gaussian process methods can become infeasible and range approximation schemes have been developed that have better scaling with training set size than the exact approach gibbs tresp smola and bartlett williams and seeger csat and opper seeger practical issues the application gaussian processes are discussed bishop and nabney have introduced gaussian process regression for the case single target variable the extension this formalism multiple target variables known kriging cressie straightforward various other extensions gausexercise figure illustration gaussian process regression applied the sinusoidal data set figure which the three right most data points have been omitted the green curve shows the sinusoidal function from which the data points shown blue are obtained sampling and addition gaussian noise the red line shows the mean the gaussian process predictive distribution and the shaded region corresponds plus and minus two standard deviations notice how the uncertainty increases the region the right the data points gaussian processes sian process regression have also been considered for purposes such modelling the distribution over low dimensional manifolds for unsupervised learning bishop and the solution stochastic differential equations graepel"}, "126": {"Section": "6.4.3", "Title": "Learning the hyperparameters", "Content": "the predictions gaussian process model will depend part the choice covariance function practice rather than fixing the covariance function may prefer use parametric family functions and then infer the parameter values from the data these parameters govern such things the length scale the correlations and the precision the noise and correspond the hyperparameters standard parametric model techniques for learning the hyperparameters are based the evaluation the likelihood function where denotes the hyperparameters the gaussian process model the simplest approach make point estimate maximizing the log likelihood function because represents set hyperparameters for the regression problem this can viewed analogous the type maximum likelihood procedure for linear regression models maximization the log likelihood can done using efficient gradient based optimization algorithms such conjugate gradients fletcher nocedal and wright bishop and nabney the log likelihood function for gaussian process regression model easily evaluated using the standard form for multivariate gaussian distribution giving ttc for nonlinear optimization also need the gradient the log likelihood function with respect the parameter vector shall assume that evaluation the derivatives straightforward would the case for the covariance functions considered this chapter making use the result for the derivative together with the result for the derivative obtain ttc because will general nonconvex function can have multiple maxima straightforward introduce prior over and maximize the log posterior using gradient based methods fully bayesian treatment need evaluate marginals over weighted the product the prior and the likelihood function general however exact marginalization will intractable and must resort approximations the gaussian process regression model gives predictive distribution whose mean and variance are functions the input vector however have assumed that the contribution the predictive variance arising from the additive noise governed the parameter constant for some problems known heteroscedastic the noise variance itself will also depend model this can extend the section kernel methods for gaussian processes figure samples from the ard prior which the kernel function given the left plot corresponds and the right plot corresponds gaussian process framework introducing second gaussian process represent the dependence the input goldberg because variance and hence nonnegative use the gaussian process model"}, "127": {"Section": "6.4.4", "Title": "Automatic relevance determination", "Content": "the previous section saw how maximum likelihood could used determine value for the correlation length scale parameter gaussian process this technique can usefully extended incorporating separate parameter for each input variable rasmussen and williams the result shall see that the optimization these parameters maximum likelihood allows the relative importance different inputs inferred from the data this represents example the gaussian process context automatic relevance determination ard which was originally formulated the framework neural networks mackay neal the mechanism which appropriate inputs are preferred discussed section consider gaussian process with two dimensional input space having kernel function the form exp samples from the resulting prior over functions are shown for two different settings the precision parameters figure see that particular parameter becomes small the function becomes relatively insensitive the corresponding input variable adapting these parameters data set using maximum likelihood becomes possible detect input variables that have little effect the predictive distribution because the corresponding values will small this can useful practice because allows such inputs discarded ard illustrated using simple synthetic data set having three inputs and nabney figure the target variable generated sampling values from gaussian evaluating the function sin and then adding gaussian processes figure illustration automatic relevance determination gaussian process for synthetic problem having three inputs and for which the curves show the corresponding values the hyperparameters red green and blue function the number iterations when optimizing the marginal likelihood details are given the text note the logarithmic scale the vertical axis gaussian noise values are given copying the corresponding values and adding noise and values are sampled from independent gaussian distribution thus good predictor more noisy predictor and has only chance correlations with the marginal likelihood for gaussian process with ard parameters optimized using the scaled conjugate gradients algorithm see from figure that converges relatively large value converges much smaller value and becomes very small indicating that irrelevant for predicting the ard framework easily incorporated into the exponential quadratic kernel give the following form kernel function which has been found useful for applications gaussian processes range regression problems exp xni xmi xnixmi where the dimensionality the input space"}, "128": {"Section": "6.4.5", "Title": "Gaussian processes for classification", "Content": "probabilistic approach classification our goal model the posterior probabilities the target variable for new input vector given set training data these probabilities must lie the interval whereas gaussian process model makes predictions that lie the entire real axis however can easily adapt gaussian processes classification problems transforming the output the gaussian process using appropriate nonlinear activation function consider first the two class problem with target variable define gaussian process over function and then transform the function using logistic sigmoid given then will obtain non gaussian stochastic process over functions where this illustrated for the case one dimensional input space figure which the probability distri kernel methods figure the left plot shows sample from gaussian process prior over functions and the right plot shows the result transforming this sample using logistic sigmoid function bution over the target variable then given the bernoulli distribution usual denote the training set inputs with corresponding observed target variables also consider single test point with target value our goal determine the predictive distribution where have left the conditioning the input variables implicit this introduce gaussian process prior over the vector which has components this turn defines non gaussian process over and conditioning the training data obtain the required predictive distribution the gaussian process prior for takes the form unlike the regression case the covariance matrix longer includes noise term because assume that all the training data points are correctly labelled however for numerical reasons convenient introduce noise like term governed parameter that ensures that the covariance matrix positive definite thus the covariance matrix has elements given where any positive semidefinite kernel function the kind considered section and the value typically fixed advance shall assume that the kernel function governed vector parameters and shall later discuss how may learned from the training data for two class problems sufficient predict because the value then given the required predictive distribution given dan where this integral analytically intractable and may approximated using sampling methods neal alternatively can consider techniques based analytical approximation section derived the approximate formula for the convolution logistic sigmoid with gaussian distribution can use this result evaluate the integral provided have gaussian approximation the posterior distribution the usual justification for gaussian approximation posterior distribution that the true posterior will tend gaussian the number data points increases consequence the central limit theorem the case gaussian processes the number variables grows with the number data points and this argument does not apply directly however consider increasing the number data points falling fixed region space then the corresponding uncertainty the function will decrease again leading asymptotically gaussian williams and barber three different approaches obtaining gaussian approximation have been considered one technique based variational inference gibbs and mackay and makes use the local variational bound the logistic sigmoid this allows the product sigmoid functions approximated product gaussians thereby allowing the marginalization over performed analytically the approach also yields lower bound the likelihood function the variational framework for gaussian process classification can also extended multiclass problems using gaussian approximation the softmax function gibbs second approach uses expectation propagation opper and winther minka seeger because the true posterior distribution unimodal shall see shortly the expectation propagation approach can give good results"}, "129": {"Section": "6.4.6", "Title": "Laplace approximation", "Content": "the third approach gaussian process classification based the laplace approximation which now consider detail order evaluate the predictive distribution seek gaussian approximation the posterior distribution over which using bayes theorem given dan dan dan dan gaussian processes section section section section kernel methods where have used the conditional distribution obtained invoking the results and for gaussian process regression give ktc ktc can therefore evaluate the integral finding laplace approximation for the posterior distribution and then using the standard result for the convolution two gaussian distributions the prior given zero mean gaussian process with covariance matrix and the data term assuming independence the data points given eantn then obtain the laplace approximation taylor expanding the logarithm which additive normalization constant given the quantity ean const first need find the mode the posterior distribution and this requires that evaluate the gradient which given where vector with elements cannot simply find the mode setting this gradient zero because depends nonlinearly and resort iterative scheme based the newton raphson method which gives rise iterative reweighted least squares irls algorithm this requires the second derivatives which also require for the laplace approximation anyway and which are given where diagonal matrix with elements and have used the result for the derivative the logistic sigmoid function note that these diagonal elements lie the range and hence positive definite matrix because and hence its inverse positive definite construction and because the sum two positive definite matrices also positive definite see that the hessian matrix positive definite and the posterior distribution log convex and therefore has single mode that the global section exercise gaussian processes maximum the posterior distribution not gaussian however because the hessian function using the newton raphson formula the iterative update equation for given anew these equations are iterated until they converge the mode which denote the mode the gradient will vanish and hence will satisfy once have found the mode the posterior can evaluate the hessian matrix given where the elements are evaluated using proximation the posterior distribution given this defines our gaussian apq exercise exercise can now combine this with and hence evaluate the integral because this corresponds linear gaussian model can use the general result give var now that have gaussian distribution for can approximate the integral using the result with the bayesian logistic regression model section are only interested the decision boundary corresponding then need only consider the mean and can ignore the effect the variance also need determine the parameters the covariance function one approach maximize the likelihood function given for which need expressions for the log likelihood and its gradient desired suitable regularization terms can also added leading penalized maximum likelihood solution the likelihood function defined dan this integral analytically intractable again make use the laplace approximation using the result obtain the following approximation for the log the likelihood function kernel methods also need evaluate the gradient where with respect the parameter vector note that changes will cause changes leading additional terms the gradient thus when differentiate with respect obtain two sets terms the first arising from the dependence the covariance matrix and the rest arising from dependence the terms arising from the explicit dependence can found using together with the results and and are given compute the terms arising from the dependence note that the laplace approximation has been constructed such that has zero gradient gives contribution the gradient result its dependence this leaves the following contribution the derivative with respect component and where definition can evaluate the derivative entiating the relation with respect give and again have used the result together with the with respect differ rearranging then gives combining and can evaluate the gradient the log likelihood function which can used with standard nonlinear optimization algorithms order determine value for can illustrate the application the laplace approximation for gaussian processes using the synthetic two class data set shown figure extension the laplace approximation gaussian processes involving classes using the softmax activation function straightforward williams and barber appendix gaussian processes figure illustration the use gaussian process for classification showing the data the left together with the optimal decision boundary from the true distribution green and the decision boundary from the gaussian process classifier black the right the predicted posterior probability for the blue and red classes together with the gaussian process decision boundary"}, "130": {"Section": "6.4.7", "Title": "Connection to neural networks", "Content": "have seen that the range functions which can represented neural network governed the number hidden units and that for sufficiently large two layer network can approximate any given function with arbitrary accuracy the framework maximum likelihood the number hidden units needs limited level dependent the size the training set order avoid over fitting however from bayesian perspective makes little sense limit the number parameters the network according the size the training set bayesian neural network the prior distribution over the parameter vector conjunction with the network function produces prior distribution over functions from where the vector network outputs neal has shown that for broad class prior distributions over the distribution functions generated neural network will tend gaussian process the limit should noted however that this limit the output variables the neural network become independent one the great merits neural networks that the outputs share the hidden units and they can borrow statistical strength from each other that the weights associated with each hidden unit are influenced all the output variables not just one them this property therefore lost the gaussian process limit have seen that gaussian process determined its covariance kernel function williams has given explicit forms for the covariance the case two specific choices for the hidden unit activation function probit and gaussian these kernel functions are nonstationary they cannot expressed function the difference consequence the gaussian weight prior being centred zero which breaks translation invariance weight space kernel methods exercises working directly with the covariance function have implicitly marginalized over the distribution weights the weight prior governed hyperparameters then their values will determine the length scales the distribution over functions can understood studying the examples figure for the case finite number hidden units note that cannot marginalize out the hyperparameters analytically and must instead resort techniques the kind discussed section"}, "131": {"Section": "7", "Title": "Sparse Kernel Machines", "Content": "the previous chapter explored variety learning algorithms based nonlinear kernels one the significant limitations many such algorithms that the kernel function must evaluated for all possible pairs and training points which can computationally infeasible during training and can lead excessive computation times when making predictions for new data points this chapter shall look kernel based algorithms that have sparse solutions that predictions for new inputs depend only the kernel function evaluated subset the training data points begin looking some detail the support vector machine svm which became popular some years ago for solving problems classification regression and novelty detection important property support vector machines that the determination the model parameters corresponds convex optimization problem and any local solution also global optimum because the discussion support vector machines makes extensive use lagrange multipliers the reader sparse kernel machines encouraged review the key concepts covered appendix additional information support vector machines can found vapnik burges cristianini and shawe taylor uller sch olkopf and smola and herbrich the svm decision machine and does not provide posterior probabilities have already discussed some the benefits determining probabilities section alternative sparse kernel technique known the relevance vector machine rvm based bayesian formulation and provides posterior probabilistic outputs well having typically much sparser solutions than the svm section"}, "132": {"Section": "7.1", "Title": "Maximum Margin Classifiers", "Content": "begin our discussion support vector machines returning the two class classification problem using linear models the form where denotes fixed feature space transformation and have made the bias parameter explicit note that shall shortly introduce dual representation expressed terms kernel functions which avoids having work explicitly feature space the training data set comprises input vectors with corresponding target values where and new data points are classified according the sign shall assume for the moment that the training data set linearly separable feature space that definition there exists least one choice the parameters and such that function the form satisfies for points having and for points having that tny for all training data points there may course exist many such solutions that separate the classes exactly section described the perceptron algorithm that guaranteed find solution finite number steps the solution that finds however will dependent the arbitrary initial values chosen for and well the order which the data points are presented there are multiple solutions all which classify the training data set exactly then should try find the one that will give the smallest generalization error the support vector machine approaches this problem through the concept the margin which defined the smallest distance between the decision boundary and any the samples illustrated figure support vector machines the decision boundary chosen the one for which the margin maximized the maximum margin solution can motivated using computational learning theory also known statistical learning theory however simple insight into the origins maximum margin has been given tong and koller who consider framework for classification based hybrid generative and discriminative approaches they first model the distribution over input vectors for each class using parzen density estimator with gaussian kernels section maximum margin classifiers margin figure the margin defined the perpendicular distance between the decision boundary and the closest the data points shown the left figure maximizing the margin leads particular choice decision boundary shown the right the location this boundary determined subset the data points known support vectors which are indicated the circles having common parameter together with the class priors this defines optimal misclassification rate decision boundary however instead using this optimal boundary they determine the best hyperplane minimizing the probability error relative the learned density model the limit the optimal hyperplane shown the one having maximum margin the intuition behind this result that reduced the hyperplane increasingly dominated nearby data points relative more distant ones the limit the hyperplane becomes independent data points that are not support vectors shall see figure that marginalization with respect the prior distribution the parameters bayesian approach for simple linearly separable data set leads decision boundary that lies the middle the region separating the data points the large margin solution has similar behaviour recall from figure that the perpendicular distance point from hyperplane defined where takes the form given furthermore are only interested solutions for which all data points are correctly classified that tny for all thus the distance point the decision surface given tny the margin given the perpendicular distance the closest point from the data set and wish optimize the parameters and order maximize this distance thus the maximum margin solution found solving arg max min where have taken the factor outside the optimization over because appendix sparse kernel machines does not depend direct solution this optimization problem would very complex and shall convert into equivalent problem that much easier solve this note that make the rescaling and then the distance from any point the decision surface given tny unchanged can use this freedom set for the point that closest the surface this case all data points will satisfy the constraints this known the canonical representation the decision hyperplane the case data points for which the equality holds the constraints are said active whereas for the remainder they are said inactive definition there will always least one active constraint because there will always closest point and once the margin has been maximized there will least two active constraints the optimization problem then simply requires that maximize which equivalent minimizing and have solve the optimization problem arg min subject the constraints given the factor included for later convenience this example quadratic programming problem which are trying minimize quadratic function subject set linear inequality constraints appears that the bias parameter has disappeared from the optimization however determined implicitly via the constraints because these require that changes compensated changes shall see how this works shortly order solve this constrained optimization problem introduce lagrange multipliers with one multiplier for each the constraints giving the lagrangian function where note the minus sign front the lagrange multiplier term because are minimizing with respect and and maximizing with respect setting the derivatives with respect and equal zero obtain the following two conditions antn antn maximum margin classifiers eliminating and from using these conditions then gives the dual representation the maximum margin problem which maximize with respect subject the constraints anamtntmk antn here the kernel function defined again this takes the form quadratic programming problem which optimize quadratic function subject set inequality constraints shall discuss techniques for solving such quadratic programming problems section the solution quadratic programming problem variables general has computational complexity that going the dual formulation have turned the original optimization problem which involved minimizing over variables into the dual problem which has variables for fixed set basis functions whose number smaller than the number data points the move the dual problem appears disadvantageous however allows the model reformulated using kernels and the maximum margin classifier can applied efficiently feature spaces whose dimensionality exceeds the number data points including infinite feature spaces the kernel formulation also makes clear the role the constraint that the kernel function positive definite because this bounded below giving rise wellensures that the lagrangian function defined optimization problem order classify new data points using the trained model evaluate the sign defined this can expressed terms the parameters and the kernel function substituting for using give antnk joseph louis lagrange although widely considered french mathematician lagrange was born turin italy the age nineteen had already made important contributions mathematics and had been appointed professor the royal artillery school turin for many years euler worked hard persuade lagrange move berlin which eventually did where succeeded euler director mathematics the berlin academy later moved paris narrowly escaping with his life during the french revolution thanks the personal intervention lavoisier the french chemist who discovered oxygen who himself was later executed the guillotine lagrange made key contributions the calculus variations and the foundations dynamics sparse kernel machines appendix show that constrained optimization this form satisfies the karush kuhn tucker kkt conditions which this case require that the following three properties hold tny tny thus for every data point either tny any data point for which will not appear the sum and hence plays role making predictions for new data points the remaining data points are called support vectors and because they satisfy tny they correspond points that lie the maximum margin hyperplanes feature space illustrated figure this property central the practical applicability support vector machines once the model trained significant proportion the data points can discarded and only the support vectors retained having solved the quadratic programming problem and found value for can then determine the value the threshold parameter noting that any support vector satisfies tny using this gives amtmk where denotes the set indices the support vectors although can solve this equation for using arbitrarily chosen support vector numerically more stable solution obtained first multiplying through making use and then averaging these equations over all support vectors and solving for give amtmk where the total number support vectors for later comparison with alternative models can express the maximummargin classifier terms the minimization error function with simple quadratic regularizer the form where function that zero and otherwise and ensures that the constraints are satisfied note that long the regularization parameter satisfies its precise value plays role figure shows example the classification resulting from training support vector machine simple synthetic data set using gaussian kernel the maximum margin classifiers figure example synthetic data from two classes two dimensions showing contours constant obtained from support vector machine having gaussian kernel function also shown are the decision boundary the margin boundaries and the support vectors form although the data set not linearly separable the two dimensional data space linearly separable the nonlinear feature space defined implicitly the nonlinear kernel function thus the training data points are perfectly separated the original data space this example also provides geometrical insight into the origin sparsity the svm the maximum margin hyperplane defined the location the support vectors other data points can moved around freely long they remain outside the margin region without changing the decision boundary and the solution will independent such data points"}, "133": {"Section": "7.1.1", "Title": "Overlapping class distributions", "Content": "far have assumed that the training data points are linearly separable the feature space the resulting support vector machine will give exact separation the training data the original input space although the corresponding decision boundary will nonlinear practice however the class conditional distributions may overlap which case exact separation the training data can lead poor generalization therefore need way modify the support vector machine allow some the training points misclassified from see that the case separable classes implicitly used error function that gave infinite error data point was misclassified and zero error was classified correctly and then optimized the model parameters maximize the margin now modify this approach that data points are allowed the wrong side the margin boundary but with penalty that increases with the distance from that boundary for the subsequent optimization problem convenient make this penalty linear function this distance this introduce slack variables where with one slack variable for each training data point bennett cortes and vapnik these are defined for data points that are inside the correct margin boundary and for other points thus data point that the decision boundary will have and points sparse kernel machines figure illustration the slack variables data points with circles around them are support vectors with will misclassified the exact classification constraints are then replaced with tny which the slack variables are constrained satisfy data points for which are correctly classified and are either the margin the correct side the margin points for which lie inside the margin but the correct side the decision boundary and those data points for which lie the wrong side the decision boundary and are misclassified illustrated figure this sometimes described relaxing the hard margin constraint give soft margin and allows some the training set data points misclassified note that while slack variables allow for overlapping class distributions this framework still sensitive outliers because the penalty for misclassification increases linearly with our goal now maximize the margin while softly penalizing points that lie the wrong side the margin boundary therefore minimize where the parameter controls the trade off between the slack variable penalty and the margin because any point that misclassified has follows that upper bound the number misclassified points the parameter therefore analogous the inverse regularization coefficient because controls the trade off between minimizing training errors and controlling model complexity the limit will recover the earlier support vector machine for separable data now wish minimize subject the constraints together with the corresponding lagrangian given tny using these results eliminate and from the lagrangian obtain the dual lagrangian the form anamtntmk which identical the separable case except that the constraints are somewhat different see what these constraints are note that required because these are lagrange multipliers furthermore together with implies therefore have minimize with respect the dual variables subject antn for where are known box constraints this again represents quadratic programming problem substitute into see that predictions for new data points are again made using can now interpret the resulting solution before subset the data points may have which case they not contribute the predictive appendix where and are lagrange multipliers the corresponding set kkt conditions are given maximum margin classifiers where now optimize out and making use the definition give tny tny antn antn antn sparse kernel machines model the remaining data points constitute the support vectors these have and hence from must satisfy tny then implies that which from requires and hence such points lie the margin points with can lie inside the margin and can either correctly classified misclassified determine the parameter note that those support vectors for which have that tny and hence will satisfy amtmk again numerically stable solution obtained averaging give amtmk where denotes the set indices data points having alternative equivalent formulation the support vector machine known the svm has been proposed sch olkopf this involves maximizing anamtntmk subject the constraints this approach has the advantage that the parameter which replaces can interpreted both upper bound the fraction margin errors points for which and hence which lie the wrong side the margin boundary and which may may not misclassified and lower bound the fraction support vectors example the svm applied synthetic data set shown figure here gaussian kernels the form exp have been used with although predictions for new inputs are made using only the support vectors the training phase the determination the parameters and makes use the whole data set and important have efficient algorithms for solving maximum margin classifiers figure illustration the svm applied nonseparable data set two dimensions the support vectors are indicated circles the quadratic programming problem first note that the objective function given quadratic and any local optimum will also global optimum provided the constraints define convex region which they consequence being linear direct solution the quadratic programming problem using traditional techniques often infeasible due the demanding computation and memory requirements and more practical approaches need found the technique chunking vapnik exploits the fact that the value the lagrangian unchanged remove the rows and columns the kernel matrix corresponding lagrange multipliers that have value zero this allows the full quadratic programming problem broken down into series smaller ones whose goal eventually identify all the nonzero lagrange multipliers and discard the others chunking can implemented using protected conjugate gradients burges although chunking reduces the size the matrix the quadratic function from the number data points squared approximately the number nonzero lagrange multipliers squared even this may too big fit memory for large scale applications decomposition methods osuna also solve series smaller quadratic programming problems but are designed that each these fixed size and the technique can applied arbitrarily large data sets however still involves numerical solution quadratic programming subproblems and these can problematic and expensive one the most popular approaches training support vector machines called sequential minimal optimization smo platt takes the concept chunking the extreme limit and considers just two lagrange multipliers time this case the subproblem can solved analytically thereby avoiding numerical quadratic programming altogether heuristics are given for choosing the pair lagrange multipliers considered each step practice smo found have scaling with the number data points that somewhere between linear and quadratic depending the particular application have seen that kernel functions correspond inner products feature spaces that can have high even infinite dimensionality working directly terms the kernel function without introducing the feature space explicitly might therefore seem that support vector machines somehow manage avoid the curse sparse kernel machines section mensionality this not the case however because there are constraints amongst the feature values that restrict the effective dimensionality feature space see this consider simple second order polynomial kernel that can expand terms its components xtz this kernel function therefore represents inner product feature space having six dimensions which the mapping from input space feature space described the vector function however the coefficients weighting these different features are constrained have specific forms thus any set points the original two dimensional space would constrained lie exactly two dimensional nonlinear manifold embedded the six dimensional feature space have already highlighted the fact that the support vector machine does not provide probabilistic outputs but instead makes classification decisions for new input vectors veropoulos discuss modifications the svm allow the trade off between false positive and false negative errors controlled however wish use the svm module larger probabilistic system then probabilistic predictions the class label for new inputs are required address this issue platt has proposed fitting logistic sigmoid the outputs previously trained support vector machine specifically the required conditional probability assumed the form where defined values for the parameters and are found minimizing the cross entropy error function defined training set consisting pairs values and the data used fit the sigmoid needs independent that used train the original svm order avoid severe over fitting this twostage approach equivalent assuming that the output the support vector machine represents the log odds belonging class because the svm training procedure not specifically intended encourage this the svm can give poor approximation the posterior probabilities tipping"}, "134": {"Section": "7.1.2", "Title": "Relation to logistic regression", "Content": "with the separable case can cast the svm for nonseparable distributions terms the minimization regularized error function this will also allow highlight similarities and differences compared the logistic regression model have seen that for data points that are the correct side the margin boundary and which therefore satisfy yntn have and for the section figure plot the hinge error function used support vector machines shown blue along with the error function for logistic regression rescaled factor that passes through the point shown red also shown are the misclassification error black and the squared error green maximum margin classifiers remaining points have yntn thus the objective function can written overall multiplicative constant the form esv yntn where and esv the hinge error function defined esv yntn yntn where denotes the positive part the hinge error function called because its shape plotted figure can viewed approximation the misclassification error the error function that ideally would like minimize which also shown figure when considered the logistic regression model section found convenient work with target variable for comparison with the support vector machine first reformulate maximum likelihood logistic regression using the target variable this note that where given and the logistic sigmoid function defined follows that where have used the properties the logistic sigmoid function and can write exercise from this can construct error function taking the negative logarithm the likelihood function that with quadratic regularizer takes the form where elr yntn elr exp sparse kernel machines for comparison with other error functions can divide that the error function passes through the point this rescaled error function also plotted figure and see that has similar form the support vector error function the key difference that the flat region esv leads sparse solutions both the logistic error and the hinge loss can viewed continuous approximations the misclassification error another continuous error function that has sometimes been used solve classification problems the squared error which again plotted figure has the property however placing increasing emphasis data points that are correctly classified but that are long way from the decision boundary the correct side such points will strongly weighted the expense misclassified points and the objective minimize the misclassification rate then monotonically decreasing error function would better choice"}, "135": {"Section": "7.1.3", "Title": "Multiclass SVMs", "Content": "the support vector machine fundamentally two class classifier practice however often have tackle problems involving classes various methods have therefore been proposed for combining multiple two class svms order build multiclass classifier one commonly used approach vapnik construct separate svms which the kth model trained using the data from class the positive examples and the data from the remaining classes the negative examples this known the one versus the rest approach however figure saw that using the decisions the individual classifiers can lead inconsistent results which input assigned multiple classes simultaneously this problem sometimes addressed making predictions for new inputs using max unfortunately this heuristic approach suffers from the problem that the different classifiers were trained different tasks and there guarantee that the realvalued quantities for different classifiers will have appropriate scales another problem with the one versus the rest approach that the training sets are imbalanced for instance have ten classes each with equal numbers training data points then the individual classifiers are trained data sets comprising negative examples and only positive examples and the symmetry the original problem lost variant the one versus the rest scheme was proposed lee who modify the target values that the positive class has target and the negative class has target weston and watkins define single objective function for training all svms simultaneously based maximizing the margin from each remaining classes however this can result much slower training because instead solving separate optimization problems each over data points with overall cost single optimization problem size must solved giving overall cost maximum margin classifiers another approach train different class svms all possible pairs classes and then classify test points according which class has the highest number votes approach that sometimes called one versus one again saw figure that this can lead ambiguities the resulting classification also for large this approach requires significantly more training time than the one versus the rest approach similarly evaluate test points significantly more computation required the latter problem can alleviated organizing the pairwise classifiers into directed acyclic graph not confused with probabilistic graphical model leading the dagsvm platt for classes the dagsvm has total classifiers and classify new test point only pairwise classifiers need evaluated with the particular classifiers used depending which path through the graph traversed different approach multiclass classification based error correcting output codes was developed dietterich and bakiri and applied support vector machines allwein this can viewed generalization the voting scheme the one versus one approach which more general partitions the classes are used train the individual classifiers the classes themselves are represented particular sets responses from the two class classifiers chosen and together with suitable decoding scheme this gives robustness errors and ambiguity the outputs the individual classifiers although the application svms multiclass classification problems remains open issue practice the one versus the rest approach the most widely used spite its hoc formulation and its practical limitations there are also single class support vector machines which solve unsupervised learning problem related probability density estimation instead modelling the density data however these methods aim find smooth boundary enclosing region high density the boundary chosen represent quantile the density that the probability that data point drawn from the distribution will land inside that region given fixed number between and that specified advance this more restricted problem than estimating the full density but may sufficient specific applications two approaches this problem using support vector machines have been proposed the algorithm sch olkopf tries find hyperplane that separates all but fixed fraction the training data from the origin while the same time maximizing the distance margin the hyperplane from the origin while tax and duin look for the smallest sphere feature space that contains all but fraction the data points for kernels that are functions only the two algorithms are equivalent"}, "136": {"Section": "7.1.4", "Title": "SVMs for regression", "Content": "now extend support vector machines regression problems while the same time preserving the property sparseness simple linear regression section sparse kernel machines figure plot insensitive error function red which the error increases linearly with distance beyond the insensitive region also shown for comparison the quadratic error function green minimize regularized error function given obtain sparse solutions the quadratic error function replaced insensitive error function vapnik which gives zero error the absolute difference between the prediction and the target less than where simple example insensitive error function having linear cost associated with errors outside the insensitive region given and illustrated figure otherwise therefore minimize regularized error function given where given convention the inverse regularization parameter denoted appears front the error term before can express the optimization problem introducing slack variables for each data point now need two slack variables and where corresponds point for which and corresponds point for which illustrated figure the condition for target point lie inside the tube that where introducing the slack variables allows points lie outside the tube provided the slack variables are nonzero and the corresponding conditions are maximum margin classifiers figure illustration svm regression showing the regression curve together with the insensitive tube also shown are exam points ples the slack variables and above the tube have and points below the tube have and and points inside the tube have the error function for support vector regression can then written well which must minimized subject the constraints and and this can achieved introducing lagrange multipliers and and optimizing the lagrangian now substitute for using and then set the derivatives the lagrangian with respect and zero giving exercise using these results eliminate the corresponding variables from the lagrangian see that the dual problem involves maximizing sparse kernel machines where have introduced the kernel with respect and again this constrained maximization and find the constraints are both required because these are lagrange note that and multipliers also and together with and require and and again have the box constraints together with the condition substituting into see that predictions for new inputs can made using which again expressed terms the kernel function the corresponding karush kuhn tucker kkt conditions which state that the solution the product the dual variables and the constraints must vanish are given from these can obtain several useful results first all note that coefficient can only nonzero which implies that the data point either lies the upper boundary the tube lies above the upper boundary similarly nonzero value for and such points must lie either below the lower boundary the tube are incompatible easily seen adding them together and noting that and are nonnegative while strictly positive and for every data point either furthermore the two constraints and both must zero implies the support vectors are those data points that contribute predictions given these are points that other words those for which either lie the boundary the tube outside the tube all points within the tube have maximum margin classifiers again have sparse solution and the only terms that have evaluated the predictive model are those that involve the support vectors the parameter can found considering data point for which which from must have and from must therefore satisfy using and solving for obtain where have used can obtain analogous result considering point for which practice better average over all such estimates with the classification case there alternative formulation the svm for regression which the parameter governing complexity has more intuitive interpretation sch olkopf particular instead fixing the width the insensitive region fix instead parameter that bounds the fraction points lying outside the tube this involves maximizing subject the constraints appendix can shown that there are most data points falling outside the insensitive tube while least data points are support vectors and lie either the tube outside the use support vector machine solve regression problem illustrated using the sinusoidal data set figure here the parameters and have been chosen hand practice their values would typically determined crossvalidation sparse kernel machines figure illustration the svm for regression applied the sinusoidal synthetic data set using gaussian kernels the predicted regression curve shown the red line and the insensitive tube corresponds the shaded region also the data points are shown green and those with support vectors are indicated blue circles"}, "137": {"Section": "7.1.5", "Title": "Computational learning theory", "Content": "historically support vector machines have largely been motivated and analysed using theoretical framework known computational learning theory also sometimes called statistical learning theory anthony and biggs kearns and vazirani vapnik vapnik this has its origins with valiant who formulated the probably approximately correct pac learning framework the goal the pac framework understand how large data set needs order give good generalization also gives bounds for the computational cost learning although not consider these here suppose that data set size drawn from some joint distribution where the input variable and represents the class label and that restrict attention noise free situations which the class labels are determined some unknown deterministic function pac learning say that function drawn from space such functions the basis the training set has good generalization its expected error rate below some pre specified threshold that where the indicator function and the expectation with respect the distribution the quantity the left hand side random variable because depends the training set and the pac framework requires that holds with probability greater than for data set drawn randomly from here another pre specified parameter and the terminology probably approximately correct comes from the requirement that with high probability greater than the error rate small less than for given choice model space and for given parameters and pac learning aims provide bounds the minimum size data set needed meet this criterion key quantity pac learning the vapnik chervonenkis dimension dimension which provides measure the complexity space functions and which allows the pac framework extended spaces containing infinite number functions the bounds derived within the pac framework are often described worst"}, "138": {"Section": "7.2", "Title": "Relevance Vector Machines", "Content": "case because they apply any choice for the distribution long both the training and the test examples are drawn independently from the same distribution and for any choice for the function long belongs real world applications machine learning deal with distributions that have significant regularity for example which large regions input space carry the same class label consequence the lack any assumptions about the form the distribution the pac bounds are very conservative other words they strongly over estimate the size data sets required achieve given generalization performance for this reason pac bounds have found few any practical applications one attempt improve the tightness the pac bounds the pac bayesian framework mcallester which considers distribution over the space functions somewhat analogous the prior bayesian treatment this still considers any possible choice for and although the bounds are tighter they are still very conservative relevance vector machines support vector machines have been used variety classification and regression applications nevertheless they suffer from number limitations several which have been highlighted already this chapter particular the outputs svm represent decisions rather than posterior probabilities also the svm was originally formulated for two classes and the extension classes problematic there complexity parameter well parameter the case regression that must found using hold out method such cross validation finally predictions are expressed linear combinations kernel functions that are centred training data points and that are required positive definite the relevance vector machine rvm tipping bayesian sparse kernel technique for regression and classification that shares many the characteristics the svm whilst avoiding its principal limitations additionally typically leads much sparser models resulting correspondingly faster performance test data whilst maintaining comparable generalization error contrast the svm shall find more convenient introduce the regression form the rvm first and then consider the extension classification tasks"}, "139": {"Section": "7.2.1", "Title": "RVM for regression", "Content": "the relevance vector machine for regression linear model the form studied chapter but with modified prior that results sparse solutions the model defines conditional distribution for real valued target variable given input vector which takes the form with fixed nonlinear basis functions which will typically include constant term that the corresponding weight parameter represents bias the relevance vector machine specific instance this model which intended mirror the structure the support vector machine particular the basis functions are given kernels with one kernel associated with each the data points from the training set the general expression then takes the svm like form wnk where bias parameter the number parameters this case and has the same form the predictive model for the svm except that the coefficients are here denoted should emphasized that the subsequent analysis valid for arbitrary choices basis function and for generality shall work with the form contrast the svm there restriction positivedefinite kernels nor are the basis functions tied either number location the training data points suppose are given set observations the input vector which with the denote collectively data matrix whose nth row corresponding target values are given thus the likelihood function given next introduce prior distribution over the parameter vector and chapter shall consider zero mean gaussian prior however the key difference the rvm that introduce separate hyperparameter for each the weight parameters instead single shared hyperparameter thus the weight prior takes the form where represents the precision the corresponding parameter and denotes shall see that when maximize the evidence with respect these hyperparameters significant proportion them infinity and the corresponding weight parameters have posterior distributions that are concentrated zero the basis functions associated with these parameters therefore play role sparse kernel machines where the noise precision inverse noise variance and the mean given linear model the form relevance vector machines the predictions made the model and are effectively pruned out resulting sparse model using the result for linear regression models see that the posterior distribution for the weights again gaussian and takes the form where the mean and covariance are given where the design matrix with elements and diag note that the specific case the model have where the symmetric kernel matrix with elements the values and are determined using type maximum likelihood also known the evidence approximation which maximize the marginal likelihood function obtained integrating out the weight parameters because this represents the convolution two gaussians readily evaluated give the log marginal likelihood the form lnn ttc where and have defined the matrix given our goal now maximize with respect the hyperparameters and this requires only small modification the results obtained section for the evidence approximation the linear regression model again can identify two approaches the first simply set the required derivatives the marginal likelihood zero and obtain the following estimation equations new new section exercise exercise section where the ith component the posterior mean defined the quantity measures how well the corresponding parameter determined the data and defined sparse kernel machines which the ith diagonal component the posterior covariance given learning therefore proceeds choosing initial values for and evaluating the mean and covariance the posterior using and respectively and then alternately estimating the hyperparameters using and and estimating the posterior mean and covariance using and until suitable convergence criterion satisfied the second approach use the algorithm and discussed section these two approaches finding the values the hyperparameters that maximize the evidence are formally equivalent numerically however found that the direct optimization approach corresponding and gives somewhat faster convergence tipping result the optimization find that proportion the hyperparameters are driven large principle infinite values and the weight parameters corresponding these hyperparameters have posterior distributions with mean and variance both zero thus those parameters and the corresponding basis functions are removed from the model and play role making predictions for new inputs the case models the form the inputs corresponding the remaining nonzero weights are called relevance vectors because they are identified through the mechanism automatic relevance determination and are analogous the support vectors svm worth emphasizing however that this mechanism for achieving sparsity probabilistic models through automatic relevance determination quite general and can applied any model expressed adaptive linear combination basis functions having found values and for the hyperparameters that maximize the marginal likelihood can evaluate the predictive distribution over for new input using and this given thus the predictive mean given with set equal the posterior mean and the variance the predictive distribution given where given which and are set their optimized values and this just the familiar result obtained the context linear regression recall that for localized basis functions the predictive variance for linear regression models becomes small regions input space where there are basis functions the case rvm with the basis functions centred data points the model will therefore become increasingly certain its predictions when extrapolating outside the domain the data rasmussen and qui nonero candela which course undesirable the predictive distribution gaussian process regression does not exercise section exercise section relevance vector machines figure illustration rvm regression using the same data set and the same gaussian kernel functions the used figure for svm regression model the mean the predictive distribution for the rvm shown the red line and the one standarddeviation predictive distribution shown the shaded region also the data points are shown green and the relevance vectors are indicated blue circles note that there are only relevance vectors compared support vectors for the svm figure suffer from this problem however the computational cost making predictions with gaussian processes typically much higher than with rvm figure shows example the rvm applied the sinusoidal regression data set here the noise precision parameter also determined through evidence maximization see that the number relevance vectors the rvm significantly smaller than the number support vectors used the svm for wide range regression and classification tasks the rvm found give models that are typically order magnitude more compact than the corresponding support vector machine resulting significant improvement the speed processing test data remarkably this greater sparsity achieved with little reduction generalization error compared with the corresponding svm the principal disadvantage the rvm compared the svm that training involves optimizing nonconvex function and training times can longer than for comparable svm for model with basis functions the rvm requires inversion matrix size which general requires computation the specific case the svm like model have have noted there are techniques for training svms whose cost roughly quadratic course the case the rvm always have the option starting with smaller number basis functions than more significantly the relevance vector machine the parameters governing complexity and noise variance are determined automatically from single training run whereas the support vector machine the parameters and are generally found using cross validation which involves multiple training runs furthermore the next section shall derive alternative procedure for training the relevance vector machine that improves training speed significantly"}, "140": {"Section": "7.2.2", "Title": "Analysis of sparsity", "Content": "have noted earlier that the mechanism automatic relevance determination causes subset parameters driven zero now examine more detail sparse kernel machines figure illustration the mechanism for sparsity bayesian linear regression model showing training set vector target values given indicated the cross for model with one basis vector which poorly aligned with the target data vector the left see model having only isotropic noise that corresponding with set its most probable value the right see the same model but with finite value each case the red ellipse corresponds unit mahalanobis distance with taking the same value for both plots while the dashed green circle shows the contrition arising from the noise term see that any finite value reduces the probability the observed data and for the most probable solution the basis vector removed the mechanism sparsity the context the relevance vector machine the process will arrive significantly faster procedure for optimizing the hyperparameters compared the direct techniques given above before proceeding with mathematical analysis first give some informal insight into the origin sparsity bayesian linear models consider data set comprising observations and together with model having single basis function with hyperparameter along with isotropic noise having precision from the marginal likelihood given which the covariance matrix takes the form where denotes the dimensional vector and similarly notice that this just zero mean gaussian process model over with covariance given particular observation for our goal find and maximizing the marginal likelihood see from figure that there poor alignment between the direction and that the training data vector then the corresponding hyperparameter will driven and the basis vector will pruned from the model this arises because any finite value for will always assign lower probability the data thereby decreasing the value the density provided that set its optimal value see that any finite value for would cause the distribution elongated direction away from the data thereby increasing the probability mass regions away from the observed data and hence reducing the value the density the target data vector itself for the more general case relevance vector machines basis vectors similar intuition holds namely that particular basis vector poorly aligned with the data vector then likely pruned from the model now investigate the mechanism for sparsity from more mathematical perspective for general case involving basis functions motivate this analysis first note that the result for estimating the parameter the terms the right hand side are themselves also functions these results therefore represent implicit solutions and iteration would required even determine single with all other for fixed this suggests different approach solving the optimization problem for the rvm which make explicit all the dependence the marginal likelihood particular and then determine its stationary points explicitly faul and tipping tipping and faul this first pull out the contribution from the matrix defined give where denotes the ith column other words the dimensional vector with elements contrast which denotes the nth row the matrix represents the matrix with the contribution from basis function removed using the matrix identities and the determinant and inverse can then written exercise using these results can then write the log marginal likelihood function the form where simply the log marginal likelihood with basis function omitted and the quantity defined and contains all the dependence here have introduced the two quantities here called the sparsity and known the quality and shall see large value relative the value means that the basis function sparse kernel machines the figure plots log likelihood versus marginal showing the left the single maximum finite for and that and the right the maximum and that for more likely pruned from the model the sparsity measures the extent which basis function overlaps with the other basis vectors the model and the quality represents measure the alignment the basis vector with the error between the training set values and the vector predictions that would result from the model with the vector excluded tipping and faul the stationary points the marginal likelihood with respect occur when the derivative equal zero there are two possible forms for the solution recalling that see that can solve for obtain then provides solution conversely exercise these two solutions are illustrated figure see that the relative size the quality and sparsity terms determines whether particular basis vector will pruned from the model not more complete analysis faul and tipping based the second derivatives the marginal likelihood confirms these solutions are indeed the unique maxima note that this approach has yielded closed form solution for for given values the other hyperparameters well providing insight into the origin sparsity the rvm this analysis also leads practical algorithm for optimizing the hyperparameters that has significant speed advantages this uses fixed set candidate basis vectors and then cycles through them turn decide whether each vector should included the model not the resulting sequential sparse bayesian learning algorithm described below sequential sparse bayesian learning algorithm solving regression problem initialize initialize using one basis function with hyperparameter set using with the remaining hyperparameters for initialized infinity that only included the model relevance vector machines evaluate and along with and for all basis functions select candidate basis function the model then update using and that the basis vector already included and then add the model and evaluate hyperpai and then remove basis function from the model rameter using and set solving regression problem update converged terminate otherwise note that from the model and action required and then the basis function already excluded practice convenient evaluate the quantities the quality and sparseness variables can then expressed the form iqi isi exercise note that when have and using can write where and involve only those basis vectors that correspond finite hyperparameters each stage the required computations therefore scale like where the number active basis vectors the model and typically much smaller than the number training patterns"}, "141": {"Section": "7.2.3", "Title": "RVM for classification", "Content": "can extend the relevance vector machine framework classification problems applying the ard prior over weights probabilistic linear classification model the kind studied chapter start with consider two class problems with binary target variable the model now takes the form linear combination basis functions transformed logistic sigmoid function sparse kernel machines section where the logistic sigmoid function defined introduce gaussian prior over the weight vector then obtain the model that has been considered already chapter the difference here that the rvm this model uses the ard prior which there separate precision hyperparameter associated with each weight parameter contrast the regression model can longer integrate analytically over the parameter vector here follow tipping and use the laplace approximation which was applied the closely related problem bayesian logistic regression section begin initializing the hyperparameter vector for this given value then build gaussian approximation the posterior distribution and thereby obtain approximation the marginal likelihood maximization this approximate marginal likelihood then leads estimated value for and the process repeated until convergence let consider the laplace approximation for this model more detail for fixed value the mode the posterior distribution over obtained maximizing wtaw const where diagonal matrix with elements the vector and the design matrix with elements here have used the property for the derivative the logistic sigmoid function convergence the irls algorithm the negative hessian represents the inverse covariance matrix for the gaussian approximation the posterior distribution the mode the resulting approximation the posterior distribution corresponding the mean the gaussian approximation obtained setting zero giving the mean and covariance the laplace approximation the form can now use this laplace approximation evaluate the marginal likelihood using the general result for integral evaluated using the laplace approxiexercise where diag this can done using iterative reweighted least squares irls discussed section for this need the gradient vector and hessian matrix the log posterior distribution which from are given relevance vector machines mation have substitute for and and then set the derivative the marginal likelihood with respect equal zero obtain exercise defining and rearranging then gives new which identical the estimation formula obtained for the regression rvm define can write the approximate log marginal likelihood the form where appendix section this takes the same form the regression case and can apply the same analysis sparsity and obtain the same fast learning algorithm which fully optimize single hyperparameter each step figure shows the relevance vector machine applied synthetic classification data set see that the relevance vectors tend not lie the region the decision boundary contrast the support vector machine this consistent with our earlier discussion sparsity the rvm because basis function centred data point near the boundary will have vector that poorly aligned with the training data vector one the potential advantages the relevance vector machine compared with the svm that makes probabilistic predictions for example this allows the rvm used help construct emission density nonlinear extension the linear dynamical system for tracking faces video sequences williams far have considered the rvm for binary classification problems for classes again make use the probabilistic approach section which there are linear models the form exp the log likelihood function then given ytnk where the target values tnk have coding for each data point and matrix with elements tnk again the laplace approximation can used optimize the hyperparameters tipping which the model and its hessian are found using irls this gives more principled approach multiclass classification than the pairwise method used the support vector machine and also provides probabilistic predictions for new data points the principal disadvantage that the hessian matrix has size where the number active basis functions which gives additional factor the computational cost training compared with the two class rvm the principal disadvantage the relevance vector machine the relatively long training times compared with the svm this offset however the avoidance cross validation runs set the model complexity parameters furthermore because yields sparser models the computation time test points which usually the more important consideration practice typically much less"}, "142": {"Section": "8", "Title": "Graphical Models", "Content": "probabilities play central role modern pattern recognition have seen chapter that probability theory can expressed terms two simple equations corresponding the sum rule and the product rule all the probabilistic inference and learning manipulations discussed this book matter how complex amount repeated application these two equations could therefore proceed formulate and solve complicated probabilistic models purely algebraic manipulation however shall find highly advantageous augment the analysis using diagrammatic representations probability distributions called probabilistic graphical models these offer several useful properties they provide simple way visualize the structure probabilistic model and can used design and motivate new models insights into the properties the model including conditional independence properties can obtained inspection the graph graphical models complex computations required perform inference and learning sophisticated models can expressed terms graphical manipulations which underlying mathematical expressions are carried along implicitly graph comprises nodes also called vertices connected links also known edges arcs probabilistic graphical model each node represents random variable group random variables and the links express probabilistic relationships between these variables the graph then captures the way which the joint distribution over all the random variables can decomposed into product factors each depending only subset the variables shall begin discussing bayesian networks also known directed graphical models which the links the graphs have particular directionality indicated arrows the other major class graphical models are markov random fields also known undirected graphical models which the links not carry arrows and have directional significance directed graphs are useful for expressing causal relationships between random variables whereas undirected graphs are better suited expressing soft constraints between random variables for the purposes solving inference problems often convenient convert both directed and undirected graphs into different representation called factor graph this chapter shall focus the key aspects graphical models needed for applications pattern recognition and machine learning more general treatments graphical models can found the books whittaker lauritzen jensen castillo jordan cowell and jordan"}, "143": {"Section": "8.1", "Title": "Bayesian Networks", "Content": "order motivate the use directed graphs describe probability distributions consider first arbitrary joint distribution over three variables and note that this stage not need specify anything further about these variables such whether they are discrete continuous indeed one the powerful aspects graphical models that specific graph can make probabilistic statements for broad class distributions application the product rule probability can write the joint distribution the form second application the product rule this time the second term the righthand side gives note that this decomposition holds for any choice the joint distribution now represent the right hand side terms simple graphical model follows first introduce node for each the random variables and and associate each node with the corresponding conditional distribution the right hand side figure directed graphical model representing the joint probability distribution over three variables and corresponding the decomposition the right hand side bayesian networks then for each conditional distribution add directed links arrows the graph from the nodes corresponding the variables which the distribution conditioned thus for the factor there will links from nodes and node whereas for the factor there will incoming links the result the graph shown figure there link going from node node then say that node the parent node and say that node the child node note that shall not make any formal distinction between node and the variable which corresponds but will simply use the same symbol refer both interesting point note about that the left hand side symmetrical with respect the three variables and whereas the right hand side not indeed making the decomposition have implicitly chosen particular ordering namely and had chosen different ordering would have obtained different decomposition and hence different graphical representation shall return this point later for the moment let extend the example figure considering the joint distribution over variables given repeated application the product rule probability this joint distribution can written product conditional distributions one for each the variables for given choice can again represent this directed graph having nodes one for each conditional distribution the right hand side with each node having incoming links from all lower numbered nodes say that this graph fully connected because there link between every pair nodes far have worked with completely general joint distributions that the decompositions and their representations fully connected graphs will applicable any choice distribution shall see shortly the absence links the graph that conveys interesting information about the properties the class distributions that the graph represents consider the graph shown figure this not fully connected graph because for instance there link from from shall now from this graph the corresponding representation the joint probability distribution written terms the product set conditional distributions one for each node the graph each such conditional distribution will conditioned only the parents the corresponding node the graph for instance will conditioned and the joint distribution all variables therefore given the reader should take moment study carefully the correspondence between and figure can now state general terms the relationship between given directed graph and the corresponding distribution over the variables the joint distribution defined graph given the product over all the nodes the graph conditional distribution for each node conditioned the variables corresponding the parents that node the graph thus for graph with nodes the joint distribution given graphical models figure example directed acyclic graph describing the joint distribution over variables the corresponding decomposition the joint distribution given exercise exercise pak where pak denotes the set parents and this key equation expresses the factorization properties the joint distribution for directed graphical model although have considered each node correspond single variable can equally well associate sets variables and vector valued variables with the nodes graph easy show that the representation the righthand side always correctly normalized provided the individual conditional distributions are normalized the directed graphs that are considering are subject important restriction namely that there must directed cycles other words there are closed paths within the graph such that can move from node node along links following the direction the arrows and end back the starting node such graphs are also called directed acyclic graphs dags this equivalent the statement that there exists ordering the nodes such that there are links that from any node any lower numbered node"}, "144": {"Section": "8.1.1", "Title": "Example: Polynomial regression", "Content": "illustration the use directed graphs describe probability distributions consider the bayesian polynomial regression model introduced secn figure directed graphical model representing the joint distribution corresponding the bayesian polynomial regression model introduced section bayesian networks tion the random variables this model are the vector polynomial coefficients and the observed data addition this model contains the input data the noise variance and the hyperparameter representing the precision the gaussian prior over all which are parameters the model rather than random variables focussing just the random variables for the moment see that the joint distribution given the product the prior and conditional distributions for that this joint distribution can represented graphical model shown figure when start deal with more complex models later the book shall find inconvenient have write out multiple nodes the form explicitly figure therefore introduce graphical notation that allows such multiple nodes expressed more compactly which draw single representative node and then surround this with box called plate labelled with indicating that there are nodes this kind writing the graph figure this way obtain the graph shown figure shall sometimes find helpful make the parameters model well its stochastic variables explicit this case becomes correspondingly can make and explicit the graphical representation this shall adopt the convention that random variables will denoted open circles and deterministic parameters will denoted smaller solid circles take the graph figure and include the deterministic parameters obtain the graph shown figure when apply graphical model problem machine learning pattern recognition will typically set some the random variables specific observed figure alternative more compact representation the graph shown figure which have introduced plate the box labelled that represents nodes which only single example shown explicitly graphical models figure this shows the same model figure but with the deterministic parameters shown explicitly the smaller solid nodes values for example the variables from the training set the case polynomial curve fitting graphical model will denote such observed variables shading the corresponding nodes thus the graph corresponding figure which the variables are observed shown figure note that the value not observed and example latent variable also known hidden variable such variables play crucial role many probabilistic models and will form the focus chapters and having observed the values can desired evaluate the posterior distribution the polynomial coefficients discussed section for the moment note that this involves straightforward application bayes theorem where again have omitted the deterministic parameters order keep the notation uncluttered general model parameters such are little direct interest themselves because our ultimate goal make predictions for new input values suppose and wish find the corresponding probability disare given new input value conditioned the observed data the graphical model that describes tribution for this problem shown figure and the corresponding joint distribution all the random variables this model conditioned the deterministic parameters then given figure figure but with the nodes shaded indicate that the corresponding random variables have been set their observed training set values figure the polynomial regression model corresponding figure showing also new input value together with the corresponding model prediction bayesian networks the required predictive distribution for probability integrating out the model parameters that then obtained from the sum rule where are implicitly setting the random variables the specific values observed the data set the details this calculation were discussed chapter"}, "145": {"Section": "8.1.2", "Title": "Generative models", "Content": "there are many situations which wish draw samples from given probability distribution although shall devote the whole chapter detailed discussion sampling methods instructive outline here one technique called ancestral sampling which particularly relevant graphical models consider joint distribution over variables that factorizes according corresponding directed acyclic graph shall suppose that the variables have been ordered such that there are links from any node any lower numbered node other words each node has higher number than any its parents our goal draw sample from the joint distribution this start with the lowest numbered node and draw sample from the distribution which call then work through each the nodes order that for node draw sample from the conditional distribution pan which the parent variables have been set their sampled values note that each stage these parent values will always available because they correspond lowernumbered nodes that have already been sampled techniques for sampling from specific distributions will discussed detail chapter once have sampled from the final variable will have achieved our objective obtaining sample from the joint distribution obtain sample from some marginal distribution corresponding subset the variables simply take the sampled values for the required nodes and ignore the sampled values for the remaining nodes for example draw sample from the distribution simply sample from and discard the remaining the full joint distribution and then retain the values values graphical models figure graphical model representing the process which images objects are created which the identity object discrete variable and the position and orientation that object continuous variables have independent prior probabilities the image vector pixel intensities has probability distribution that dependent the identity the object well its position and orientation object position orientation image for practical applications probabilistic models will typically the highernumbered variables corresponding terminal nodes the graph that represent the observations with lower numbered nodes corresponding latent variables the primary role the latent variables allow complicated distribution over the observed variables represented terms model constructed from simpler typically exponential family conditional distributions can interpret such models expressing the processes which the observed data arose for instance consider object recognition task which each observed data point corresponds image comprising vector pixel intensities one the objects this case the latent variables might have interpretation the position and orientation the object given particular observed image our goal find the posterior distribution over objects which integrate over all possible positions and orientations can represent this problem using graphical model the form show figure the graphical model captures the causal process pearl which the observed data was generated for this reason such models are often called generative models contrast the polynomial regression model described figure not generative because there probability distribution associated with the input variable and not possible generate synthetic data points from this model could make generative introducing suitable prior distribution the expense more complex model the hidden variables probabilistic model need not however have any explicit physical interpretation but may introduced simply allow more complex joint distribution constructed from simpler components either case the technique ancestral sampling applied generative model mimics the creation the observed data and would therefore give rise fantasy data whose probability distribution the model were perfect representation reality would the same that the observed data practice producing synthetic observations from generative model can prove informative understanding the form the probability distribution represented that model"}, "146": {"Section": "8.1.3", "Title": "Discrete variables", "Content": "have discussed the importance probability distributions that are members the exponential family and have seen that this family includes many wellknown distributions particular cases although such distributions are relatively simple they form useful building blocks for constructing more complex probability section figure this fully connected graph describes general distribution over two state discrete variables having total parameters dropping the link between the nodes the number parameters reduced bayesian networks distributions and the framework graphical models very useful expressing the way which these building blocks are linked together such models have particularly nice properties choose the relationship between each parent child pair directed graph conjugate and shall explore several examples this shortly two cases are particularly worthy note namely when the parent and child node each correspond discrete variables and when they each correspond gaussian variables because these two cases the relationship can extended hierarchically construct arbitrarily complex directed acyclic graphs begin examining the discrete case the probability distribution for single discrete variable having possible states using the representation given distribution and governed the parameters due the constraint only values for need specified order define the now suppose that have two discrete variables and each which has states and wish model their joint distribution denote the probability observing both and the parameter where denotes the kth component and similarly for the joint distribution can written this distribecause the parameters are subject the constraint bution governed parameters easily seen that the total number parameters that must specified for arbitrary joint distribution over variables and therefore grows exponentially with the number variables using the product rule can factor the joint distribution the form which corresponds two node graph with link going from the node the node shown figure the marginal distribution governed parameters before similarly the conditional distribution requires the specification parameters for each the possible values the total number parameters that must specified the joint distribution therefore before now suppose that the variables and were independent corresponding the graphical model shown figure each variable then described graphical models figure this chain discrete nodes each having states requires the specification parameters which grows linearly with the length the chain contrast fully connected graph nodes would have parameters which grows exponentially with separate multinomial distribution and the total number parameters would for distribution over independent discrete variables each having states the total number parameters would which therefore grows linearly with the number variables from graphical perspective have reduced the number parameters dropping links the graph the expense having restricted class distributions more generally have discrete variables can model the joint distribution using directed graph with one variable corresponding each node the conditional distribution each node given set nonnegative parameters subject the usual normalization constraint the graph fully connected then have completely general distribution having parameters whereas there are links the graph the joint distribution factorizes into the product the marginals and the total number parameters graphs having intermediate levels connectivity allow for more general distributions than the fully factorized one while requiring fewer parameters than the general joint distribution illustration consider the chain nodes shown figure the marginal distribution requires parameters whereas each the conditional distributions for requires parameters this gives total parameter count which quadratic and which grows linearly rather than exponentially with the length the chain alternative way reduce the number independent parameters model sharing parameters also known tying parameters for instance the chain example figure can arrange that all the conditional distributions for are governed the same set parameters together with the parameters governing the distribution this gives total parameters that must specified order define the joint distribution can turn graph over discrete variables into bayesian model introducing dirichlet priors for the parameters from graphical point view each node then acquires additional parent representing the dirichlet distribution over the parameters associated with the corresponding discrete node this illustrated for the chain model figure the corresponding model which tie the parameters governing the conditional distributions for shown figure another way controlling the exponential growth the number parameters models discrete variables use parameterized models for the conditional distributions instead complete tables conditional probability values illustrate this idea consider the graph figure which all the nodes represent binary variables each the parent variables governed single paramefigure extension the model figure include dirichlet priors over the parameters governing the discrete distributions figure figure but with single set parameters shared amongst all the conditional distributions bayesian networks ter representing the probability giving parameters total for the parent nodes the conditional distribution however would require parameters representing the probability for each the possible settings the parent variables thus general the number parameters required specify this conditional distribution will grow exponentially with can obtain more parsimonious form for the conditional distribution using logistic sigmoid function acting linear combination the parent variables giving section wixi wtx where exp the logistic sigmoid dimensional vector parent states augmented with additional variable whose value clamped and vector parameters this more restricted form conditional distribution than the general case but now governed number parameters that grows linearly with this sense analogous the choice restrictive form covariance matrix for example diagonal matrix multivariate gaussian distribution the motivation for the logistic sigmoid representation was discussed section figure graph comprising parents and single child used illustrate the idea parameterized conditional distributions for discrete variables pai pai graphical models"}, "147": {"Section": "8.1.4", "Title": "Linear-Gaussian models", "Content": "the previous section saw how construct joint probability distributions over set discrete variables expressing the variables nodes directed acyclic graph here show how multivariate gaussian can expressed directed graph corresponding linear gaussian model over the component variables this allows impose interesting structure the distribution with the general gaussian and the diagonal covariance gaussian representing opposite extremes several widely used techniques are examples linear gaussian models such probabilistic principal component analysis factor analysis and linear dynamical systems roweis and ghahramani shall make extensive use the results this section later chapters when consider some these techniques detail consider arbitrary directed acyclic graph over variables which node represents single continuous random variable having gaussian distribution the mean this distribution taken linear combination the states its parent nodes pai node pai wijxj where wij and are parameters governing the mean and the variance the conditional distribution for the log the joint distribution then the log the product these conditionals over all nodes the graph and hence takes the form pai wijxj pai const where and const denotes terms independent see that this quadratic function the components and hence the joint distribution multivariate gaussian can determine the mean and covariance the joint distribution recursively follows each variable has conditional the states its parents gaussian distribution the form and wijxj where zero mean unit variance gaussian random variable satisfying and iij where iij the element the identity matrix taking the expectation have wije pai figure directed graph over three gaussian variables with one missing link bayesian networks thus can find the components starting the lowest numbered node and working recursively through the graph here again assume that the nodes are numbered such that each node has higher number than its parents similarly can use and obtain the element the covariance matrix for the form recursion relation cov wjk paj wjkcov iijvj paj and the covariance can similarly evaluated recursively starting from the lowest numbered node let consider two extreme cases first all suppose that there are links the graph which therefore comprises isolated nodes this case there are parameters wij and there are just parameters and parameters from the recursion relations and see that the mean given and the covariance matrix diagonal the form diag the joint distribution has total parameters and represents set independent univariate gaussian distributions now consider fully connected graph which each node has all lower numbered nodes parents the matrix wij then has entries the ith row and hence lower triangular matrix with entries the leading diagonal then the total number parameters wij obtained taking the number elements matrix subtracting account for the absence elements the leading diagonal and then dividing because the matrix has elements only below the diagonal giving total the total number independent parameters wij and the covariance matrix therefore corresponding general symmetric covariance matrix graphs having some intermediate level complexity correspond joint gaussian distributions with partially constrained covariance matrices consider for example the graph shown figure which has link missing between variables and using the recursion relations and see that the mean and covariance the joint distribution are given section exercise graphical models can readily extend the linear gaussian graphical model the case which the nodes the graph represent multivariate gaussian variables this case can write the conditional distribution for node the form pai pai wijxj section where now wij matrix which nonsquare and have different dimensionalities again easy verify that the joint distribution over all variables gaussian note that have already encountered specific example the linear gaussian relationship when saw that the conjugate prior for the mean gaussian variable itself gaussian distribution over the joint distribution over and therefore gaussian this corresponds simple two node graph which the node representing the parent the node representing the mean the distribution over parameter controlling prior and can viewed hyperparameter because the value this hyperparameter may itself unknown can again treat from bayesian perspective introducing prior over the hyperparameter sometimes called hyperprior which again given gaussian distribution this type construction can extended principle any level and illustration hierarchical bayesian model which shall encounter further examples later chapters"}, "148": {"Section": "8.2", "Title": "Conditional Independence", "Content": "important concept for probability distributions over multiple variables that conditional independence dawid consider three variables and and suppose that the conditional distribution given and such that does not depend the value that say that conditionally independent given this can expressed slightly different way consider the joint distribution and conditioned which can write the form where have used the product rule probability together with thus see that conditioned the joint distribution and factorizes into the product the marginal distribution and the marginal distribution again both conditioned this says that the variables and are statistically independent given note that our definition conditional independence will require that figure the first three examples graphs over three variables and used discuss conditional independence properties directed graphical models conditional independence equivalently must hold for every possible value and not just for some values shall sometimes use shorthand notation for conditional independence dawid which denotes that conditionally independent given and equivalent conditional independence properties play important role using probabilistic models for pattern recognition simplifying both the structure model and the computations needed perform inference and learning under that model shall see examples this shortly are given expression for the joint distribution over set variables terms product conditional distributions the mathematical representation underlying directed graph then could principle test whether any potential conditional independence property holds repeated application the sum and product rules probability practice such approach would very time consuming important and elegant feature graphical models that conditional independence properties the joint distribution can read directly from the graph without having perform any analytical manipulations the general framework for achieving this called separation where the stands for directed pearl here shall motivate the concept separation and give general statement the separation criterion formal proof can found lauritzen"}, "149": {"Section": "8.2.1", "Title": "Three example graphs", "Content": "begin our discussion the conditional independence properties directed graphs considering three simple examples each involving graphs having just three nodes together these will motivate and illustrate the key concepts separation the first the three examples shown figure and the joint distribution corresponding this graph easily written down using the general result give none the variables are observed then can investigate whether and are independent marginalizing both sides with respect give general this does not factorize into the product and where denotes the empty set and the symbol means that the conditional independence property does not hold general course may hold for particular distribution virtue the specific numerical values associated with the various conditional probabilities but does not follow general from the structure the graph now suppose condition the variable represented the graph figure from can easily write down the conditional distribution and given the form and obtain the conditional independence property can provide simple graphical interpretation this result considering the path from node node via the node said tail tail with respect this path because the node connected the tails the two arrows and the presence such path connecting nodes and causes these nodes dependent however when condition node figure the conditioned node blocks the path from and causes and become conditionally independent can similarly consider the graph shown figure the joint distribution corresponding this graph again obtained from our general formula give first all suppose that none the variables are observed again can test see and are independent marginalizing over give graphical models figure figure but where have conditioned the value variable figure the second our three examples node graphs used motivate the conditional independence framework for directed graphical models figure figure but now conditioning node conditional independence which general does not factorize into and before now suppose condition node shown figure using bayes theorem together with obtain and again obtain the conditional independence property before can interpret these results graphically the node said head tail with respect the path from node node such path connects nodes and and renders them dependent now observe figure then this observation blocks the path from and obtain the conditional independence property finally consider the third our node examples shown the graph figure shall see this has more subtle behaviour than the two previous graphs the joint distribution can again written down using our general result give consider first the case where none the variables are observed marginalizing both sides over obtain figure the last our three examples node graphs used explore conditional independence properties graphical models this graph has rather different properties from the two previous examples graphical models figure figure but conditioning the value node this graph the act conditioning induces dependence between and and and are independent with variables observed contrast the two previous examples can write this result now suppose condition indicated figure the conditional distribution and then given which general does not factorize into the product and thus our third example has the opposite behaviour from the first two graphically say that node head head with respect the path from because connects the heads the two arrows when node unobserved blocks the path and the variables and are independent however conditioning unblocks the path and renders and dependent there one more subtlety associated with this third example that need consider first introduce some more terminology say that node descendant node there path from which each step the path follows the directions the arrows then can shown that head head path will become unblocked either the node any its descendants observed summary tail tail node head tail node leaves path unblocked unless observed which case blocks the path contrast head head node blocks path unobserved but once the node and least one its descendants observed the path becomes unblocked worth spending moment understand further the unusual behaviour the graph figure consider particular instance such graph corresponding problem with three binary random variables relating the fuel system car shown figure the variables are called representing the state battery that either charged flat representing the state the fuel tank that either full fuel empty and which the state electric fuel gauge and which indicates either full empty exercise given the state the fuel tank and the battery the fuel gauge reads full with probabilities given conditional independence figure example node graph used illustrate the phenomenon explaining away the three nodes represent the state the battery the state the fuel tank and the reading the electric fuel gauge see the text for details the battery either charged flat and independently the fuel tank either full empty with prior probabilities this rather unreliable fuel gauge all remaining probabilities are determined the requirement that probabilities sum one and have complete specification the probabilistic model before observe any data the prior probability the fuel tank being empty now suppose that observe the fuel gauge and discover that reads empty corresponding the middle graph figure can use bayes theorem evaluate the posterior probability the fuel tank being empty first evaluate the denominator for bayes theorem given and similarly evaluate and using these results have and thus observing that the gauge reads empty makes more likely that the tank indeed empty would intuitively expect next suppose that also check the state the battery and find that flat have now observed the states both the fuel gauge and the battery shown the right hand graph figure the posterior probability that the fuel tank empty given the observations both the fuel gauge and the battery state then given graphical models where the prior probability has cancelled between numerator and denominator thus the probability that the tank empty has decreased from result the observation the state the battery this accords with our intuition that finding out that the battery flat explains away the observation that the fuel gauge reads empty see that the state the fuel tank and that the battery have indeed become dependent each other result observing the reading the fuel gauge fact this would also the case instead observing the fuel gauge directly observed the state some descendant note that the probability greater than the prior probability because the observation that the fuel gauge reads zero still provides some evidence favour empty fuel tank"}, "150": {"Section": "8.2.2", "Title": "D-separation", "Content": "now give general statement the separation property pearl for directed graphs consider general directed graph which and are arbitrary nonintersecting sets nodes whose union may smaller than the complete set nodes the graph wish ascertain whether particular conditional independence statement implied given directed acyclic graph consider all possible paths from any node any node any such path said blocked includes node such that either the arrows the path meet either head tail tail tail the node and the node the set the arrows meet head head the node and neither the node nor any its descendants the set all paths are blocked then said separated from and the joint distribution over all the variables the graph will satisfy the concept separation illustrated figure graph the path from not blocked node because tail tail node for this path and not observed nor blocked node because although the latter head head node has descendant because the conditioning set thus the conditional independence statement does not follow from this graph graph the path from blocked node because this tail tail node that observed and the conditional independence property will conditional independence figure illustration the concept separation see the text for details section satisfied any distribution that factorizes according this graph note that this path also blocked node because head head node and neither nor its descendant are the conditioning set for the purposes separation parameters such and figure indicated small filled circles behave the same was observed nodes however there are marginal distributions associated with such nodes consequently parameter nodes never themselves have parents and all paths through these nodes will always tail tail and hence blocked consequently they play role separation another example conditional independence and separation provided the concept independent identically distributed data introduced section consider the problem finding the posterior distribution for the mean univariate gaussian distribution this can represented the directed graph shown figure which the joint distribution defined prior together with set conditional distributions for practice observe and our goal infer suppose for moment that condition and consider the joint distribution the observations using separation note that there unique path from any any other and that this path tail tail with respect the observed node every such path blocked and the observations are independent given that figure directed graph corresponding the problem inferring the mean univariate gaussian distribution from observations the same graph drawn using the plate notation graphical models figure graphical representation the naive bayes model conditioned the class label the components the observed vector are assumed independent for classification section however integrate over the observations are general longer independent here latent variable because its value not observed another example model representing data the graph figure corresponding bayesian polynomial regression here the stochastic nodes corret see that the node for tail tail with respect spond and the path from any one the nodes and have the following conditional independence property thus conditioned the polynomial coefficients the predictive distribution for independent the training data can therefore first use the training data determine the posterior distribution over the coefficients and then can discard the training data and use the posterior distribution for make predictions for new input observations related graphical structure arises approach classification called the naive bayes model which use conditional independence assumptions simplify the model structure suppose our observed variable consists dimensional vector and wish assign observed values one classes using the encoding scheme can represent these classes kdimensional binary vector can then define generative model introducing multinomial prior over the class labels where the kth component the prior probability class together with conditional distribution for the observed vector the key assumption the naive bayes model that conditioned the class the distributions the input variables are independent the graphical representation this model shown figure see that observation blocks the path between and for because such paths are tail tail the node and and are conditionally independent given however marginalize out that unobserved the tail tail path from longer blocked this tells that general the marginal density will not factorize with respect the components encountered simple application the naive bayes model the context fusing data from different sources for medical diagnosis section are given labelled training set comprising inputs together with their class labels then can fit the naive bayes model the training data conditional independence using maximum likelihood assuming that the data are drawn independently from the model the solution obtained fitting the model for each class separately using the correspondingly labelled data example suppose that the probability density within each class chosen gaussian this case the naive bayes assumption then implies that the covariance matrix for each gaussian diagonal and the contours constant density within each class will axis aligned ellipsoids the marginal density however given superposition diagonal gaussians with weighting coefficients given the class priors and will longer factorize with respect its components the naive bayes assumption helpful when the dimensionality the input space high making density estimation the full dimensional space more challenging also useful the input vector contains both discrete and continuous variables since each can represented separately using appropriate models bernoulli distributions for binary observations gaussians for real valued variables the conditional independence assumption this model clearly strong one that may lead rather poor representations the class conditional densities nevertheless even this assumption not precisely satisfied the model may still give good classification performance practice because the decision boundaries can insensitive some the details the class conditional densities illustrated figure have seen that particular directed graph represents specific decomposition joint probability distribution into product conditional probabilities the graph also expresses set conditional independence statements obtained through the separation criterion and the separation theorem really expression the equivalence these two properties order make this clear helpful think directed graph filter suppose consider particular joint probability distribution over the variables corresponding the nonobserved nodes the graph the filter will allow this distribution pass through and only can expressed terms the factorization implied the graph present the filter the set all possible distributions over the set variables then the subset distributions that are passed the filter will denoted for directed factorization this illustrated figure alternatively can use the graph different kind filter first listing all the conditional independence properties obtained applying the separation criterion the graph and then allowing distribution pass only satisfies all these properties present all possible distributions this second kind filter then the separation theorem tells that the set distributions that will allowed through precisely the set should emphasized that the conditional independence properties obtained from separation apply any probabilistic model described that particular directed graph this will true for instance whether the variables are discrete continuous combination these again see that particular graph describing whole family probability distributions one extreme have fully connected graph that exhibits conditional independence properties all and which can represent any possible joint probability distribution over the given variables the set will contain all possible distribuk dxi pak pak dxi which the integral replaced summation the case discrete variables now observe that any factor pak that does not have any functional dependence can taken outside the integral over and will therefore cancel between numerator and denominator the only factors that remain will the conditional distribution pai for node itself together with the conditional distributions for any nodes such that node the conditioning set pak other words for which parent the conditional pai will depend the parents node whereas the conditionals pak will depend the children graphical models figure can view graphical model this case directed graph filter which probability distribution allowed through the filter and only satisfies the directed factorization property the set all possible probability distributions that pass through the filter denoted can alternatively use the graph filter distributions according whether they respect all the conditional independencies implied the separation properties the graph the separation theorem says that the same set distributions that will allowed through this second kind filter tions the other extreme have the fully disconnected graph one having links all this corresponds joint distributions which factorize into the product the marginal distributions over the variables comprising the nodes the graph note that for any given graph the set distributions will include any distributions that have additional independence properties beyond those described the graph for instance fully factorized distribution will always passed through the filter implied any graph over the corresponding set variables end our discussion conditional independence properties exploring the concept markov blanket markov boundary consider joint distribution represented directed graph having nodes and consider the conditional distribution particular node with variables conditioned all the remaining variables using the factorization property can express this conditional distribution the form"}, "151": {"Section": "8.3", "Title": "Markov Random Fields", "Content": "figure the markov blanket node comprises the set parents children and parents the node has the property that the conditional distribution conditioned all the remaining variables the graph dependent only the variables the markov blanket well the parents other words variables corresponding parents node other than node the set nodes comprising the parents the children and the parents called the markov blanket and illustrated figure can think the markov blanket node being the minimal set nodes that isolates from the rest the graph note that not sufficient include only the parents and children node because the phenomenon explaining away means that observations the child nodes will not block paths the parents must therefore observe the parent nodes also markov random fields have seen that directed graphical models specify factorization the joint distribution over set variables into product local conditional distributions they also define set conditional independence properties that must satisfied any distribution that factorizes according the graph turn now the second major class graphical models that are described undirected graphs and that again specify both factorization and set conditional independence relations markov random field also known markov network undirected graphical model kindermann and snell has set nodes each which corresponds variable group variables well set links each which connects pair nodes the links are undirected that they not carry arrows the case undirected graphs convenient begin with discussion conditional independence properties"}, "152": {"Section": "8.3.1", "Title": "Conditional independence properties", "Content": "the case directed graphs saw that was possible test whether particular conditional independence property holds applying graphical test called separation this involved testing whether not the paths connecting two sets nodes were blocked the definition blocked however was somewhat subtle due the presence paths having head head nodes might ask whether possible define alternative graphical semantics for probability distributions such that conditional independence determined simple graph separation this indeed the case and corresponds undirected graphical models removing the section graphical models figure example undirected graph which every path from any node set any node set passes through least one node set consequently the conditional independence property holds for any probability distribution described this graph directionality from the links the graph the asymmetry between parent and child nodes removed and the subtleties associated with head head nodes longer arise suppose that undirected graph identify three sets nodes denoted and and that consider the conditional independence property test whether this property satisfied probability distribution defined graph consider all possible paths that connect nodes set nodes set all such paths pass through one more nodes set then all such paths are blocked and the conditional independence property holds however there least one such path that not blocked then the property does not necessarily hold more precisely there will exist least some distributions corresponding the graph that not satisfy this conditional independence relation this illustrated with example figure note that this exactly the same the separation criterion except that there explaining away phenomenon testing for conditional independence undirected graphs therefore simpler than directed graphs alternative way view the conditional independence test imagine removing all nodes set from the graph together with any links that connect those nodes then ask there exists path that connects any node any node there are such paths then the conditional independence property must hold the markov blanket for undirected graph takes particularly simple form because node will conditionally independent all other nodes conditioned only the neighbouring nodes illustrated figure"}, "153": {"Section": "8.3.2", "Title": "Factorization properties", "Content": "now seek factorization rule for undirected graphs that will correspond the above conditional independence test again this will involve expressing the joint distribution product functions defined over sets variables that are local the graph therefore need decide what the appropriate notion locality this case markov random fields figure for undirected graph the markov blanket node consists the set neighbouring nodes has the property that the conditional distribution conditioned all the remaining variables the graph dependent only the variables the markov blanket consider two nodes and that are not connected link then these variables must conditionally independent given all other nodes the graph this follows from the fact that there direct path between the two nodes and all other paths pass through nodes that are observed and hence those paths are blocked this conditional independence property can expressed where denotes the set all variables with and removed the factorization the joint distribution must therefore such that and not appear the same factor order for the conditional independence property hold for all possible distributions belonging the graph this leads consider graphical concept called clique which defined subset the nodes graph such that there exists link between all pairs nodes the subset other words the set nodes clique fully connected furthermore maximal clique clique such that not possible include any other nodes from the graph the set without ceasing clique these concepts are illustrated the undirected graph over four variables shown figure this graph has five cliques two nodes given and well two maximal cliques given and the set not clique because the missing link from can therefore define the factors the decomposition the joint distribution functions the variables the cliques fact can consider functions the maximal cliques without loss generality because other cliques must subsets maximal cliques thus maximal clique and define arbitrary function over this clique then including another factor defined over subset these variables would redundant let denote clique and the set variables that clique then figure four node undirected graph showing clique outlined green and maximal clique outlined blue the joint distribution written product potential functions over the maximal cliques the graph graphical models here the quantity sometimes called the partition function normalization constant and given which ensures that the distribution given correctly normalized considering only potential functions which satisfy ensure that have assumed that comprises discrete variables but the framework equally applicable continuous variables combination the two which the summation replaced the appropriate combination summation and integration note that not restrict the choice potential functions those that have specific probabilistic interpretation marginal conditional distributions this contrast directed graphs which each factor represents the conditional distribution the corresponding variable conditioned the state its parents however special cases for instance where the undirected graph constructed starting with directed graph the potential functions may indeed have such interpretation shall see shortly one consequence the generality the potential functions that their product will general not correctly normalized therefore have introduce explicit normalization factor given recall that for directed graphs the joint distribution was automatically normalized consequence the normalization each the conditional distributions the factorization the presence this normalization constant one the major limitations undirected graphs have model with discrete nodes each having states then the evaluation the normalization term involves summing over states and the worst case exponential the size the model the partition function needed for parameter learning because will function any parameters that govern the potential functions however for evaluation local conditional distributions the partition function not needed because conditional the ratio two marginals and the partition function cancels between numerator and denominator when evaluating this ratio similarly for evaluating local marginal probabilities can work with the unnormalized joint distribution and then normalize the marginals explicitly the end provided the marginals only involves small number variables the evaluation their normalization coefficient will feasible far have discussed the notion conditional independence based simple graph separation and have proposed factorization the joint distribution that intended correspond this conditional independence structure however have not made any formal connection between conditional independence and factorization for undirected graphs need restrict attention potential functions that are strictly positive never zero negative for any markov random fields choice given this restriction can make precise relationship between factorization and conditional independence this again return the concept graphical model filter corresponding figure consider the set all possible distributions defined over fixed set variables corresponding the nodes particular undirected graph can define the set such distributions that are consistent with the set conditional independence statements that can read from the graph using graph separation similarly can define the set such distributions that can expressed factorization the form with respect the maximal cliques the graph the hammersley clifford theorem clifford states that the sets and are identical convenient express them exponentials that because are restricted potential functions which are strictly positive exp where called energy function and the exponential representation called the boltzmann distribution the joint distribution defined the product potentials and the total energy obtained adding the energies each the maximal cliques contrast the factors the joint distribution for directed graph the potentials undirected graph not have specific probabilistic interpretation although this gives greater flexibility choosing the potential functions because there normalization constraint does raise the question how motivate choice potential function for particular application this can done viewing the potential function expressing which configurations the local variables are preferred others global configurations that have relatively high probability are those that find good balance satisfying the possibly conflicting influences the clique potentials turn now specific example illustrate the use undirected graphs"}, "154": {"Section": "8.3.3", "Title": "Illustration: Image de-noising", "Content": "can illustrate the application undirected graphs using example noise removal from binary image besag geman and geman besag although very simple example this typical more sophisticated applications let the observed noisy image described array binary pixel values where the index runs over all pixels shall suppose that the image obtained taking unknown noise free image described binary pixel values and randomly flipping the sign pixels with some small probability example binary image together with noise corrupted image obtained flipping the sign the pixels with probability shown figure given the noisy image our goal recover the original noise free image because the noise level small know that there will strong correlation between and also know that neighbouring pixels and image are strongly correlated this prior knowledge can captured using the markov graphical models figure illustration image noising using markov random field the top row shows the original binary image the left and the corrupted image after randomly changing the pixels the right the bottom row shows the restored images obtained using iterated conditional models icm the left and using the graph cut algorithm the right icm produces image where the pixels agree with the original image whereas the corresponding number for graph cut random field model whose undirected graph shown figure this graph has two types cliques each which contains two variables the cliques the form have associated energy function that expresses the correlation between these variables choose very simple energy function for these cliques the form xiyi where positive constant this has the desired effect giving lower energy thus encouraging higher probability when and have the same sign and higher energy when they have the opposite sign the remaining cliques comprise pairs variables where and are indices neighbouring pixels again want the energy lower when the pixels have the same sign than when they have the opposite sign and choose energy given xixj where positive constant because potential function arbitrary nonnegative function over maximal clique can multiply any nonnegative functions subsets the clique markov random fields figure undirected graphical model representing markov random field for image noising which binary variable denoting the state pixel the unknown noise free image and denotes the corresponding value pixel the observed noisy image equivalently can add the corresponding energies this example this allows add extra term hxi for each pixel the noise free image such term has the effect biasing the model towards pixel values that have one particular sign preference the other the complete energy function for the model then takes the form xixj xiyi which defines joint distribution over and given exp now fix the elements the observed values given the pixels the noisy image which implicitly defines conditional distribution over noisefree images this example the ising model which has been widely studied statistical physics for the purposes image restoration wish find image having high probability ideally the maximum probability this shall use simple iterative technique called iterated conditional modes icm kittler and oglein which simply application coordinate wise gradient ascent the idea first initialize the variables which simply setting for all then take one node time and evaluate the total energy for the two possible states and keeping all other node variables fixed and set whichever state has the lower energy this will either leave the probability unchanged unchanged will increase because only one variable changed this simple local computation that can performed efficiently then repeat the update for another site and until some suitable stopping criterion satisfied the nodes may updated systematic way for instance repeatedly raster scanning through the image choosing nodes random have sequence updates which every site visited least once and which changes the variables are made then definition the algorithm exercise graphical models figure example directed graph the equivalent undirected graph will have converged local maximum the probability this need not however correspond the global maximum for the purposes this simple illustration have fixed the parameters and note that leaving simply means that the prior probabilities the two states are equal starting with the observed noisy image the initial configuration run icm until convergence leading the noised image shown the lower left panel figure note that set which effectively removes the links between neighbouring pixels then the global most probable solution given for all corresponding the observed noisy image later shall discuss more effective algorithm for finding high probability solutions called the max product algorithm which typically leads better solutions although this still not guaranteed find the global maximum the posterior distribution however for certain classes model including the one given there exist efficient algorithms based graph cuts that are guaranteed find the global maximum greig boykov kolmogorov and zabih the lower right panel figure shows the result applying graph cut algorithm the noising problem exercise section"}, "155": {"Section": "8.3.4", "Title": "Relation to directed graphs", "Content": "have introduced two graphical frameworks for representing probability distributions corresponding directed and undirected graphs and instructive discuss the relation between these consider first the problem taking model that specified using directed graph and trying convert undirected graph some cases this straightforward the simple example figure here the joint distribution for the directed graph given product conditionals the form now let convert this undirected graph representation shown figure the undirected graph the maximal cliques are simply the pairs neighbouring nodes and from wish write the joint distribution the form figure example simple directed graph and the corresponding moral graph markov random fields this easily done identifying where have absorbed the marginal for the first node into the first potential function note that this case the partition function let consider how generalize this construction that can convert any distribution specified factorization over directed graph into one specified factorization over undirected graph this can achieved the clique potentials the undirected graph are given the conditional distributions the directed graph order for this valid must ensure that the set variables that appears each the conditional distributions member least one clique the undirected graph for nodes the directed graph having just one parent this achieved simply replacing the directed link with undirected link however for nodes the directed graph having more than one parent this not sufficient these are nodes that have head head paths encountered our discussion conditional independence consider simple directed graph over nodes shown figure the joint distribution for the directed graph takes the form see that the factor involves the four variables and and these must all belong single clique this conditional distribution absorbed into clique potential ensure this add extra links between all pairs parents the node anachronistically this process marrying the parents has become known moralization and the resulting undirected graph after dropping the arrows called the moral graph important observe that the moral graph this example fully connected and exhibits conditional independence properties contrast the original directed graph thus general convert directed graph into undirected graph first add additional undirected links between all pairs parents for each node the graph and graphical models section section then drop the arrows the original links give the moral graph then initialize all the clique potentials the moral graph then take each conditional distribution factor the original directed graph and multiply into one the clique potentials there will always exist least one maximal clique that contains all the variables the factor result the moralization step note that all cases the partition function given the process converting directed graph into undirected graph plays important role exact inference techniques such the junction tree algorithm converting from undirected directed representation much less common and general presents problems due the normalization constraints saw that going from directed undirected representation had discard some conditional independence properties from the graph course could always trivially convert any distribution over directed graph into one over undirected graph simply using fully connected undirected graph this would however discard all conditional independence properties and would vacuous the process moralization adds the fewest extra links and retains the maximum number independence properties have seen that the procedure for determining the conditional independence properties different between directed and undirected graphs turns out that the two types graph can express different conditional independence properties and worth exploring this issue more detail return the view specific directed undirected graph filter that the set all possible distributions over the given variables could reduced subset that respects the conditional independencies implied the graph graph said map for dependency map distribution every conditional independence statement satisfied the distribution reflected the graph thus completely disconnected graph links will trivial map for any distribution alternatively can consider specific distribution and ask which graphs have the appropriate conditional independence properties every conditional independence statement implied graph satisfied specific distribution then the graph said map for independence map that distribution clearly fully connected graph will trivial map for any distribution the case that every conditional independence property the distribution reflected the graph and vice versa then the graph said perfect map for figure venn diagram illustrating the set all distributions over given set variables together with the set distributions that can represented perfect map using directed graph and the set that can represented perfect map using undirected graph"}, "156": {"Section": "8.4", "Title": "Inference in Graphical Models", "Content": "figure directed graph whose conditional independence properties cannot expressed using undirected graph over the same three variables that distribution perfect map therefore both map and map consider the set distributions such that for each distribution there exists directed graph that perfect map this set distinct from the set distributions such that for each distribution there exists undirected graph that perfect map addition there are distributions for which neither directed nor undirected graphs offer perfect map this illustrated venn diagram figure figure shows example directed graph that perfect map for distribution satisfying the conditional independence properties and there corresponding undirected graph over the same three variables that perfect map conversely consider the undirected graph over four variables shown figure this graph exhibits the properties and there directed graph over four variables that implies the same set conditional independence properties the graphical framework can extended consistent way graphs that include both directed and undirected links these are called chain graphs lauritzen and wermuth frydenberg and contain the directed and undirected graphs considered far special cases although such graphs can represent broader class distributions than either directed undirected alone there remain distributions for which even chain graph cannot provide perfect map chain graphs are not discussed further this book figure undirected graph whose conditional independence properties cannot expressed terms directed graph over the same variables inference graphical models turn now the problem inference graphical models which some the nodes graph are clamped observed values and wish compute the posterior distributions one more subsets other nodes shall see can exploit the graphical structure both find efficient algorithms for inference and graphical models figure graphical representation bayes see the text for details theorem make the structure those algorithms transparent specifically shall see that many algorithms can expressed terms the propagation local messages around the graph this section shall focus primarily techniques for exact inference and chapter shall consider number approximate inference algorithms start with let consider the graphical interpretation bayes theorem suppose decompose the joint distribution over two variables and into product factors the form this can represented the directed graph shown figure now suppose observe the value indicated the shaded node figure can view the marginal distribution prior over the latent variable and our goal infer the corresponding posterior distribution over using the sum and product rules probability can evaluate which can then used bayes theorem calculate thus the joint distribution now expressed terms and from graphical perspective the joint distribution now represented the graph shown figure which the direction the arrow reversed this the simplest example inference problem for graphical model"}, "157": {"Section": "8.4.1", "Title": "Inference on a chain", "Content": "now consider more complex problem involving the chain nodes the form shown figure this example will lay the foundation for discussion exact inference more general graphs later this section specifically shall consider the undirected graph figure have already seen that the directed chain can transformed into equivalent undirected chain because the directed graph does not have any nodes with more than one parent this does not require the addition any extra links and the directed and undirected versions this graph express exactly the same set conditional independence statements inference graphical models the joint distribution for this graph takes the form shall consider the specific case which the nodes represent discrete variables each having states which case each potential function comprises table and the joint distribution has parameters let consider the inference problem finding the marginal distribution for specific node that part way along the chain note that for the moment there are observed nodes definition the required marginal obtained summing the joint distribution over all variables except that naive implementation would first evaluate the joint distribution and then perform the summations explicitly the joint distribution can represented set numbers one for each possible value for because there are variables each with states there are values for and evaluation and storage the joint distribution well marginalization obtain all involve storage and computation that scale exponentially with the length the chain can however obtain much more efficient algorithm exploiting the conditional independence properties the graphical model substitute the factorized expression for the joint distribution into then can rearrange the order the summations and the multiplications allow the required marginal evaluated much more efficiently consider for instance the summation over the potential the only one that depends and can perform the summation first give function can then use this perform the summation over which will involve only this new function together with the potential because this the only other place that appears similarly the summation over involves only the potential and can performed separately give function and because each summation effectively removes variable from the distribution this can viewed the removal node from the graph group the potentials and summations together this way can express graphical models the desired marginal the form the reader encouraged study this ordering carefully the underlying idea forms the basis for the later discussion the general sum product algorithm here the key concept that are exploiting that multiplication distributive over addition that which the left hand side involves three arithmetic operations whereas the righthand side reduces this two operations let work out the computational cost evaluating the required marginal using this ordered expression have perform summations each which over states and each which involves function two variables for instance the summation over involves only the function which table numbers have sum this table over for each value and this has cost the resulting vector numbers multiplied the matrix numbers and again because there are summations and multiplications this kind the total cost evaluating the marginal this linear the length the chain contrast the exponential cost naive approach have therefore been able exploit the many conditional independence properties this simple graph order obtain efficient calculation the graph had been fully connected there would have been conditional independence properties and would have been forced work directly with the full joint distribution now give powerful interpretation this calculation terms the passing local messages around the graph from see that the expression for the marginal decomposes into the product two factors times the normalization constant shall interpret message passed forwards along the chain from node node similarly can viewed message passed backwards figure the marginal distribution for node along the chain obtained multiplying the two messages and and then normalizing these messages can themselves evaluated recursively passing messages from both ends the chain towards node inference graphical models along the chain node from node note that each the messages comprises set values one for each choice and the product two messages should interpreted the point wise multiplication the elements the two messages give another set values the message can evaluated recursively because therefore first evaluate and then apply repeatedly until reach the desired node note carefully the structure the message passing equation the outgoing message obtained multiplying the incoming message the local potential involving the node variable and the outgoing variable and then summing over the node variable similarly the message can evaluated recursively starting with node and using this recursive message passing illustrated figure the normalization constant easily evaluated summing the right hand side over all states operation that requires only computation graphs the form shown figure are called markov chains and the corresponding message passing equations represent example the chapmankolmogorov equations for markov processes papoulis graphical models now suppose wish evaluate the marginals for every node the chain simply applying the above procedure separately for each node will have computational cost that however such approach would very wasteful computation for instance find need propagate message from node back node similarly evaluate need propagate messages from node back node this will involve much duplicated computation because most the messages will identical the two cases suppose instead first launch message starting from node and propagate corresponding messages all the way back node and suppose similarly launch message starting from node and propagate the corresponding messages all the way forward node provided store all the intermediate messages along the way then any node can evaluate its marginal simply applying the computational cost only twice that for finding the marginal single node rather than times much observe that message has passed once each direction across each link the graph note also that the normalization constant need evaluated only once using any convenient node some the nodes the graph are observed then the corresponding variables are simply clamped their observed values and there summation see this note that the effect clamping variable observed value can expressed multiplying the joint distribution one more copies additional function and the value otherwise one such function can then absorbed into each the potentials that contain summations over then contain only one term which which takes the value when exercise chapter now suppose wish calculate the joint distribution for two neighbouring nodes the chain this similar the evaluation the marginal for single node except that there are now two variables that are not summed out few moments thought will show that the required joint distribution can written the form thus can obtain the joint distributions over all the sets variables each the potentials directly once have completed the message passing required obtain the marginals this useful result because practice may wish use parametric forms for the clique potentials equivalently for the conditional distributions started from directed graph order learn the parameters these potentials situations where not all the variables are observed can employ the algorithm and turns out that the local joint distributions the cliques conditioned any observed data precisely what needed the step shall consider some examples this detail chapter"}, "158": {"Section": "8.4.2", "Title": "Trees", "Content": "have seen that exact inference graph comprising chain nodes can performed efficiently time that linear the number nodes using algorithm inference graphical models figure examples treestructured graphs showing undirected tree directed tree and directed polytree exercise that can interpreted terms messages passed along the chain more generally inference can performed efficiently using local message passing broader class graphs called trees particular shall shortly generalize the message passing formalism derived above for chains give the sum product algorithm which provides efficient framework for exact inference tree structured graphs the case undirected graph tree defined graph which there one and only one path between any pair nodes such graphs therefore not have loops the case directed graphs tree defined such that there single node called the root which has parents and all other nodes have one parent convert directed tree into undirected graph see that the moralization step will not add any links all nodes have most one parent and consequence the corresponding moralized graph will undirected tree examples undirected and directed trees are shown figure and note that distribution represented directed tree can easily converted into one represented undirected tree and vice versa there are nodes directed graph that have more than one parent but there still only one path ignoring the direction the arrows between any two nodes then the graph called polytree illustrated figure such graph will have more than one node with the property having parents and furthermore the corresponding moralized undirected graph will have loops"}, "159": {"Section": "8.4.3", "Title": "Factor graphs", "Content": "the sum product algorithm that derive the next section applicable undirected and directed trees and polytrees can cast particularly simple and general form first introduce new graphical construction called factor graph frey kschischnang both directed and undirected graphs allow global function several variables expressed product factors over subsets those variables factor graphs make this decomposition explicit introducing additional nodes for the factors themselves addition the nodes representing the variables they also allow more explicit about the details the factorization shall see let write the joint distribution over set variables the form product factors where denotes subset the variables for convenience shall denote the graphical models figure example factor graph which corresponds the factorization individual variables however earlier discussions these can comprise groups variables such vectors matrices each factor function corresponding set variables directed graphs whose factorization defined represent special cases which the factors are local conditional distributions similarly undirected graphs given are special case which the factors are potential functions over the maximal cliques the normalizing coefficient can viewed factor defined over the empty set variables factor graph there node depicted usual circle for every variable the distribution was the case for directed and undirected graphs there are also additional nodes depicted small squares for each factor the joint distribution finally there are undirected links connecting each factor node all the variables nodes which that factor depends consider for example distribution that expressed terms the factorization this can expressed the factor graph shown figure note that there are two factors and that are defined over the same set variables undirected graph the product two such factors would simply lumped together into the same clique potential similarly and could combined into single potential over and the factor graph however keeps such factors explicit and able convey more detailed information about the underlying factorization figure undirected graph with single clique potential factor graph with factor representing the same distribution the undirected graph different factor graph representing the same distribution whose factors satisfy inference graphical models figure directed graph with the factorization factor graph representing the same distribution the directed graph whose factor satisfies different factor graph representing the same distribution with factors and factor graphs are said bipartite because they consist two distinct kinds nodes and all links between nodes opposite type general factor graphs can therefore always drawn two rows nodes variable nodes the top and factor nodes the bottom with links between the rows shown the example figure some situations however other ways laying out the graph may more intuitive for example when the factor graph derived from directed undirected graph shall see are given distribution that expressed terms undirected graph then can readily convert factor graph this create variable nodes corresponding the nodes the original undirected graph and then create additional factor nodes corresponding the maximal cliques the factors are then set equal the clique potentials note that there may several different factor graphs that correspond the same undirected graph these concepts are illustrated figure similarly convert directed graph factor graph simply create variable nodes the factor graph corresponding the nodes the directed graph and then create factor nodes corresponding the conditional distributions and then finally add the appropriate links again there can multiple factor graphs all which correspond the same directed graph the conversion directed graph factor graph illustrated figure have already noted the importance tree structured graphs for performing efficient inference take directed undirected tree and convert into factor graph then the result will again tree other words the factor graph will have loops and there will one and only one path connecting any two nodes the case directed polytree conversion undirected graph results loops due the moralization step whereas conversion factor graph again results tree illustrated figure fact local cycles directed graph due links connecting parents node can removed conversion factor graph defining the appropriate factor function shown figure have seen that multiple different factor graphs can represent the same directed undirected graph this allows factor graphs more specific about the graphical models figure directed polytree the result converting the polytree into undirected graph showing the creation loops the result converting the polytree into factor graph which retains the tree structure precise form the factorization figure shows example fully connected undirected graph along with two different factor graphs the joint distribution given general form whereas given the more specific factorization should emphasized that the factorization does not correspond any conditional independence properties"}, "160": {"Section": "8.4.4", "Title": "The sum-product algorithm", "Content": "shall now make use the factor graph framework derive powerful class efficient exact inference algorithms that are applicable tree structured graphs here shall focus the problem evaluating local marginals over nodes subsets nodes which will lead the sum product algorithm later shall modify the technique allow the most probable state found giving rise the max sum algorithm also shall suppose that all the variables the model are discrete and marginalization corresponds performing sums the framework however equally applicable linear gaussian models which case marginalization involves integration and shall consider example this detail when discuss linear dynamical systems section figure fragment directed graph having local cycle conversion fragment factor graph having tree structure which figure fully connected undirected graph and two factor graphs each which corresponds the undirected graph inference graphical models there algorithm for exact inference directed graphs without loops known belief propagation pearl lauritzen and spiegelhalter and equivalent special case the sum product algorithm here shall consider only the sum product algorithm because simpler derive and apply well being more general shall assume that the original graph undirected tree directed tree polytree that the corresponding factor graph has tree structure first convert the original graph into factor graph that can deal with both directed and undirected models using the same framework our goal exploit the structure the graph achieve two things obtain efficient exact inference algorithm for finding marginals situations where several marginals are required allow computations shared efficiently begin considering the problem finding the marginal for particular variable node for the moment shall suppose that all the variables are hidden later shall see how modify the algorithm incorporate evidence corresponding observed variables definition the marginal obtained summing the joint distribution over all variables except that where denotes the set variables with variable omitted the idea substitute for using the factor graph expression and then interchange summations and products order obtain efficient algorithm consider the fragment graph shown figure which see that the tree structure the graph allows partition the factors the joint distribution into groups with one group associated with each the factor nodes that neighbour the variable node see that the joint distribution can written product the form denotes the set factor nodes that are neighbours and denotes the set all variables the subtree connected the variable node via the factor node graphical models figure fragment factor graph illustrating the evaluation the marginal and represents the product all the factors the group associated with factor substituting into and interchanging the sums and products obtain here have introduced set functions defined which can viewed messages from the factor nodes the variable node see that the required marginal given the product all the incoming messages arriving node order evaluate these messages again turn figure and note that each factor described factor sub graph and can itself factorized particular can write xsm where for convenience have denoted the variables associated with factor addition this factorization illustrated figure note that the set variables the set variables which the factor depends and can also denoted using the notation substituting into obtain xsm xxm xsm where denotes the set variable nodes that are neighbours the factor node and denotes the same set but with node removed here have defined the following messages from variable nodes factor nodes xsm have therefore introduced two distinct kinds message those that from factor nodes variable nodes denoted and those that from variable nodes factor nodes denoted each case see that messages passed along link are always function the variable associated with the variable node that link connects the result says that evaluate the message sent factor node variable node along the link connecting them take the product the incoming messages along all other links coming into the factor node multiply the factor associated with that node and then marginalize over all the variables associated with the incoming messages this illustrated figure important note that factor node can send message variable node once has received incoming messages from all other neighbouring variable nodes finally derive expression for evaluating the messages from variable nodes factor nodes again making use the sub graph factorization from figure see that term xsm associated with node given product terms xml each associated with one the factor nodes that linked node excluding node that xsm xml where the product taken over all neighbours node except for node note that each the factors xml represents subtree the original graph precisely the same kind introduced substituting into figure illustration the factorization the subgraph associated with factor node inference graphical models xsm xml graphical models figure illustration the evaluation the message sent variable node adjacent factor node then obtain xml xml where have used the definition the messages passed from factor nodes variable nodes thus evaluate the message sent variable node adjacent factor node along the connecting link simply take the product the incoming messages along all the other links note that any variable node that has only two neighbours performs computation but simply passes messages through unchanged also note that variable node can send message factor node once has received incoming messages from all other neighbouring factor nodes recall that our goal calculate the marginal for variable node and that this marginal given the product incoming messages along all the links arriving that node each these messages can computed recursively terms other messages order start this recursion can view the node the root the tree and begin the leaf nodes from the definition see that leaf node variable node then the message that sends along its one and only link given illustrated figure similarly the leaf node factor node see from that the message sent should take the form figure the sum product algorithm begins with messages sent the leaf nodes which depend whether the leaf node variable node factor node inference graphical models illustrated figure this point worth pausing summarize the particular version the sumproduct algorithm obtained far for evaluating the marginal start viewing the variable node the root the factor graph and initiating messages the leaves the graph using and the message passing steps and are then applied recursively until messages have been propagated along every link and the root node has received messages from all its neighbours each node can send message towards the root once has received messages from all its other neighbours once the root node has received messages from all its neighbours the required marginal can evaluated using shall illustrate this process shortly see that each node will always receive enough messages able send out message can use simple inductive argument follows clearly for graph comprising variable root node connected directly several factor leaf nodes the algorithm trivially involves sending messages the form directly from the leaves the root now imagine building general graph adding nodes one time and suppose that for some particular graph have valid algorithm when one more variable factor node added can connected only single link because the overall graph must remain tree and the new node will leaf node therefore sends message the node which linked which turn will therefore receive all the messages requires order send its own message towards the root and again have valid algorithm thereby completing the proof now suppose wish find the marginals for every variable node the graph this could done simply running the above algorithm afresh for each such node however this would very wasteful many the required computations would repeated can obtain much more efficient procedure overlaying these multiple message passing algorithms obtain the general sum product algorithm follows arbitrarily pick any variable factor node and designate the root propagate messages from the leaves the root before this point the root node will have received messages from all its neighbours can therefore send out messages all its neighbours these turn will then have received messages from all their neighbours and can send out messages along the links going away from the root and this way messages are passed outwards from the root all the way the leaves now message will have passed both directions across every link the graph and every node will have received message from all its neighbours again simple inductive argument can used verify the validity this message passing protocol because every variable node will have received messages from all its neighbours can readily calculate the marginal distribution for every variable the graph the number messages that have computed given twice the number links the graph and involves only twice the computation involved finding single marginal comparison had run the sum product algorithm separately for each node the amount computation would grow quadratically with the size the graph note that this algorithm fact independent which node was designated the root exercise graphical models figure the sum product algorithm can viewed purely terms messages sent out factor nodes other factor nodes this example the outgoing message shown the blue arrow obtained taking the product all the incoming messages shown green arrows multiplying the factor and marginalizing over the variables and and indeed the notion one node having special status was introduced only convenient way explain the message passing protocol next suppose wish find the marginal distributions associated with the sets variables belonging each the factors similar argument that used above easy see that the marginal associated with factor given the product messages arriving the factor node and the local factor that node exercise the factors are complete analogy with the marginals the variable nodes parameterized functions and wish learn the values the parameters using the algorithm then these marginals are precisely the quantities will need calculate the step shall see detail when discuss the hidden markov model chapter the message sent variable node factor node have seen simply the product the incoming messages other links can wish view the sum product algorithm slightly different form eliminating messages from variable nodes factor nodes and simply considering messages that are sent out factor nodes this most easily seen considering the example figure far have rather neglected the issue normalization the factor graph was derived from directed graph then the joint distribution already correctly normalized and the marginals obtained the sum product algorithm will similarly normalized correctly however started from undirected graph then general there will unknown normalization coefficient with the simple chain example figure this easily handled working with unnormalp first run the ized version the sum product algorithm find the corresponding unnormalized marginals coefficient then easily obtained normalizing any one these marginals and this computationally efficient because the normalization done over single variable rather than over the entire set variables would required normalize directly the joint distribution where this point may helpful consider simple example illustrate the operation the sum product algorithm figure shows simple node factor figure simple factor graph used illustrate the sum product algorithm inference graphical models graph whose unnormalized joint distribution given order apply the sum product algorithm this graph let designate node the root which case there are two leaf nodes and starting with the leaf nodes then have the following sequence six messages the direction flow these messages illustrated figure once this message propagation complete can then propagate messages from the root node out the leaf nodes and these are given graphical models figure flow messages for the sum product algorithm applied the example graph figure from the leaf nodes and towards the root node from the root node towards the leaf nodes one message has now passed each direction across each link and can now evaluate the marginals simple check let verify that the marginal given the correct expression using and substituting for the messages using the above results have required far have assumed that all the variables the graph are hidden most practical applications subset the variables will observed and wish calculate posterior distributions conditioned these observations observed nodes are easily handled within the sum product algorithm follows suppose partition into hidden variables and observed variables and that the observed value denoted where otherwise this product corresponds runto ning the sum product algorithm can efficiently calculate the posterior marginals normalization coefficient whose value can found efficiently using local computation any summations over variables then collapse into single term then simply multiply the joint distribution and hence unnormalized version and have assumed throughout this section that are dealing with discrete variables however there nothing specific discrete variables either the graphical framework the probabilistic construction the sum product algorithm for inference graphical models table example joint distribution over two binary variables for which the maximum the joint distribution occurs for different variable values compared the maxima the two marginals section continuous variables the summations are simply replaced integrations shall give example the sum product algorithm applied graph linear gaussian variables when consider linear dynamical systems"}, "161": {"Section": "8.4.5", "Title": "The max-sum algorithm", "Content": "the sum product algorithm allows take joint distribution expressed factor graph and efficiently find marginals over the component variables two other common tasks are find setting the variables that has the largest probability and find the value that probability these can addressed through closely related algorithm called max sum which can viewed application dynamic programming the context graphical models cormen simple approach finding latent variable values having high probability would run the sum product algorithm obtain the marginals for every variable and then for each marginal turn find the value that maximizes that marginal however this would give the set values that are individually the most probable practice typically wish find the set values that jointly have the largest probability other words the vector xmax that maximizes the joint distribution that xmax arg max for which the corresponding value the joint probability will given xmax max general xmax not the same the set values can easily show using simple example consider the joint distribution over two binary variables given table the joint distribution maximized setting and corresponding the value however the marginal for obtained summing over both values given and and similarly the marginal for given and and the marginals are maximized and which corresponds value for the joint distribution fact not difficult construct examples for which the set individually most probable values has probability zero under the joint distribution therefore seek efficient algorithm for finding the value that maximizes the joint distribution and that will allow obtain the value the joint distribution its maximum address the second these problems shall simply write out the max operator terms its components max max max exercise graphical models where the total number variables and then substitute for using its expansion terms product factors deriving the sum product algorithm made use the distributive law for multiplication here make use the analogous law for the max operator max max which holds will always the case for the factors graphical model this allows exchange products with maximizations consider first the simple example chain nodes described the evaluation the probability maximum can written max max max max max with the calculation marginals see that exchanging the max and product operators results much more efficient computation and one that easily interpreted terms messages passed from node backwards along the chain node can readily generalize this result arbitrary tree structured factor graphs substituting the expression for the factor graph expansion into and again exchanging maximizations with products the structure this calculation identical that the sum product algorithm and can simply translate those results into the present context particular suppose that designate particular variable node the root the graph then start set messages propagating inwards from the leaves the tree towards the root with each node sending its message towards the root once has received all incoming messages from its other neighbours the final maximization performed over the product all messages arriving the root node and gives the maximum value for this could called the max product algorithm and identical the sum product algorithm except that summations are replaced maximizations note that this stage messages have been sent from leaves the root but not the other direction practice products many small probabilities can lead numerical underflow problems and convenient work with the logarithm the joint distribution the logarithm monotonic function that then and hence the max operator and the logarithm function can interchanged that max max the distributive property preserved because max max thus taking the logarithm simply has the effect replacing the products the max product algorithm with sums and obtain the max sum algorithm from inference graphical models the results and derived earlier for the sum product algorithm can readily write down the max sum algorithm terms message passing simply replacing sum with max and replacing products with sums logarithms give max the initial messages sent the leaf nodes are obtained analogy with and and are given while the root node the maximum probability can then computed analogy with using pmax max far have seen how find the maximum the joint distribution propagating messages from the leaves arbitrarily chosen root node the result will the same irrespective which node chosen the root now turn the second problem finding the configuration the variables for which the joint distribution attains this maximum value far have sent messages from the leaves the root the process evaluating will also give the value xmax for the most probable value the root node variable defined xmax arg max this point might tempted simply continue with the message passing algorithm and send messages from the root back out the leaves using and then apply all the remaining variable nodes however because are now maximizing rather than summing possible that there may multiple configurations all which give rise the maximum value for such cases this strategy can fail because possible for the individual variable values obtained maximizing the product messages each node belong different maximizing configurations giving overall configuration that longer corresponds maximum the problem can resolved adopting rather different kind message passing from the root node the leaves see how this works let return once again the simple chain example variables each having states graphical models figure lattice trellis diagram showing explicitly the possible states one per row the diagram for each the variables the chain model this illustration the arrow shows the direction message passing the max product algorithm for every state each variable corresponding column the diagram the function defines unique state the previous variable indicated the black lines the two paths through the lattice correspond configurations that give the global maximum the joint probability distribution and either these can found tracing back along the black lines the opposite direction the arrow corresponding the graph shown figure suppose take node the root node then the first phase propagate messages from the leaf node the root node using max which follow from applying and this particular graph the initial message sent from the leaf node simply the most probable value for then given arg max xmax now need determine the states the previous variables that correspond the same maximizing configuration this can done keeping track which values the variables gave rise the maximum state each variable other words storing quantities given arg max understand better what happening helpful represent the chain variables terms lattice trellis diagram shown figure note that this not probabilistic graphical model because the nodes represent individual states variables while each variable corresponds column such states the diagram for each state given variable there unique state the previous variable that maximizes the probability ties are broken either systematically random corresponding the function given and this indicated inference graphical models the lines connecting the nodes once know the most probable value the final node can then simply follow the link back find the most probable state node and back the initial node this corresponds propagating message back down the chain using xmax xmax and known back tracking note that there could several values all which give the maximum value provided chose one these values when the back tracking are assured globally consistent maximizing configuration figure have indicated two paths each which shall suppose corresponds global maximum the joint probability distribution and each represent possible values xmax then starting from either state and tracing back along the black lines which corresponds iterating obtain valid global maximum configuration note that had run forward pass max sum message passing followed backward pass and then applied each node separately could end selecting some states from one path and some from the other path giving overall configuration that not global maximizer see that necessary instead keep track the maximizing states during the forward pass using the functions and then use back tracking find consistent solution the extension general tree structured factor graph should now clear message sent from factor node variable node maximization performed over all other variable nodes that are neighbours that factor node using when perform this maximization keep record which values the variables gave rise the maximum then the back tracking step having found xmax can then use these stored values assign consistent maximizing states xmax the max sum algorithm with back tracking gives exact maximizing configuration for the variables provided the factor graph tree important application this technique for finding the most probable sequence hidden states hidden markov model which case known the viterbi algorithm xmax with the sum product algorithm the inclusion evidence the form observed variables straightforward the observed variables are clamped their observed values and the maximization performed over the remaining hidden variables this can shown formally including identity functions for the observed variables into the factor functions did for the sum product algorithm interesting compare max sum with the iterated conditional modes icm algorithm described page each step icm computationally simpler because the messages that are passed from one node the next comprise single value consisting the new state the node for which the conditional distribution maximized the max sum algorithm more complex because the messages are functions node variables and hence comprise set values for each possible state unlike max sum however icm not guaranteed find global maximum even for tree structured graphs section graphical models"}, "162": {"Section": "8.4.6", "Title": "Exact inference in general graphs", "Content": "the sum product and max sum algorithms provide efficient and exact solutions inference problems tree structured graphs for many practical applications however have deal with graphs having loops the message passing framework can generalized arbitrary graph topologies giving exact inference procedure known the junction tree algorithm lauritzen and spiegelhalter jordan here give brief outline the key steps involved this not intended convey detailed understanding the algorithm but rather give flavour the various stages involved the starting point directed graph first converted undirected graph moralization whereas starting from undirected graph this step not required next the graph triangulated which involves finding chord less cycles containing four more nodes and adding extra links eliminate such chord less cycles for instance the graph figure the cycle chord less link could added between and alternatively between and note that the joint distribution for the resulting triangulated graph still defined product the same potential functions but these are now considered functions over expanded sets variables next the triangulated graph used construct new tree structured undirected graph called join tree whose nodes correspond the maximal cliques the triangulated graph and whose links connect pairs cliques that have variables common the selection which pairs cliques connect this way important and done give maximal spanning tree defined follows all possible trees that link the cliques the one that chosen one for which the weight the tree largest where the weight for link the number nodes shared the two cliques connects and the weight for the tree the sum the weights for the links the tree condensed that any clique that subset another clique absorbed into the larger clique this gives junction tree consequence the triangulation step the resulting tree satisfies the running intersection property which means that variable contained two cliques then must also contained every clique the path that connects them this ensures that inference about variables will consistent across the graph finally two stage message passing algorithm essentially equivalent the sum product algorithm can now applied this junction tree order find marginals and conditionals although the junction tree algorithm sounds complicated its heart the simple idea that have used already exploiting the factorization properties the distribution allow sums and products interchanged that partial summations can performed thereby avoiding having work directly with the joint distribution the role the junction tree provide precise and efficient way organize these computations worth emphasizing that this achieved using purely graphical operations the junction tree exact for arbitrary graphs and efficient the sense that for given graph there does not general exist computationally cheaper approach unfortunately the algorithm must work with the joint distributions within each node each which corresponds clique the triangulated graph and the computational cost the algorithm determined the number variables the largest inference graphical models clique and will grow exponentially with this number the case discrete variables important concept the treewidth graph bodlaender which defined terms the number variables the largest clique fact defined one less than the size the largest clique ensure that tree has treewidth because there general there can multiple different junction trees that can constructed from given starting graph the treewidth defined the junction tree for which the largest clique has the fewest variables the treewidth the original graph high the junction tree algorithm becomes impractical"}, "163": {"Section": "8.4.7", "Title": "Loopy belief propagation", "Content": "for many problems practical interest will not feasible use exact inference and need exploit effective approximation methods important class such approximations that can broadly called variational methods will discussed detail chapter complementing these deterministic approaches wide range sampling methods also called monte carlo methods that are based stochastic numerical sampling from distributions and that will discussed length chapter here consider one simple approach approximate inference graphs with loops which builds directly the previous discussion exact inference trees the idea simply apply the sum product algorithm even though there guarantee that will yield good results this approach known loopy belief propagation frey and mackay and possible because the message passing rules and for the sum product algorithm are purely local however because the graph now has cycles information can flow many times around the graph for some models the algorithm will converge whereas for others will not order apply this approach need define message passing schedule let assume that one message passed time any given link and any given direction each message sent from node replaces any previous message sent the same direction across the same link and will itself function only the most recent messages received that node previous steps the algorithm have seen that message can only sent across link from node when all other messages have been received that node across its other links because there are loops the graph this raises the problem how initiate the message passing algorithm resolve this suppose that initial message given the unit function has been passed across every link each direction every node then position send message there are now many possible ways organize the message passing schedule for example the flooding schedule simultaneously passes message across every link both directions each time step whereas schedules that pass one message time are called serial schedules following kschischnang will say that variable factor node has message pending its link node node has received any message any its other links since the last time send message thus when node receives message one its links this creates pending messages all its other links only pending messages need transmitted because graphical models exercise other messages would simply duplicate the previous message the same link for graphs that have tree structure any schedule that sends only pending messages will eventually terminate once message has passed each direction across every link this point there are pending messages and the product the received messages every variable give the exact marginal graphs having loops however the algorithm may never terminate because there might always pending messages although practice generally found converge within reasonable time for most applications once the algorithm has converged once has been stopped convergence not observed the approximate local marginals can computed using the product the most recently received incoming messages each variable node factor node every link some applications the loopy belief propagation algorithm can give poor results whereas other applications has proven very effective particular state the art algorithms for decoding certain kinds error correcting codes are equivalent loopy belief propagation gallager berrou mceliece mackay and neal frey"}, "164": {"Section": "8.4.8", "Title": "Learning the graph structure", "Content": "our discussion inference graphical models have assumed that the structure the graph known and fixed however there also interest going beyond the inference problem and learning the graph structure itself from data friedman and koller this requires that define space possible structures well measure that can used score each structure from bayesian viewpoint would ideally like compute posterior distribution over graph structures and make predictions averaging with respect this distribution have prior over graphs indexed then the posterior distribution given where the observed data set the model evidence then provides the score for each model however evaluation the evidence involves marginalization over the latent variables and presents challenging computational problem for many models exploring the space structures can also problematic because the number different graph structures grows exponentially with the number nodes often necessary resort heuristics find good candidates"}, "165": {"Section": "9", "Title": "Mixture Models and EM", "Content": "define joint distribution over observed and latent variables the corresponding distribution the observed variables alone obtained marginalization this allows relatively complex marginal distributions over observed variables expressed terms more tractable joint distributions over the expanded space observed and latent variables the introduction latent variables thereby allows complicated distributions formed from simpler components this chapter shall see that mixture distributions such the gaussian mixture discussed section can interpreted terms discrete latent variables continuous latent variables will form the subject chapter well providing framework for building more complex probability distributions mixture models can also used cluster data therefore begin our discussion mixture distributions considering the problem finding clusters set data points which approach first using nonprobabilistic technique called the means algorithm lloyd then introduce the latent variable section mixture models and section section section view mixture distributions which the discrete latent variables can interpreted defining assignments data points specific components the mixture general technique for finding maximum likelihood estimators latent variable models the expectation maximization algorithm first all use the gaussian mixture distribution motivate the algorithm fairly informal way and then give more careful treatment based the latent variable viewpoint shall see that the means algorithm corresponds particular nonprobabilistic limit applied mixtures gaussians finally discuss some generality gaussian mixture models are widely used data mining pattern recognition machine learning and statistical analysis many applications their parameters are determined maximum likelihood typically using the algorithm however shall see there are some significant limitations the maximum likelihood approach and chapter shall show that elegant bayesian treatment can given using the framework variational inference this requires little additional computation compared with and resolves the principal difficulties maximum likelihood while also allowing the number components the mixture inferred automatically from the data"}, "166": {"Section": "9.1", "Title": "K-means Clustering", "Content": "begin considering the problem identifying groups clusters data points multidimensional space suppose have data set consisting observations random dimensional euclidean variable our goal partition the data set into some number clusters where shall suppose for the moment that the value given intuitively might think cluster comprising group data points whose inter point distances are small compared with the distances points outside the cluster can formalize this notion first introducing set dimensional vectors where which prototype associated with the kth cluster shall see shortly can think the representing the centres the clusters our goal then find assignment data points clusters well set vectors such that the sum the squares the distances each data point its closest vector minimum convenient this point define some notation describe the assignment data points clusters for each data point introduce corresponding set binary indicator variables rnk where describing which the clusters the data point assigned that data point assigned cluster then rnk and rnj for this known the coding scheme can then define objective function sometimes called distortion measure given rnk which represents the sum the squares the distances each data point its which can easily solve for give rnk rnkxn rnk the denominator this expression equal the number points assigned cluster and this result has simple interpretation namely set equal the mean all the data points assigned cluster for this reason the procedure known the means algorithm the two phases assigning data points clusters and computing the cluster means are repeated turn until there further change the assignments until some maximum number iterations exceeded because each phase reduces the value the objective function convergence the algorithm assured however may converge local rather than global minimum the convergence properties the means algorithm were studied macqueen the means algorithm illustrated using the old faithful data set figure for the purposes this example have made linear scaling the data known standardizing such that each the variables has zero mean and unit standard deviation for this example have chosen and this means clustering assigned vector our goal find values for the rnk and the minimize can this through iterative procedure which each iteration involves two successive steps corresponding successive optimizations with respect the rnk and the first choose some initial values for the then the first phase minimize with respect the rnk keeping the fixed the second phase minimize with respect the keeping rnk fixed this two stage optimization then repeated until convergence shall see that these two stages updating rnk and updating correspond respectively the expectation and maximization steps the algorithm and emphasize this shall use the terms step and step the context the means algorithm consider first the determination the rnk because linear function rnk this optimization can performed easily give closed form solution the terms involving different are independent and can optimize for each separately choosing rnk for whichever value gives the minimum value other words simply assign the nth data point the closest cluster centre more formally this can expressed arg minj otherwise rnk now consider the optimization the with the rnk held fixed the objective function quadratic function and can minimized setting its derivative with respect zero giving section exercise appendix mixture models and figure illustration the means algorithm using the scaled old faithful data set green points denote the data set two dimensional euclidean space the initial choices for centres and are shown the red and blue crosses respectively the initial step each data point assigned either the red cluster the blue cluster according which cluster centre nearer this equivalent classifying the points according which side the perpendicular bisector the two cluster centres shown the magenta line they lie the subsequent step each cluster centre computed the mean the points assigned the corresponding cluster show successive and steps through final convergence the algorithm means clustering figure plot the cost function given after each step blue points and step red points the kmeans algorithm for the example shown figure the algorithm has converged after the third step and the final cycle produces changes either the assignments the prototype vectors case the assignment each data point the nearest cluster centre equivalent classification the data points according which side they lie the perpendicular bisector the two cluster centres plot the cost function given for the old faithful example shown figure note that have deliberately chosen poor initial values for the cluster centres that the algorithm takes several steps before convergence practice better initialization procedure would choose the cluster centres equal random subset data points also worth noting that the means algorithm itself often used initialize the parameters gaussian mixture model before applying the algorithm direct implementation the means algorithm discussed here can relatively slow because each step necessary compute the euclidean distance between every prototype vector and every data point various schemes have been proposed for speeding the means algorithm some which are based precomputing data structure such tree such that nearby points are the same subtree ramasubramanian and paliwal moore other approaches make use the triangle inequality for distances thereby avoiding unnecessary distance calculations hodgson elkan far have considered batch version means which the whole data set used together update the prototype vectors can also derive line stochastic algorithm macqueen applying the robbins monro procedure the problem finding the roots the regression function given the derivatives with respect this leads sequential update which for each data point turn update the nearest prototype using section section exercise new old old where the learning rate parameter which typically made decrease monotonically more data points are considered the means algorithm based the use squared euclidean distance the measure dissimilarity between data point and prototype vector not only does this limit the type data variables that can considered would inappropriate for cases where some all the variables represent categorical labels for instance mixture models and section but can also make the determination the cluster means nonrobust outliers can generalize the means algorithm introducing more general dissimilarity measure between two vectors and and then minimizing the following distortion measure rnkv which gives the medoids algorithm the step again involves for given cluster prototypes assigning each data point the cluster for which the dissimilarity the corresponding prototype smallest the computational cost this the case for the standard means algorithm for general choice dissimilarity measure the step potentially more complex than for means and common restrict each cluster prototype equal one the data vectors assigned that cluster this allows the algorithm implemented for any choice dissimilarity measure long can readily evaluated thus the step involves for each cluster discrete search over the points assigned that cluster which requires one notable feature the means algorithm that each iteration every data point assigned uniquely one and only one the clusters whereas some data points will much closer particular centre than any other centre there may other data points that lie roughly midway between cluster centres the latter case not clear that the hard assignment the nearest cluster the most appropriate shall see the next section that adopting probabilistic approach obtain soft assignments data points clusters way that reflects the level uncertainty over the most appropriate assignment this probabilistic formulation brings with numerous benefits evaluations"}, "167": {"Section": "9.1.1", "Title": "Image segmentation and compression", "Content": "illustration the application the means algorithm consider the related problems image segmentation and image compression the goal segmentation partition image into regions each which has reasonably homogeneous visual appearance which corresponds objects parts objects forsyth and ponce each pixel image point dimensional space comprising the intensities the red blue and green channels and our segmentation algorithm simply treats each pixel the image separate data point note that strictly this space not euclidean because the channel intensities are bounded the interval nevertheless can apply the means algorithm without difficulty illustrate the result running means convergence for any particular value drawing the image replacing each pixel vector with the intensity triplet given the centre which that pixel has been assigned results for various values are shown figure see that for given value the algorithm representing the image using palette only colours should emphasized that this use means not particularly sophisticated approach image segmentation not least because takes account the spatial proximity different pixels the image segmentation problem general extremely difficult original image means clustering figure two examples the application the means clustering algorithm image segmentation showing the initial images together with their means segmentations obtained using various values this also illustrates the use vector quantization for data compression which smaller values give higher compression the expense poorer image quality and remains the subject active research and introduced here simply illustrate the behaviour the means algorithm can also use the result clustering algorithm perform data compression important distinguish between lossless data compression which the goal able reconstruct the original data exactly from the compressed representation and lossy data compression which accept some errors the reconstruction return for higher levels compression than can achieved the lossless case can apply the means algorithm the problem lossy data compression follows for each the data points store only the identity the cluster which assigned also store the values the cluster centres which typically requires significantly less data provided choose each data point then approximated its nearest centre new data points can similarly compressed first finding the nearest and then storing the label instead the original data vector this framework often called vector quantization and the vectors are called code book vectors mixture models and the image segmentation problem discussed above also provides illustration the use clustering for data compression suppose the original image has pixels comprising values each which stored with bits precision then transmit the whole image directly would cost bits now suppose first run means the image data and then instead transmitting the original pixel intensity vectors transmit the identity the nearest vector because there are such vectors this requires log bits per pixel must also transmit the code book vectors which requires bits and the total number bits required transmit the image log rounding the nearest integer the original image shown figure has pixels and requires bits transmit directly comparison the compressed images require bits bits and bits respectively transmit these represent compression ratios compared the original image and respectively see that there trade off between degree compression and image quality note that our aim this example illustrate the means algorithm had been aiming produce good image compressor then would more fruitful consider small blocks adjacent pixels for instance and thereby exploit the correlations that exist natural images between nearby pixels"}, "168": {"Section": "9.2", "Title": "Mixtures of Gaussians", "Content": "section motivated the gaussian mixture model simple linear superposition gaussian components aimed providing richer class density models than the single gaussian now turn formulation gaussian mixtures terms discrete latent variables this will provide with deeper insight into this important distribution and will also serve motivate the expectation maximization algorithm recall from that the gaussian mixture distribution can written linear superposition gaussians the form let introduce dimensional binary random variable having representation which particular element equal and all other elements are equal the values therefore satisfy and and see that there are possible states for the vector according which element nonzero shall define the joint distribution terms marginal distribution and conditional distribution corresponding the graphical model figure the marginal distribution over specified terms the mixing coefficients such that figure graphical representation mixture model which the joint distribution expressed the form mixtures gaussians where the parameters must satisfy together with order valid probabilities because uses representation can also write this distribution the form similarly the conditional distribution given particular value for gaussian which can also written the form exercise the joint distribution given and the marginal distribution then obtained summing the joint distribution over all possible states give where have made use and thus the marginal distribution gaussian mixture the form have several observations then because have represented the marginal distribution the form follows that for every observed data point there corresponding latent variable have therefore found equivalent formulation the gaussian mixture involving explicit latent variable might seem that have not gained much doing however are now able work with the joint distribution mixture models and instead the marginal distribution and this will lead significant simplifications most notably through the introduction the expectation maximization algorithm another quantity that will play important role the conditional probability given shall use denote whose value can found using bayes theorem section shall view the prior probability and the quantity the corresponding posterior probability once have observed shall see later can also viewed the responsibility that component takes for explaining the observation can use the technique ancestral sampling generate random samples distributed according the gaussian mixture model this first generate from the marginal distribution and then generate value for which denote value for from the conditional distribution techniques for sampling from standard distributions are discussed chapter can depict samples from the joint distribution plotting points the corresponding values and then colouring them according the value other words according which gaussian component was responsible for generating them shown figure similarly samples from the marginal distribution are obtained taking the samples from the joint distribution and ignoring the values these are illustrated figure plotting the values without any coloured labels can also use this synthetic data set illustrate the responsibilities evaluating for every data point the posterior probability for each component the mixture distribution from which this data set was generated particular can represent the value the responsibilities znk associated with data point plotting the corresponding point using proportions red blue and green ink given znk for respectively shown figure for instance data point for which will coloured red whereas one for which will coloured with equal proportions blue and green ink and will appear cyan this should compared with figure which the data points were labelled using the true identity the component from which they were generated"}, "169": {"Section": "9.2.1", "Title": "Maximum likelihood", "Content": "suppose have data set observations and wish model this data using mixture gaussians can represent this data set mixtures gaussians figure example points drawn from the mixture gaussians shown figure samples from the joint distribution which the three states corresponding the three components the mixture are depicted red green and blue and the corresponding samples from the marginal distribution which obtained simply ignoring the values and just plotting the values the data set said complete whereas that incomplete the same samples which the colours represent the value the responsibilities znk associated with data point obtained plotting the corresponding point using proportions red blue and green ink given znk for respectively matrix which the nth row given similarly the corresponding latent variables will denoted matrix with rows assume that the data points are drawn independently from the distribution then can express the gaussian mixture model for this data set using the graphical representation shown figure from the log the likelihood function given before discussing how maximize this function worth emphasizing that there significant problem associated with the maximum likelihood framework applied gaussian mixture models due the presence singularities for simplicity consider gaussian mixture whose components have covariance matrices given where the unit matrix although the conclusions will hold for general covariance matrices suppose that one the components the mixture model let say the jth component has its mean exactly equal one the data figure graphical representation gaussian mixture model for set data points with corresponding latent points where mixture models and figure illustration how singularities the likelihood function arise with mixtures gaussians this should compared with the case single gaussian shown figure for which singularities arise points that for some value this data point will then contribute term the likelihood function the form consider the limit then see that this term goes infinity and the log likelihood function will also infinity thus the maximization the log likelihood function not well posed problem because such singularities will always present and will occur whenever one the gaussian components collapses onto specific data point recall that this problem did not arise the case single gaussian distribution understand the difference note that single gaussian collapses onto data point will contribute multiplicative factors the likelihood function arising from the other data points and these factors will zero exponentially fast giving overall likelihood that goes zero rather than infinity however once have least two components the mixture one the components can have finite variance and therefore assign finite probability all the data points while the other component can shrink onto one specific data point and thereby contribute ever increasing additive value the log likelihood this illustrated figure these singularities provide another example the severe over fitting that can occur maximum likelihood approach shall see that this difficulty does not occur adopt bayesian approach for the moment however simply note that applying maximum likelihood gaussian mixture models must take steps avoid finding such pathological solutions and instead seek local maxima the likelihood function that are well behaved can hope avoid the singularities using suitable heuristics for instance detecting when gaussian component collapsing and resetting its mean randomly chosen value while also resetting its covariance some large value and then continuing with the optimization further issue finding maximum likelihood solutions arises from the fact that for any given maximum likelihood solution component mixture will have total equivalent solutions corresponding the ways assigning sets parameters components other words for any given nondegenerate point the space parameter values there will further additional points all which give rise exactly the same distribution this problem known section mixtures gaussians identifiability casella and berger and important issue when wish interpret the parameter values discovered model identifiability will also arise when discuss models having continuous latent variables chapter however for the purposes finding good density model irrelevant because any the equivalent solutions good any other maximizing the log likelihood function for gaussian mixture model turns out more complex problem than for the case single gaussian the difficulty arises from the presence the summation over that appears inside the logarithm that the logarithm function longer acts directly the gaussian set the derivatives the log likelihood zero will longer obtain closed form solution shall see shortly one approach apply gradient based optimization techniques fletcher nocedal and wright bishop and nabney although gradient based techniques are feasible and indeed will play important role when discuss mixture density networks chapter now consider alternative approach known the algorithm which has broad applicability and which will lay the foundations for discussion variational inference techniques chapter"}, "170": {"Section": "9.2.2", "Title": "EM for Gaussian mixtures", "Content": "elegant and powerful method for finding maximum likelihood solutions for models with latent variables called the expectation maximization algorithm algorithm dempster mclachlan and krishnan later shall give general treatment and shall also show how can generalized obtain the variational inference framework initially shall motivate the algorithm giving relatively informal treatment the context the gaussian mixture model emphasize however that has broad applicability and indeed will encountered the context variety different models this book let begin writing down the conditions that must satisfied maximum the likelihood function setting the derivatives with respect the means the gaussian components zero obtain znk where have made use the form for the gaussian distribution note that the posterior probabilities responsibilities given appear naturally the right hand side multiplying which assume nonsingular and rearranging obtain where have defined znk znk section mixture models and can interpret the effective number points assigned cluster note carefully the form this solution see that the mean for the kth gaussian component obtained taking weighted mean all the points the data set which the weighting factor for data point given the posterior probability znk that component was responsible for generating set the derivative with respect zero and follow similar line reasoning making use the result for the maximum likelihood solution for the covariance matrix single gaussian obtain znk which has the same form the corresponding result for single gaussian fitted the data set but again with each data point weighted the corresponding posterior probability and with the denominator given the effective number points associated with the corresponding component finally maximize with respect the mixing coefficients here must take account the constraint which requires the mixing coefficients sum one this can achieved using lagrange multiplier and maximizing the following quantity section appendix which gives where again see the appearance the responsibilities now multiply both sides and sum over making use the constraint find using this eliminate and rearranging obtain that the mixing coefficient for the kth component given the average responsibility which that component takes for explaining the data points worth emphasizing that the results and not constitute closed form solution for the parameters the mixture model because the responsibilities znk depend those parameters complex way through however these results suggest simple iterative scheme for finding solution the maximum likelihood problem which shall see turns out instance the algorithm for the particular case the gaussian mixture model first choose some initial values for the means covariances and mixing coefficients then alternate between the following two updates that shall call the step mixtures gaussians figure illustration the algorithm using the old faithful set used for the illustration the means algorithm figure see the text for details section and the step for reasons that will become apparent shortly the expectation step step use the current values for the parameters evaluate the posterior probabilities responsibilities given then use these probabilities the maximization step step estimate the means covariances and mixing coefficients using the results and note that doing first evaluate the new means using and then use these new values find the covariances using keeping with the corresponding result for single gaussian distribution shall show that each update the parameters resulting from step followed step guaranteed increase the log likelihood function practice the algorithm deemed have converged when the change the log likelihood function alternatively the parameters falls below some threshold illustrate the algorithm for mixture two gaussians applied the rescaled old faithful data set figure here mixture two gaussians used with centres initialized using the same values for the means algorithm figure and with precision matrices initialized proportional the unit matrix plot shows the data points green together with the initial configuration the mixture model which the one standard deviation contours for the two mixture models and gaussian components are shown blue and red circles plot shows the result the initial step which each data point depicted using proportion blue ink equal the posterior probability having been generated from the blue component and corresponding proportion red ink given the posterior probability having been generated the red component thus points that have significant probability for belonging either cluster appear purple the situation after the first step shown plot which the mean the blue gaussian has moved the mean the data set weighted the probabilities each data point belonging the blue cluster other words has moved the centre mass the blue ink similarly the covariance the blue gaussian set equal the covariance the blue ink analogous results hold for the red component plots and show the results after and complete cycles respectively plot the algorithm close convergence note that the algorithm takes many more iterations reach approximate convergence compared with the means algorithm and that each cycle requires significantly more computation therefore common run the means algorithm order find suitable initialization for gaussian mixture model that subsequently adapted using the covariance matrices can conveniently initialized the sample covariances the clusters found the means algorithm and the mixing coefficients can set the fractions data points assigned the respective clusters with gradient based approaches for maximizing the log likelihood techniques must employed avoid singularities the likelihood function which gaussian component collapses onto particular data point should emphasized that there will generally multiple local maxima the log likelihood function and that not guaranteed find the largest these maxima because the algorithm for gaussian mixtures plays such important role summarize below for gaussian mixtures given gaussian mixture model the goal maximize the likelihood function with respect the parameters comprising the means and covariances the components and the mixing coefficients initialize the means covariances and mixing coefficients and evaluate the initial value the log likelihood step evaluate the responsibilities using the current parameter values znk znk"}, "171": {"Section": "9.3", "Title": "An Alternative View of EM", "Content": "step estimate the parameters using the current responsibilities new new new where znk new new evaluate the log likelihood znk and check for convergence either the parameters the log likelihood the convergence criterion not satisfied return step alternative view this section present complementary view the algorithm that recognizes the key role played latent variables discuss this approach first all abstract setting and then for illustration consider once again the case gaussian mixtures the goal the algorithm find maximum likelihood solutions for models having latent variables denote the set all observed data which the and similarly denote the set all latent variables nth row represents with corresponding row the set all model parameters denoted and the log likelihood function given note that our discussion will apply equally well continuous latent variables simply replacing the sum over with integral key observation that the summation over the latent variables appears inside the logarithm even the joint distribution belongs the exponential mixture models and family the marginal distribution typically does not result this summation the presence the sum prevents the logarithm from acting directly the joint distribution resulting complicated expressions for the maximum likelihood solution now suppose that for each observation were told the corresponding value the latent variable shall call the complete data set and shall refer the actual observed data incomplete illustrated figure the likelihood function for the complete data set simply takes the form and shall suppose that maximization this complete data log likelihood function straightforward practice however are not given the complete data set but only the incomplete data our state knowledge the values the latent variables given only the posterior distribution because cannot use the complete data log likelihood consider instead its expected value under the posterior distribution the latent variable which corresponds shall see the step the algorithm the subsequent step maximize this expectation the current estimate for the parameters denoted old then pair successive and steps gives rise revised estimate new the algorithm initialized choosing some starting value for the parameters the use the expectation may seem somewhat arbitrary however shall see the motivation for this choice when give deeper treatment section the step use the current parameter values old find the posterior distribution the latent variables given old then use this posterior distribution find the expectation the complete data log likelihood evaluated for some general parameter value this expectation denoted old given old old the step determine the revised parameter estimate new maximizing this function new arg max old section note that the definition old the logarithm acts directly the joint distribution and the corresponding step maximization will supposition tractable the general algorithm summarized below has the property shall show later that each cycle will increase the incomplete data log likelihood unless already local maximum the general algorithm given joint distribution over observed variables and latent variables governed parameters the goal maximize the likelihood function with respect choose initial setting for the parameters old alternative view step evaluate old step evaluate new given new arg max old where old old exercise check for convergence either the log likelihood the parameter values the convergence criterion not satisfied then let old new and return step the algorithm can also used find map maximum posterior solutions for models which prior defined over the parameters this case the step remains the same the maximum likelihood case whereas the step the quantity maximized given old suitable choices for the prior will remove the singularities the kind illustrated figure here have considered the use the algorithm maximize likelihood function when there are discrete latent variables however can also applied when the unobserved variables correspond missing values the data set the distribution the observed values obtained taking the joint distribution all the variables and then marginalizing over the missing ones can then used maximize the corresponding likelihood function shall show example the application this technique the context principal component analysis figure this will valid procedure the data values are missing random meaning that the mechanism causing values missing does not depend the unobserved values many situations this will not the case for instance sensor fails return value whenever the quantity measuring exceeds some threshold"}, "172": {"Section": "9.3.1", "Title": "Gaussian mixtures revisited", "Content": "now consider the application this latent variable view the specific case gaussian mixture model recall that our goal maximize the log likelihood function which computed using the observed data set and saw that this was more difficult than for the case single gaussian distribution due the presence the summation over that occurs inside the logarithm suppose then that addition the observed data set were also given the values the corresponding discrete variables recall that figure shows complete data set one that includes labels showing which component generated each data point while figure shows the corresponding incomplete data set the graphical model for the complete data shown figure now consider the problem maximizing the likelihood for the complete data set from and this likelihood function takes the form znk znk where znk denotes the kth component taking the logarithm obtain znk lnn comparison with the log likelihood function for the incomplete data shows that the summation over and the logarithm have been interchanged the logarithm now acts directly the gaussian distribution which itself member the exponential family not surprisingly this leads much simpler solution the maximum likelihood problem now show consider first the maximization with respect the means and covariances because dimensional vector with all elements equal except for single element having the value the complete data log likelihood function simply sum independent contributions one for each mixture component thus the maximization with respect mean covariance exactly for single gaussian except that involves only the subset data points that are assigned that component for the maximization with respect the mixing coefficients note that these are coupled for different values virtue the summation constraint again this can enforced using lagrange multiplier before and leads the result znk that the mixing coefficients are equal the fractions data points assigned the corresponding components thus see that the complete data log likelihood function can maximized trivially closed form practice however not have values for the latent variables discussed earlier consider the expectation with respect the posterior distribution the latent variables the complete data log likelihood mixture models and figure this shows the same graph figure except that now suppose that the discrete variables are observed well the data variables znk znj alternative view using and together with bayes theorem see that this posterior distribution takes the form znk and hence factorizes over that under the posterior distribution the are independent this easily verified inspection the directed graph figure and making use the separation criterion the expected value the indicator variable znk under this posterior distribution then given znk znk znk znj znk which just the responsibility component for data point the expected value the complete data log likelihood function therefore given znk lnn can now proceed follows first choose some initial values for the parameters old old and old and use these evaluate the responsibilities the step then keep the responsibilities fixed and maximize with respect and the step this leads closed form solutions for new new and new given and before this precisely the algorithm for gaussian mixtures derived earlier shall gain more insight into the role the expected complete data log likelihood function when give proof convergence the algorithm section"}, "173": {"Section": "9.3.2", "Title": "Relation to K-means", "Content": "comparison the means algorithm with the algorithm for gaussian mixtures shows that there close similarity whereas the means algorithm performs hard assignment data points clusters which each data point associated uniquely with one cluster the algorithm makes soft assignment based the posterior probabilities fact can derive the means algorithm particular limit for gaussian mixtures follows consider gaussian mixture model which the covariance matrices the mixture components are given where variance parameter that shared exercise section exercise mixture models and all the components and the identity matrix that exp now consider the algorithm for mixture gaussians this form which treat fixed constant instead parameter estimated from the posterior probabilities responsibilities for particular data point are given znk exp exp consider the limit see that the denominator the term for which smallest will zero most slowly and hence the responsibilities znk for the data point all zero except for term for which the responsibility znj will unity note that this holds independently the values the long none the zero thus this limit obtain hard assignment data points clusters just the means algorithm that znk rnk where rnk defined each data point thereby assigned the cluster having the closest mean the estimation equation for the given then reduces the means result note that the estimation formula for the mixing coefficients simply sets the value equal the fraction data points assigned cluster although these parameters longer play active role the algorithm finally the limit the expected complete data log likelihood given becomes rnk const thus see that this limit maximizing the expected complete data log likelihood equivalent minimizing the distortion measure for the means algorithm given note that the means algorithm does not estimate the covariances the clusters but only the cluster means hard assignment version the gaussian mixture model with general covariance matrices known the elliptical means algorithm has been considered sung and poggio"}, "174": {"Section": "9.3.3", "Title": "Mixtures of Bernoulli distributions", "Content": "far this chapter have focussed distributions over continuous variables described mixtures gaussians further example mixture modelling and illustrate the algorithm different context now discuss mixtures discrete binary variables described bernoulli distributions this model also known latent class analysis lazarsfeld and henry mclachlan and peel well being practical importance its own right our discussion bernoulli mixtures will also lay the foundation for consideration hidden markov models over discrete variables exercise section alternative view consider set binary variables where each which governed bernoulli distribution with parameter that where and see that the individual variables are independent given the mean and covariance this distribution are easily seen now let consider finite mixture these distributions given cov diag where and exercise the mean and covariance this mixture distribution are given cov where diag because the covariance matrix cov longer diagonal the mixture distribution can capture correlations between the variables unlike single bernoulli distribution are given data set then the log likelihood function for this model given again see the appearance the summation inside the logarithm that the maximum likelihood solution longer has closed form now derive the algorithm for maximizing the likelihood function for the mixture bernoulli distributions this first introduce explicit latent znk znk znj mixture models and variable associated with each instance the case the gaussian mixture binary dimensional variable having single component equal with all other components equal can then write the conditional distribution given the latent variable while the prior distribution for the latent variables the same for the mixture gaussians model that exercise form the product and and then marginalize over then recover order derive the algorithm first write down the complete data log likelihood function which given xni xni where and next take the expectation the complete data log likelihood with respect the posterior distribution the latent variables give znk xni xni where znk znk the posterior probability responsibility component given data point the step these responsibilities are evaluated using bayes theorem which takes the form znk znk znk znk znj alternative view consider the sum over see that the responsibilities enter only through two terms which can written znk znk where the effective number data points associated with component the step maximize the expected complete data log likelihood with respect the parameters and set the derivative with respect equal zero and rearrange the terms obtain see that this sets the mean component equal weighted mean the data with weighting coefficients given the responsibilities that component takes for data points for the maximization with respect need introduce following analogous lagrange multiplier enforce the constraint steps those used for the mixture gaussians then obtain which represents the intuitively reasonable result that the mixing coefficient for component given the effective fraction points the data set explained that component note that contrast the mixture gaussians there are singularities which the likelihood function goes infinity this can seen noting that the likelihood function bounded above because there exist singularities which the likelihood function goes zero but these will not found provided not initialized pathological starting point because the algorithm always increases the value the likelihood function until local maximum found illustrate the bernoulli mixture model figure using model handwritten digits here the digit images have been turned into binary vectors setting all elements whose values exceed and setting the remaining elements now fit data set such digits comprising the digits and with mixture bernoulli distributions running iterations the algorithm the mixing coefficients were initialized and the parameters were set random values chosen uniformly the range and then normalized satisfy the constraint that see that mixture bernoulli distributions able find the three clusters the data set corresponding the different digits the conjugate prior for the parameters bernoulli distribution given the beta distribution and have seen that beta prior equivalent introducing exercise exercise exercise section mixture models and figure illustration the bernoulli mixture model which the top row shows examples from the digits data set after converting the pixel values from grey scale binary using threshold the bottom row the first three images show the parameters for each the three components the mixture model comparison also fit the same data set using single multivariate bernoulli distribution again using maximum likelihood this amounts simply averaging the counts each pixel and shown the right most image the bottom row section exercise exercise additional effective observations can similarly introduce priors into the bernoulli mixture model and use maximize the posterior probability distributions straightforward extend the analysis bernoulli mixtures the case multinomial binary variables having states making use the discrete distribution again can introduce dirichlet priors over the model parameters desired"}, "175": {"Section": "9.3.4", "Title": "EM for Bayesian linear regression", "Content": "third example the application return the evidence approximation for bayesian linear regression section obtained the reestimation equations for the hyperparameters and evaluation the evidence and then setting the derivatives the resulting expression zero now turn alternative approach for finding and based the algorithm recall that our goal maximize the evidence function given with respect and because the parameter vector marginalized out can regard latent variable and hence can optimize this marginal likelihood function using the step compute the posterior distribution given the current setting the parameters and and then use this find the expected complete data log likelihood the step maximize this quantity with respect and have already derived the posterior distribution because this given the complete data log likelihood function then given alternative view where the likelihood and the prior are given and respectively and given taking the expectation with respect the posterior distribution then gives wtw setting the derivatives with respect zero obtain the step estimation equation wtw analogous result holds for note that this estimation equation takes slightly different form from the corresponding result derived direct evaluation the evidence function however they each involve computation and inversion eigen decomposition matrix and hence will have comparable computational cost per iteration these two approaches determining should course converge the same result assuming they find the same local maximum the evidence function this can verified first noting that the quantity defined exercise exercise stationary point the evidence function the estimation equation will self consistently satisfied and hence can substitute for give and solving for obtain which precisely the estimation equation final example consider closely related model namely the relevance vector machine for regression discussed section there used direct maximization the marginal likelihood derive estimation equations for the hyperparameters and here consider alternative approach which view the weight vector latent variable and apply the algorithm the step involves finding the posterior distribution over the weights and this given the step maximize the expected complete data log likelihood which defined exercise where the expectation taken with respect the posterior distribution computed using the old parameter values compute the new parameter values maximize with respect and give the expectation maximization algorithm algorithm general technique for finding maximum likelihood solutions for probabilistic models having latent variables dempster mclachlan and krishnan here give very general treatment the algorithm and the process provide proof that the algorithm derived heuristically sections and for gaussian mixtures does indeed maximize the likelihood function csisz and tusn ady hathaway neal and hinton our discussion will also form the basis for the derivation the variational inference framework consider probabilistic model which collectively denote all the observed variables and all the hidden variables the joint distribution governed set parameters denoted our goal maximize the likelihood function that given here are assuming discrete although the discussion identical comprises continuous variables combination discrete and continuous variables with summation replaced integration appropriate shall suppose that direct optimization difficult but that optimization the complete data likelihood function significantly easier next introduce distribution defined over the latent variables and observe that for any choice the following decomposition holds where have defined note that functional see appendix for discussion functionals the distribution and function the parameters worth studying mixture models and new new these estimation equations are formally equivalent those obtained direct maxmization"}, "176": {"Section": "9.4", "Title": "The EM Algorithm in General", "Content": "exercise section the algorithm general figure illustration the decomposition given which holds for any choice distribution because the kullback leibler divergence satisfies see that the quantity lower bound the log likelihood function exercise section carefully the forms the expressions and and particular noting that they differ sign and also that contains the joint distribution and while contains the conditional distribution given verify the decomposition first make use the product rule probability give which then substitute into the expression for this gives rise two terms one which cancels while the other gives the required log likelihood after noting that normalized distribution that sums from see that the kullback leibler divergence between and the posterior distribution recall that the kullback leibler divergence satisfies with equality and only therefore follows from that other words that lower bound the decomposition illustrated figure the algorithm two stage iterative optimization technique for finding maximum likelihood solutions can use the decomposition define the algorithm and demonstrate that does indeed maximize the log likelihood suppose that the current value the parameter vector old the step the lower bound old maximized with respect while holding old fixed the solution this maximization problem easily seen noting that the value old does not depend and the largest value old will occur when the kullback leibler divergence vanishes other words when equal the posterior distribution old this case the lower bound will equal the log likelihood illustrated figure the subsequent step the distribution held fixed and the lower bound maximized with respect give some new value new this will cause the lower bound increase unless already maximum which will necessarily cause the corresponding log likelihood function increase because the distribution determined using the old parameter values rather than the new values and held fixed during the step will not equal the new posterior distribution new and hence there will nonzero divergence the increase the log likelihood function therefore greater than the increase the lower bound mixture models and figure illustration the step the algorithm the distribution set equal the posterior distribution for the current parameter values old causing the lower bound move the same value the log likelihood function with the divergence vanishing old old shown figure substitute old into see that after the step the lower bound takes the form old old const old old where the constant simply the negative entropy the distribution and therefore independent thus the step the quantity that being maximized the expectation the complete data log likelihood saw earlier the case mixtures gaussians note that the variable over which are optimizing appears only inside the logarithm the joint distribution comprises member the exponential family product such members then see that the logarithm will cancel the exponential and lead step that will typically much simpler than the maximization the corresponding incomplete data log likelihood function the operation the algorithm can also viewed the space parameters illustrated schematically figure here the red curve depicts the infigure illustration the step the algorithm the distribution held fixed and the lower bound maximized with respect the parameter vector give revised value new because the divergence nonnegative this causes the log likelihood increase least much the lower bound does new new figure the algorithm involves alternately computing lower bound the log likelihood for the current parameter values and then maximizing this bound obtain the new parameter values see the text for full discussion the algorithm general old new exercise complete data log likelihood function whose value wish maximize start with some initial parameter value old and the first step evaluate the posterior distribution over latent variables which gives rise lower bound old whose value equals the log likelihood old shown the blue curve note that the bound makes tangential contact with the log likelihood old that both curves have the same gradient this bound convex function having unique maximum for mixture components from the exponential family the step the bound maximized giving the value new which gives larger value log likelihood than old the subsequent step then constructs bound that tangential new shown the green curve for the particular case independent identically distributed data set will comprise data points while will comprise corresponding latent variables where from the independence assumption have and marginalizing over the have using the sum and product rules see that the posterior probability that evaluated the step takes the form the case and the posterior distribution also factorizes with respect the gaussian mixture model this simply says that the responsibility that each the mixture components takes for particular data point depends only the value and the parameters the mixture components not the values the other data points have seen that both the and the steps the algorithm are increasing the value well defined bound the log likelihood function and that the mixture models and complete cycle will change the model parameters such way cause the log likelihood increase unless already maximum which case the parameters remain unchanged can also use the algorithm maximize the posterior distribution for models which have introduced prior over the parameters see this note that function have and making use the decomposition have where constant can again optimize the right hand side alternately with respect and the optimization with respect gives rise the same estep equations for the standard algorithm because only appears the step equations are modified through the introduction the prior term which typically requires only small modification the standard maximum likelihood step equations the algorithm breaks down the potentially difficult problem maximizing the likelihood function into two stages the step and the step each which will often prove simpler implement nevertheless for complex models may the case that either the step the step indeed both remain intractable this leads two possible extensions the algorithm follows the generalized gem algorithm addresses the problem intractable step instead aiming maximize with respect seeks instead change the parameters such way increase its value again because lower bound the log likelihood function each complete cycle the gem algorithm guaranteed increase the value the log likelihood unless the parameters already correspond local maximum one way exploit the gem approach would use one the nonlinear optimization strategies such the conjugate gradients algorithm during the step another form gem algorithm known the expectation conditional maximization ecm algorithm involves making several constrained optimizations within each step meng and rubin for instance the parameters might partitioned into groups and the step broken down into multiple steps each which involves optimizing one the subset with the remainder held fixed can similarly generalize the step the algorithm performing partial rather than complete optimization with respect neal and hinton have seen for any given value there unique maximum with respect that corresponds the posterior distribution and that for this choice the bound equal the log likelihood function follows that any algorithm that converges the global maximum will find value that also global maximum the log likelihood provided continuous function exercises then continuity any local maximum will also local maximum consider the case independent data points with corresponding latent variables the joint distribution factorizes over the data points and this structure can exploited incremental form which each cycle only one data point processed time the step instead recomputing the responsibilities for all the data points just evaluate the responsibilities for one data point might appear that the subsequent step would require computation involving the responsibilities for all the data points however the mixture components are members the exponential family then the responsibilities enter only through simple sufficient statistics and these can updated efficiently consider for instance the case gaussian mixture and suppose perform update for data point which the corresponding old and new values the responsibilities are denoted old zmk and new zmk the step the required sufficient statistics can updated incrementally for instance for the means the sufficient statistics are defined and from which obtain exercise exercises new old new zmk old zmk new old together with new old new zmk old zmk the corresponding results for the covariances and the mixing coefficients are analogous thus both the step and the step take fixed time that independent the total number data points because the parameters are revised after each data point rather than waiting until after the whole data set processed this incremental version can converge faster than the batch version each step this incremental algorithm increasing the value and have shown above the algorithm converges local global maximum this will correspond local global maximum the log likelihood function"}, "177": {"Section": "10", "Title": "Approximate Inference", "Content": "central task the application probabilistic models the evaluation the posterior distribution the latent variables given the observed visible data variables and the evaluation expectations computed with respect this distribution the model might also contain some deterministic parameters which will leave implicit for the moment may fully bayesian model which any unknown parameters are given prior distributions and are absorbed into the set latent variables denoted the vector for instance the algorithm need evaluate the expectation the complete data log likelihood with respect the posterior distribution the latent variables for many models practical interest will infeasible evaluate the posterior distribution indeed compute expectations with respect this distribution this could because the dimensionality the latent space too high work with directly because the posterior distribution has highly complex form for which expectations are not analytically tractable the case continuous variables the required integrations may not have closed form approximate inference analytical solutions while the dimensionality the space and the complexity the integrand may prohibit numerical integration for discrete variables the marginalizations involve summing over all possible configurations the hidden variables and though this always possible principle often find practice that there may exponentially many hidden states that exact calculation prohibitively expensive such situations need resort approximation schemes and these fall broadly into two classes according whether they rely stochastic deterministic approximations stochastic techniques such markov chain monte carlo described chapter have enabled the widespread use bayesian methods across many domains they generally have the property that given infinite computational resource they can generate exact results and the approximation arises from the use finite amount processor time practice sampling methods can computationally demanding often limiting their use small scale problems also can difficult know whether sampling scheme generating independent samples from the required distribution this chapter introduce range deterministic approximation schemes some which scale well large applications these are based analytical approximations the posterior distribution for example assuming that factorizes particular way that has specific parametric form such gaussian such they can never generate exact results and their strengths and weaknesses are complementary those sampling methods section discussed the laplace approximation which based local gaussian approximation mode maximum the distribution here turn family approximation techniques called variational inference variational bayes which use more global criteria and which have been widely applied conclude with brief introduction alternative variational framework known expectation propagation"}, "178": {"Section": "10.1", "Title": "Variational Inference", "Content": "variational methods have their origins the century with the work euler lagrange and others the calculus variations standard calculus concerned with finding derivatives functions can think function mapping that takes the value variable the input and returns the value the function the output the derivative the function then describes how the output value varies make infinitesimal changes the input value similarly can define functional mapping that takes function the input and that returns the value the functional the output example would the entropy which takes probability distribution the input and returns the quantity variational inference the output can the introduce the concept functional derivative which expresses how the value the functional changes response infinitesimal changes the input function feynman the rules for the calculus variations mirror those standard calculus and are discussed appendix many problems can expressed terms optimization problem which the quantity being optimized functional the solution obtained exploring all possible input functions find the one that maximizes minimizes the functional variational methods have broad applicability and include such areas finite element methods kapur and maximum entropy schwarz although there nothing intrinsically approximate about variational methods they naturally lend themselves finding approximate solutions this done restricting the range functions over which the optimization performed for instance considering only quadratic functions considering functions composed linear combination fixed basis functions which only the coefficients the linear combination can vary the case applications probabilistic inference the restriction may for example take the form factorization assumptions jordan jaakkola now let consider more detail how the concept variational optimization can applied the inference problem suppose have fully bayesian model which all parameters are given prior distributions the model may also have latent variables well parameters and shall denote the set all latent variables and parameters similarly denote the set all observed variables for example might have set independent identically distributed data for which and our probabilistic model specifies the joint distribution and our goal find approximation for the posterior distribution well for the model evidence our discussion can decompose the log marginal probability using where have defined this differs from our discussion only that the parameter vector longer appears because the parameters are now stochastic variables and are absorbed into since this chapter will mainly interested continuous variables have used integrations rather than summations formulating this decomposition however the analysis goes through unchanged some all the variables are discrete simply replacing the integrations with summations required before can maximize the lower bound optimization with respect the distribution which equivalent minimizing the divergence allow any possible choice for then the maximum the lower bound occurs when the divergence vanishes which occurs when equals the posterior distribution approximate inference figure illustration the variational approximation for the example considered earlier figure the left hand plot shows the original distribution yellow along with the laplace red and variational green approximations and the right hand plot shows the negative logarithms the corresponding curves however shall suppose the model such that working with the true posterior distribution intractable therefore consider instead restricted family distributions and then seek the member this family for which the divergence minimized our goal restrict the family sufficiently that they comprise only tractable distributions while the same time allowing the family sufficiently rich and flexible that can provide good approximation the true posterior distribution important emphasize that the restriction imposed purely achieve tractability and that subject this requirement should use rich family approximating distributions possible particular there over fitting associated with highly flexible distributions using more flexible approximations simply allows approach the true posterior distribution more closely one way restrict the family approximating distributions use parametric distribution governed set parameters the lower bound then becomes function and can exploit standard nonlinear optimization techniques determine the optimal values for the parameters example this approach which the variational distribution gaussian and have optimized with respect its mean and variance shown figure"}, "179": {"Section": "10.1.1", "Title": "Factorized distributions", "Content": "here consider alternative way which restrict the family distributions suppose partition the elements into disjoint groups that denote where then assume that the distribution factorizes with respect these groups that variational inference should emphasized that are making further assumptions about the distribution particular place restriction the functional forms the individual factors this factorized form variational inference corresponds approximation framework developed physics called mean field theory parisi amongst all distributions having the form now seek that distribution for which the lower bound largest therefore wish make free form variational optimization with respect all the distributions which optimizing with respect each the factors turn achieve this first substitute into and then dissect out the dependence one the factors denoting simply keep the notation uncluttered then obtain dzi dzj const dzj dzj where have defined new distribution dzj const the relation const here the notation denotes expectation with respect the distributions over all variables for that dzi now suppose keep the fixed and maximize with respect all possible forms for the distribution this easily done recognizing that negative kullback leibler divergence between and thus maximizing equivalent minimizing the kullback leibler leonhard euler euler was swiss mathematician and physicist who worked petersburg and berlin and who widely considered one the greatest mathematicians all time certainly the most prolific and his collected works fill volumes amongst his many contributions formulated the modern theory the function developed together with lagrange the calculus variations and discovered the formula which relates four the most important numbers mathematics during the last years his life was almost totally blind and yet produced nearly half his results during this period approximate inference divergence and the minimum occurs when general expression for the optimal solution given const thus obtain worth taking few moments study the form this solution provides the basis for applications variational methods says that the log the optimal solution for factor obtained simply considering the log the joint distribution over all hidden and visible variables and then taking the expectation with respect all the other factors for thus take the exponential both sides and normalize have the additive constant set normalizing the distribution exp exp dzj practice shall find more convenient work with the form and then reinstate the normalization constant where required inspection this will become clear from subsequent examples the set equations given for represent set consistency conditions for the maximum the lower bound subject the factorization constraint however they not represent explicit solution because the expresj depends expectations sion the right hand side for the optimum computed with respect the other factors for will therefore seek consistent solution first initializing all the factors appropriately and then cycling through the factors and replacing each turn with revised estimate given the right hand side evaluated using the current estimates for all the other factors convergence guaranteed because bound convex with respect each the factors boyd and vandenberghe"}, "180": {"Section": "10.1.2", "Title": "Properties of factorized approximations", "Content": "our approach variational inference based factorized approximation the true posterior distribution let consider for moment the problem approximating general distribution factorized distribution begin with discuss the problem approximating gaussian distribution using factorized gaussian which will provide useful insight into the types inaccuracy introduced using factorized approximations consider gaussian distribution over two correlated variables which the mean and precision have elements and due the symmetry the precision matrix now suppose wish approximate this distribution using factorized gaussian the form first apply the general result find expression for the doing useful note that the right hand side optimal factor only need retain those terms that have some functional dependence because all other terms can absorbed into the normalization constant thus have const variational inference const const next observe that the right hand side this expression quadratic function and can identify gaussian distribution worth emphasizing that did not assume that gaussian but rather derived this result variational optimization the divergence over all possible distributions note also that not need consider the additive constant explicitly because represents the normalization constant that can found the end inspection required using the technique completing the square can identify the mean and precision this gaussian giving section where symmetry also gaussian and can written which note that these solutions are coupled that depends expectations computed with respect and vice versa general address this treating the variational solutions estimation equations and cycling through the variables turn updating them until some convergence criterion satisfied shall see example this shortly here however note that the problem sufficiently simple that closed form solution can found particular because and see that the two equations are satisfied take and and easily shown that this the only solution provided the distribution nonsingular this result illustrated figure see that the mean correctly captured but that the variance controlled the direction smallest variance and that the variance along the orthogonal direction significantly under estimated general result that factorized variational approximation tends give approximations the posterior distribution that are too compact way comparison suppose instead that had been minimizing the reverse kullback leibler divergence shall see this form divergence exercise approximate inference figure comparison the two alternative forms for the kullback leibler divergence the green contours corresponding and standard deviations for correlated gaussian distribution over two variables and and the red contours represent the corresponding levels for approximating over the same variables given the product two independent univariate gaussian distributions whose parameters are obtained minimization the kullbackleibler divergence and the reverse kullback leibler divergence distribution section exercise used alternative approximate inference framework called expectation propagation therefore consider the general problem minimizing when factorized approximation the form the divergence can then written the form const where the constant term simply the entropy and does not depend can now optimize with respect each the factors which easily done using lagrange multiplier give dzi this case find that the optimal solution for just given the corresponding marginal distribution note that this closed form solution and does not require iteration apply this result the illustrative example gaussian distribution over vector can use which gives the result shown figure see that once again the mean the approximation correct but that places significant probability mass regions variable space that have very low probability the difference between these two results can understood noting that there large positive contribution the kullback leibler divergence variational inference figure another comparison the two alternative forms for the kullback leibler divergence the blue contours show bimodal distribution given mixture two gaussians and the red contours correspond the single gaussian distribution that best approximates the sense minimizing the kullbackleibler divergence but now the red contours correspond gaussian distribution found numerical minimization the kullback leibler divergence but showing different local minimum the kullback leibler divergence from regions space which near zero unless also close zero thus minimizing this form divergence leads distributions that avoid regions which small conversely the kullback leibler divergence minimized distributions that are nonzero regions where nonzero can gain further insight into the different behaviour the two divergences consider approximating multimodal distribution unimodal one illustrated figure practical applications the true posterior distribution will often multimodal with most the posterior mass concentrated some number relatively small regions parameter space these multiple modes may arise through nonidentifiability the latent space through complex nonlinear dependence the parameters both types multimodality were encountered chapter the context gaussian mixtures where they manifested themselves multiple maxima the likelihood function and variational treatment based the minimization will tend find one these modes contrast were minimize the resulting approximations would average across all the modes and the context the mixture model would lead poor predictive distributions because the average two good parameter values typically itself not good parameter value possible make use define useful inference procedure but this requires rather different approach the one discussed here and will considered detail when discuss expectation propagation the two forms kullback leibler divergence are members the alpha family section approximate inference divergences ali and silvey amari minka defined where continuous parameter the kullback leibler divergence corresponds the limit whereas corresponds the limit for all values have with equality and only suppose fixed distribution and minimize with respect some set distributions then for the divergence zero forcing that any values for which will have and typically will under estimate the support and will tend seek the mode with the largest mass conversely for the divergence zero avoiding that values for which will have and typically will stretch cover all and will over estimate the support when obtain symmetric divergence that linearly related the hellinger distance given the square root the hellinger distance valid distance metric"}, "181": {"Section": "10.1.3", "Title": "Example: The univariate Gaussian", "Content": "now illustrate the factorized variational approximation using gaussian distribution over single variable mackay our goal infer the posterior distribution for the mean and precision given data set observed values which are assumed drawn independently from the gaussian the likelihood function given exp now introduce conjugate prior distributions for and given gam where gam the gamma distribution defined together these distributions constitute gaussian gamma conjugate prior distribution for this simple problem the posterior distribution can found exactly and again takes the form gaussian gamma distribution however for tutorial purposes will consider factorized variational approximation the posterior distribution given exercise section exercise variational inference note that the true posterior distribution does not factorize this way the optimum factors and can obtained from the general result follows for have const const exercise completing the square over see that gaussian mean and precision given with note that for this gives the maximum likelihood result which and the precision infinite similarly the optimal solution for the factor given const const and hence gamma distribution gam with parameters exercise section again this exhibits the expected behaviour when should emphasized that did not assume these specific functional forms for the optimal distributions and they arose naturally from the structure the likelihood function and the corresponding conjugate priors thus have expressions for the optimal distributions and each which depends moments evaluated with respect the other distribution one approach finding solution therefore make initial guess for say the moment and use this compute the distribution given this revised distribution can then extract the required moments and and use these recompute the distribution and since the space hidden variables for this example only two dimensional can illustrate the variational approximation the posterior distribution plotting contours both the true posterior and the factorized approximation illustrated figure approximate inference figure illustration variational inference for the mean and precision univariate gaussian distribution contours the true posterior distribution are shown green contours the initial factorized approximation are shown blue after estimating the factor after estimating the factor contours the optimal factorized approximation which the iterative scheme converges are shown red general will need use iterative approach such this order solve for the optimal factorized posterior distribution for the very simple example are considering here however can find explicit solution solving the simultaneous equations for the optimal factors and before doing this can simplify these expressions considering broad noninformative priors which although these parameter settings correspond improper priors see that the posterior distribution still well defined using the standard result for the mean gamma distribution together with and have appendix then using and obtain the first and second order moments exercise section exercise exercise variational inference the form can now substitute these moments into and then solve for give recognize the right hand side the familiar unbiased estimator for the variance univariate gaussian distribution and see that the use bayesian approach has avoided the bias the maximum likelihood solution"}, "182": {"Section": "10.1.4", "Title": "Model comparison", "Content": "well performing inference over the hidden variables may also wish compare set candidate models labelled the index and having prior probabilities our goal then approximate the posterior probabilities where the observed data this slightly more complex situation than that considered far because different models may have different structure and indeed different dimensionality for the hidden variables cannot therefore simply consider factorized approximation but must instead recognize that the posterior over must conditioned and must consider can readily verify the following decomposition based this variational distribution where the lower bound and given here are assuming discrete but the same analysis applies continuous latent variables provided the summations are replaced with integrations can maximize with respect the distribution using lagrange multiplier with the result however maximize with respect the find that the solutions for different are coupled expect because they are conditioned proceed instead first optimizing each the individually optimization exp approximate inference section and then subsequently determining the using after normalization the resulting values for can used for model selection model averaging the usual way illustration variational mixture gaussians now return our discussion the gaussian mixture model and apply the variational inference machinery developed the previous section this will provide good illustration the application variational methods and will also demonstrate how bayesian treatment elegantly resolves many the difficulties associated with the maximum likelihood approach attias the reader encouraged work through this example detail provides many insights into the practical application variational methods many bayesian models corresponding much more sophisticated distributions can solved straightforward extensions and generalizations this analysis our starting point the likelihood function for the gaussian mixture model illustrated the graphical model figure for each observation have corresponding latent variable comprising binary vector with elements znk for before denote the observed data set and similarly denote the latent variables from can write down the conditional distribution given the mixing coefficients the form znk similarly from can write down the conditional distribution the observed data vectors given the latent variables and the component parameters znk where and note that are working terms precision matrices rather than covariance matrices this somewhat simplifies the mathematics next introduce priors over the parameters and the analysis considerably simplified use conjugate prior distributions therefore choose dirichlet distribution over the mixing coefficients dir where symmetry have chosen the same parameter for each the components and the normalization constant for the dirichlet distribution defined"}, "183": {"Section": "10.2", "Title": "Illustration: Variational Mixture of Gaussians", "Content": "figure directed acyclic graph representing the bayesian mixture gaussians model which the box plate denotes set observations here denotes and denotes section section have seen the parameter can interpreted the effective prior number observations associated with each component the mixture the value small then the posterior distribution will influenced primarily the data rather than the prior similarly introduce independent gaussian wishart prior governing the mean and precision each gaussian component given because this represents the conjugate prior distribution when both the mean and precision are unknown typically would choose symmetry the resulting model can represented directed graph shown figure note that there link from since the variance the distribution over function this example provides nice illustration the distinction between latent variables and parameters variables such that appear inside the plate are regarded latent variables because the number such variables grows with the size the data set contrast variables such that are outside the plate are fixed number independently the size the data set and are regarded parameters from the perspective graphical models however there really fundamental difference between them"}, "184": {"Section": "10.2.1", "Title": "Variational distribution", "Content": "order formulate variational treatment this model next write down the joint distribution all the random variables which given which the various factors are defined above the reader should take moment verify that this decomposition does indeed correspond the probabilistic graphical model shown figure note that only the variables are observed approximate inference now consider variational distribution which factorizes between the latent variables and the parameters that remarkable that this the only assumption that need make order obtain tractable practical solution our bayesian mixture model particular the functional form the factors and will determined automatically optimization the variational distribution note that are omitting the subscripts the distributions much with the distributions and are relying the arguments distinguish the different distributions the corresponding sequential update equations for these factors can easily derived making use the general result let consider the derivation the update equation for the factor the log the optimized factor given const now make use the decomposition note that are only interested the functional dependence the right hand side the variable thus any terms that not depend can absorbed into the additive normalization constant giving const substituting for the two conditional distributions the right hand side and again absorbing any terms that are independent into the additive constant have where have defined znk const where the dimensionality the data variable taking the exponential both sides obtain znk exercise requiring that this distribution normalized and noting that for each value the quantities znk are binary and sum over all values obtain rznk rnk illustration variational mixture gaussians where rnk see that the optimal solution for the factor takes the same functional form the prior note that because given the exponential real quantity the quantities rnk will nonnegative and will sum one required for the discrete distribution have the standard result znk rnk from which see that the quantities rnk are playing the role responsibilities note that the optimal solution for depends moments evaluated with respect the distributions other variables and again the variational update equations are coupled and must solved iteratively this point shall find convenient define three statistics the observed data set evaluated with respect the responsibilities given rnkxn rnk note that these are analogous quantities evaluated the maximum likelihood algorithm for the gaussian mixture model now let consider the factor the variational posterior distribution again using the general result have znk lnn const observe that the right hand side this expression decomposes into sum terms involving only together with terms only involving and which implies that the variational posterior factorizes give furthermore the terms involving and themselves comprise sum over terms involving and leading the further factorization approximate inference identifying the terms the right hand side that depend have rnk const where have used taking the exponential both sides recognize dirichlet distribution dir where has components given finally the variational posterior distribution does not factorize into the product the marginals but can always use the product rule write the form the two factors can found inspecting and reading off those terms that involve and the result expected gaussian wishart distribution and given where have defined nkxk nksk these update equations are analogous the step equations the algorithm for the maximum likelihood solution the mixture gaussians see that the computations that must performed order update the variational posterior distribution over the model parameters involve evaluation the same sums over the data set arose the maximum likelihood treatment order perform this variational step need the expectations znk rnk representing the responsibilities these are obtained normalizing the that are given see that this expression involves expectations with respect the variational distributions the parameters and these are easily evaluated give twk exercise exercise appendix section section exercise illustration variational mixture gaussians where have introduced definitions defined with the standard properties the wishart and dirichlet distributions and the digamma function the results and follow from and substitute and into and make use obtain the following result for the responsibilities rnk exp twk notice the similarity the corresponding result for the responsibilities maximum likelihood which from can written the form rnk exp where have used the precision place the covariance highlight the similarity thus the optimization the variational posterior distribution involves cycling between two stages analogous the and steps the maximum likelihood algorithm the variational equivalent the step use the current distributions over the model parameters evaluate the moments and and hence evaluate znk rnk then the subsequent variational equivalent the step keep these responsibilities fixed and use them compute the variational distribution over the parameters using and each case see that the variational posterior distribution has the same functional form the corresponding factor the joint distribution this general result and consequence the choice conjugate distributions figure shows the results applying this approach the rescaled old faithful data set for gaussian mixture model having components see that after convergence there are only two components for which the expected values the mixing coefficients are numerically distinguishable from their prior values this effect can understood qualitatively terms the automatic trade off bayesian model between fitting the data and the complexity the model which the complexity penalty arises from components whose parameters are pushed away from their prior values components that take essentially responsibility for explaining the data points have rnk and hence from see that and from see that the other parameters revert their prior values principle such components are fitted slightly the data points but for broad priors this effect too small seen numerically for the variational gaussian mixture model the expected values the mixing coefficients the posterior distribution are given consider component for which and the prior broad that then and the component plays role the model whereas approximate inference figure variational bayesian mixture gaussians applied the old faithful data set which the ellipses denote the one standard deviation density contours for each the components and the density red ink inside each ellipse corresponds the mean value the mixing coefficient for each component the number the top left each diagram shows the number iterations variational inference components whose expected mixing coefficient are numerically indistinguishable from zero are not plotted the prior tightly constrains the mixing coefficients that then figure the prior over the mixing coefficients dirichlet the form recall from figure that for the prior favours solutions which some the mixing coefficients are zero figure was obtained using and resulted two components having nonzero mixing coefficients instead choose obtain three components with nonzero mixing coefficients and for all six components have nonzero mixing coefficients have seen there close similarity between the variational solution for the bayesian mixture gaussians and the algorithm for maximum likelihood fact consider the limit then the bayesian treatment converges the maximum likelihood algorithm for anything other than very small data sets the dominant computational cost the variational algorithm for gaussian mixtures arises from the evaluation the responsibilities together with the evaluation and inversion the weighted data covariance matrices these computations mirror precisely those that arise the maximum likelihood algorithm and there little computational overhead using this bayesian approach compared the traditional maximum likelihood one there are however some substantial advantages first all the singularities that arise maximum likelihood when gaussian component collapses onto specific data point are absent the bayesian treatment section exercise illustration variational mixture gaussians indeed these singularities are removed simply introduce prior and then use map estimate instead maximum likelihood furthermore there over fitting choose large number components the mixture saw figure finally the variational treatment opens the possibility determining the optimal number components the mixture without resorting techniques such cross validation"}, "185": {"Section": "10.2.2", "Title": "Variational lower bound", "Content": "can also straightforwardly evaluate the lower bound for this model practice useful able monitor the bound during the estimation order test for convergence can also provide valuable check both the mathematical expressions for the solutions and their software implementation because each step the iterative estimation procedure the value this bound should not decrease can take this stage further provide deeper test the correctness both the mathematical derivation the update equations and their software implementation using finite differences check that each update does indeed give constrained maximum the bound svens and bishop for the variational mixture gaussians the lower bound given where keep the notation uncluttered have omitted the superscript the distributions along with the subscripts the expectation operators because each expectation taken with respect all the random variables its argument the various terms the bound are easily evaluated give the following results ktr skwk twk rnk approximate inference twk ktr rnk rnk where the dimensionality the entropy the wishart distribution given and the coefficients and are defined and respectively note that the terms involving expectations the logs the distributions simply represent the negative entropies those distributions some simplifications and combination terms can performed when these expressions are summed give the lower bound however have kept the expressions separate for ease understanding finally worth noting that the lower bound provides alternative approach for deriving the variational estimation equations obtained section this use the fact that since the model has conjugate priors the functional form the factors the variational posterior distribution known namely discrete for dirichlet for and gaussian wishart for taking general parametric forms for these distributions can derive the form the lower bound function the parameters the distributions maximizing the bound with respect these parameters then gives the required estimation equations"}, "186": {"Section": "10.2.3", "Title": "Predictive density", "Content": "applications the bayesian mixture gaussians model will often the observed variable asz and the preinterested the predictive density for new value sociated with this observation will corresponding latent variable dictive density then given exercise illustration variational mixture gaussians where the unknown true posterior distribution the parameters using and can first perform the summation over give because the remaining integrations are intractable approximate the predictive density replacing the true posterior distribution with its variational approximation give exercise where have made use the factorization and each term have implicitly integrated out all variables for the remaining integrations can now evaluated analytically giving mixture student distributions kst which the kth component has mean and the precision given exercise which given when the size the data set large the predictive distribution reduces mixture gaussians section exercise"}, "187": {"Section": "10.2.4", "Title": "Determining the number of components", "Content": "have seen that the variational lower bound can used determine posterior distribution over the number components the mixture model there however one subtlety that needs addressed for any given setting the parameters gaussian mixture model except for specific degenerate settings there will exist other parameter settings for which the density over the observed variables will identical these parameter values differ only through labelling the components for instance consider mixture two gaussians and single observed variable which the parameters have the values then the parameter values which the two components have been exchanged will symmetry give rise the same value have mixture model comprising components then each parameter setting will member family equivalent settings the context maximum likelihood this redundancy irrelevant because the parameter optimization algorithm for example will depending the initialization the parameters find one specific solution and the other equivalent solutions play role bayesian setting however marginalize over all possible approximate inference figure plot the variational lower bound versus the number components the gaussian mixture model for the old faithful data showing distinct peak components for each value the model trained from different random starts and the results shown symbols plotted with small random horizontal perturbations that they can distinguished note that some solutions find suboptimal local maxima but that this happens infrequently parameter values have seen figure that the true posterior distribution multimodal variational inference based the minimization will tend approximate the distribution the neighbourhood one the modes and ignore the others again because equivalent modes have equivalent predictive densities this concern provided are considering model having specific number components however wish compare different values then need take account this multimodality simple approximate solution add term onto the lower bound when used for model comparison and averaging figure shows plot the lower bound including the multimodality factor versus the number components for the old faithful data set worth emphasizing once again that maximum likelihood would lead values the likelihood function that increase monotonically with assuming the singular solutions have been avoided and discounting the effects local maxima and cannot used determine appropriate model complexity contrast bayesian inference automatically makes the trade off between model complexity and fitting the data this approach the determination requires that range models having different values trained and compared alternative approach determining suitable value for treat the mixing coefficients parameters and make point estimates their values maximizing the lower bound corduneanu and bishop with respect instead maintaining probability distribution over them the fully bayesian approach this leads the estimation equation exercise section exercise rnk and this maximization interleaved with the variational updates for the distribution over the remaining parameters components that provide insufficient contribution section illustration variational mixture gaussians explaining the data will have their mixing coefficients driven zero during the optimization and they are effectively removed from the model through automatic relevance determination this allows make single training run which start with relatively large initial value and allow surplus components pruned out the model the origins the sparsity when optimizing with respect hyperparameters discussed detail the context the relevance vector machine"}, "188": {"Section": "10.2.5", "Title": "Induced factorizations", "Content": "deriving these variational update equations for the gaussian mixture model assumed particular factorization the variational posterior distribution given however the optimal solutions for the various factors exhibit additional factorizations particular the solution for given the product independent distribution over each the components the mixture whereas the variational posterior distribution over the latent variables given factorizes into independent distribution for each observation note that does not further factorize with respect because for each value the znk are constrained sum one over these additional factorizations are consequence the interaction between the assumed factorization and the conditional independence properties the true distribution characterized the directed graph figure shall refer these additional factorizations induced factorizations because they arise from interaction between the factorization assumed the variational posterior distribution and the conditional independence properties the true joint distribution numerical implementation the variational approach important take account such additional factorizations for instance would very inefficient maintain full precision matrix for the gaussian distribution over set variables the optimal form for that distribution always had diagonal precision matrix corresponding factorization with respect the individual variables described that gaussian such induced factorizations can easily detected using simple graphical test based separation follows partition the latent variables into three disjoint groups and then let suppose that are assuming factorization between and the remaining latent variables that using the general result together with the product rule for probabilities see that the optimal solution for given const const now ask whether this resulting solution will factorize between and other words whether this will happen and only that the conditional independence relation approximate inference satisfied can test see this relation does hold for any choice and making use the separation criterion illustrate this consider again the bayesian mixture gaussians represented the directed graph figure which are assuming variational factorization given can see immediately that the variational posterior distribution over the parameters must factorize between and the remaining parameters and because all paths connecting either must pass through one the nodes all which are the conditioning set for our conditional independence test and all which are head tail with respect such paths"}, "189": {"Section": "10.3", "Title": "Variational Linear Regression", "Content": "second illustration variational inference return the bayesian linear regression model section the evidence framework approximated the integration over and making point estimates obtained maximizing the log marginal likelihood fully bayesian approach would integrate over the hyperparameters well over the parameters although exact integration intractable can use variational methods find tractable approximation order simplify the discussion shall suppose that the noise precision parameter known and fixed its true value although the framework easily extended include the distribution over for the linear regression model the variational treatment will turn out equivalent the evidence framework nevertheless provides good exercise the use variational methods and will also lay the foundation for variational treatment bayesian logistic regression section recall that the likelihood function for and the prior over are given exercise where now introduce prior distribution over from our discussion section know that the conjugate prior for the precision gaussian given gamma distribution and choose where gam defined thus the joint distribution all the variables given gam this can represented directed graphical model shown figure"}, "190": {"Section": "10.3.1", "Title": "Variational distribution", "Content": "our first goal find approximation the posterior distribution this employ the variational framework section with variational variational linear regression figure probabilistic graphical model representing the joint disregression the bayesian linear for tribution model posterior distribution given the factorized expression can find estimation equations for the factors this distribution making use the general result recall that for each factor take the log the joint distribution over all variables and then average with respect those variables not that factor consider first the distribution over keeping only terms that have functional dependence have const wtw const recognize this the log gamma distribution and identifying the coefficients and obtain where gam wtw similarly can find the variational estimation equation for the posterior distribution over again using the general result and keeping only those terms that have functional dependence have const wtw const const because this quadratic form the distribution gaussian and can complete the square the usual way identify the mean and covariance giving approximate inference where note the close similarity the posterior distribution obtained when was treated fixed parameter the difference that here replaced its expectation under the variational distribution indeed have chosen use the same notation for the covariance matrix both cases using the standard results and can obtain the required moments follows wwt the evaluation the variational posterior distribution begins initializing the parameters one the distributions and then alternately estimates these factors turn until suitable convergence criterion satisfied usually specified terms the lower bound discussed shortly instructive relate the variational solution that found using the evidence framework section this consider the case corresponding the limit infinitely broad prior over the mean the variational posterior distribution then given wtw comparison with shows that the case this particularly simple model the variational approach gives precisely the same expression that obtained maximizing the evidence function using except that the point estimate for replaced its expected value because the distribution depends only through the expectation see that the two approaches will give identical results for the case infinitely broad prior"}, "191": {"Section": "10.3.2", "Title": "Predictive distribution", "Content": "the predictive distribution over given new input easily evaluated for this model using the gaussian variational posterior for the parameters variational linear regression where have evaluated the integral making use the result for the linear gaussian model here the input dependent variance given tsn note that this takes the same form the result obtained with fixed except that now the expected value appears the definition"}, "192": {"Section": "10.3.3", "Title": "Lower bound", "Content": "another quantity importance the lower bound defined exercise evaluation the various terms straightforward making use results obtained previous chapters and gives ttt figure shows plot the lower bound versus the degree polynomial model for synthetic data set generated from degree three polynomial here the prior parameters have been set corresponding the noninformative prior which uniform over discussed section saw section the quantity represents lower bound the log marginal likelihood for the model assign equal prior probabilities the different values then can interpret approximation the posterior model probability thus the variational framework assigns the highest probability the model with this should contrasted with the maximum likelihood result which assigns ever smaller residual error models increasing complexity until the residual error driven zero causing maximum likelihood favour severely over fitted models approximate inference figure plot the lower bound versus the order the polynomial for polynomial model which set data points generated from polynomial with sampled over the interval with additive gaussian noise variance the value the bound gives the log probability the model and see that the value the bound peaks corresponding the true model from which the data set was generated"}, "193": {"Section": "10.4", "Title": "Exponential Family Distributions", "Content": "chapter discussed the important role played the exponential family distributions and their conjugate priors for many the models discussed this book the complete data likelihood drawn from the exponential family however general this will not the case for the marginal likelihood function for the observed data for example mixture gaussians the joint distribution observations and corresponding hidden variables member the exponential family whereas the marginal distribution mixture gaussians and hence not now have grouped the variables the model into observed variables and hidden variables now make further distinction between latent variables denoted and parameters denoted where parameters are intensive fixed number independent the size the data set whereas latent variables are extensive scale number with the size the data set for example gaussian mixture model the indicator variables zkn which specify which component responsible for generating data point represent the latent variables whereas the means precisions and mixing proportions represent the parameters consider the case independent identically distributed data denote the data values where with corresponding latent variables now suppose that the joint distribution observed and latent variables member the exponential family parameterized natural parameters that exp shall also use conjugate prior for which can written exp recall that the conjugate prior distribution can interpreted prior number observations all having the value for the vector now consider variational exponential family distributions distribution that factorizes between the latent variables and the parameters that using the general result can solve for the two factors follows const const section thus see that this decomposes into sum independent terms one for each value and hence the solution for will factorize over that this example induced factorization taking the exponential both sides have exp where the normalization coefficient has been instated comparison with the standard form for the exponential family similarly for the variational distribution over the parameters have const tezn const again taking the exponential both sides and instating the normalization coefficient inspection have exp where have defined ezn note that the solutions for and are coupled and solve them iteratively two stage procedure the variational step evaluate the expected sufficient statistics using the current posterior distribution over the latent variables and use this compute revised posterior distribution over the parameters then the subsequent variational step use this revised parameter posterior distribution find the expected natural parameters which gives rise revised variational distribution over the latent variables"}, "194": {"Section": "10.4.1", "Title": "Variational message passing", "Content": "have illustrated the application variational methods considering specific model the bayesian mixture gaussians some detail this model can approximate inference described the directed graph shown figure here consider more generally the use variational methods for models described directed graphs and derive number widely applicable results the joint distribution corresponding directed graph can written using the decomposition pai where denotes the variable associated with node and pai denotes the parent set corresponding node note that may latent variable may belong the set observed variables now consider variational approximation which the distribution assumed factorize with respect the that note that for observed nodes there factor the variational distribution now substitute into our general result give pai const any terms the right hand side that not depend can absorbed into fact the only terms that depend are the conthe additive constant ditional distribution for given paj together with any other conditional distributions that have the conditioning set definition these conditional distributions correspond the children node and they therefore also depend the parents the child nodes the other parents the child nodes besides node itself see that the set all nodes which depends corresponds the markov blanket node illustrated figure thus the update the factors the variational posterior distribution represents local calculation the graph this makes possible the construction general purpose software for variational inference which the form the model does not need specified advance bishop now specialize the case model which all the conditional distributions have conjugate exponential structure then the variational update procedure can cast terms local message passing algorithm winn and bishop particular the distribution associated with particular node can updated once that node has received messages from all its parents and all its children this turn requires that the children have already received messages from their coparents the evaluation the lower bound can also simplified because many the required quantities are already evaluated part the message passing scheme this distributed message passing formulation has good scaling properties and well suited large networks"}, "195": {"Section": "10.5", "Title": "Local Variational Methods", "Content": "local variational methods section the variational framework discussed sections and can considered global method the sense that directly seeks approximation the full posterior distribution over all random variables alternative local approach involves finding bounds functions over individual variables groups variables within model for instance might seek bound conditional distribution which itself just one factor much larger probabilistic model specified directed graph the purpose introducing the bound course simplify the resulting distribution this local approximation can applied multiple variables turn until tractable approximation obtained and section shall give practical example this approach the context logistic regression here focus developing the bounds themselves have already seen our discussion the kullback leibler divergence that the convexity the logarithm function played key role developing the lower bound the global variational approach have defined strictly convex function one for which every chord lies above the function convexity also plays central role the local variational framework note that our discussion will apply equally concave functions with min and max interchanged and with lower bounds replaced upper bounds let begin considering simple example namely the function exp which convex function and which shown the left hand plot figure our goal approximate simpler function particular linear function from figure see that this linear function will lower bound corresponds tangent can obtain the tangent line specific value say making first order taylor expansion that with equality when for our example function figure the left hand figure the red curve shows the function exp and the blue line shows the tangent defined with this line has slope exp note that any other tangent line for example the ones shown green will have smaller value the right hand figure shows the corresponding plot the function where given versus for which the maximum corresponds exp approximate inference figure the left hand plot the red curve shows convex function and the blue line represents the linear function which lower bound because for all for the given value slope the contact point the tangent line having the same slope found minimizing with respect the discrepancy shown the green dashed lines given this defines the dual function which corresponds the negative the intercept the tangent line having slope exp therefore obtain the tangent line the form exp exp which linear function parameterized for consistency with subsequent discussion let define exp that different values correspond different tangent lines and because all such lines are lower bounds the function have thus can write the function the form max have succeeded approximating the convex function simpler linear function the price have paid that have introduced variational parameter and obtain the tightest bound must optimize with respect can formulate this approach more generally using the framework convex duality rockafellar jordan consider the illustration convex function shown the left hand plot figure this example the function lower bound but not the best lower bound that can achieved linear function having slope because the tightest bound given the tangent line let write the equation the tangent line having slope where the negative intercept clearly depends the slope the tangent determine the intercept note that the line must moved vertically amount equal the smallest vertical distance between the line and the function shown figure thus min max local variational methods now instead fixing and varying can consider particular and then adjust until the tangent plane tangent that particular because the value the tangent line particular maximized when that value coincides with its contact point have max see that the functions and play dual role and are related through and let apply these duality relations our simple example exp from see that the maximizing value given and back substituting obtain the conjugate function the form obtained previously the function shown for the right hand plot figure check can substitute into which gives the maximizing value exp and back substituting then recovers the original function exp for concave functions can follow similar argument obtain upper bounds which max replaced with min that min min the function interest not convex concave then cannot directly apply the method above obtain bound however can first seek invertible transformations either the function its argument which change into convex form then calculate the conjugate function and then transform back the original variables important example which arises frequently pattern recognition the logistic sigmoid function defined exercise stands this function neither convex nor concave however take the logarithm obtain function which concave easily verified finding the second derivative from the corresponding conjugate function then takes the form min appendix which recognize the binary entropy function for variable whose probability having the value using then obtain upper bound the log sigmoid approximate inference figure the left hand plot shows the logistic sigmoid function defined red together with two examples the exponential upper bound shown blue the right hand plot shows the logistic sigmoid again red together with the gaussian lower bound shown blue here the parameter and the bound exact and denoted the dashed green lines exercise and taking the exponential obtain upper bound the logistic sigmoid itself the form exp which plotted for two values the left hand plot figure can also obtain lower bound the sigmoid having the functional form gaussian this follow jaakkola and jordan and make transformations both the input variable and the function itself first take the log the logistic function and then decompose that now note that the function convex function the variable can again verified finding the second derivative this leads lower bound which linear function whose conjugate function given max the stationarity condition leads tanh denote this value corresponding the contact point the tangent line for this particular value then have tanh section section local variational methods instead thinking the variational parameter can let play this role this leads simpler expressions for the conjugate function which then given hence the bound can written the bound the sigmoid then becomes exp where defined this bound illustrated the right hand plot figure see that the bound has the form the exponential quadratic function which will prove useful when seek gaussian representations posterior distributions defined through logistic sigmoid functions the logistic sigmoid arises frequently probabilistic models over binary variables because the function that transforms log odds ratio into posterior probability the corresponding transformation for multiclass distribution given the softmax function unfortunately the lower bound derived here for the logistic sigmoid does not directly extend the softmax gibbs proposes method for constructing gaussian distribution that conjectured bound although rigorous proof given which may used apply local variational methods multiclass problems shall see example the use local variational bounds sections for the moment however instructive consider general terms how these bounds can used suppose wish evaluate integral the form where the logistic sigmoid and gaussian probability density such integrals arise bayesian models when for instance wish evaluate the predictive distribution which case represents posterior parameter distribution because the integral intractable employ the variational bound which write the form where variational parameter the integral now becomes the product two exponential quadratic functions and can integrated analytically give bound now have the freedom choose the variational parameter which finding the value that maximizes the function the resulting value represents the tightest bound within this family bounds and can used approximation this optimized bound however will general not exact approximate inference although the bound the logistic sigmoid can optimized exactly the required choice for depends the value that the bound exact for one value only because the quantity obtained integrating over all values the value represents compromise weighted the distribution"}, "196": {"Section": "10.6", "Title": "Variational Logistic Regression", "Content": "now illustrate the use local variational methods returning the bayesian logistic regression model studied section there focussed the use the laplace approximation while here consider variational treatment based the approach jaakkola and jordan like the laplace method this also leads gaussian approximation the posterior distribution however the greater flexibility the variational approximation leads improved accuracy compared the laplace method furthermore unlike the laplace method the variational approach optimizing well defined objective function given rigourous bound the model evidence logistic regression has also been treated dybowski and roberts from bayesian perspective using monte carlo sampling techniques"}, "197": {"Section": "10.6.1", "Title": "Variational posterior distribution", "Content": "here shall make use variational approximation based the local bounds introduced section this allows the likelihood function for logistic regression which governed the logistic sigmoid approximated the exponential quadratic form therefore again convenient choose conjugate gaussian prior the form for the moment shall treat the hyperparameters and fixed constants section shall demonstrate how the variational formalism can extended the case where there are unknown hyperparameters whose values are inferred from the data the variational framework seek maximize lower bound the marginal likelihood for the bayesian logistic regression model the marginal likelihood takes the form first note that the conditional distribution for can written eat eat where order obtain lower bound make use the variational lower bound the logistic sigmoid function given which variational logistic regression reproduce here for convenience exp where can therefore write eat eat exp note that because this bound applied each the terms the likelihood function separately there variational parameter corresponding each training set observation using and multiplying the prior distribution obtain the following bound the joint distribution and where denotes the set variational parameters and exp ntn evaluation the exact posterior distribution would require normalization the lefthand side this inequality because this intractable work instead with the right hand side note that the function the right hand side cannot interpreted probability density because not normalized once normalized give variational posterior distribution however longer represents bound because the logarithm function monotonically increasing the inequality implies this gives lower bound the log the joint distribution and the form ntn substituting for the prior the right hand side this inequality becomes function const approximate inference exercise this quadratic function and can obtain the corresponding variational approximation the posterior distribution identifying the linear and quadratic terms giving gaussian variational posterior the form where with the laplace framework have again obtained gaussian approximation the posterior distribution however the additional flexibility provided the variational parameters leads improved accuracy the approximation jaakkola and jordan here have considered batch learning context which all the training data available once however bayesian methods are intrinsically well suited sequential learning which the data points are processed one time and then discarded the formulation this variational approach for the sequential case straightforward note that the bound given applies only the two class problem and this approach does not directly generalize classification problems with classes alternative bound for the multiclass case has been explored gibbs"}, "198": {"Section": "10.6.2", "Title": "Optimizing the variational parameters", "Content": "now have normalized gaussian approximation the posterior distribution which shall use shortly evaluate the predictive distribution for new data points first however need determine the variational parameters maximizing the lower bound the marginal likelihood this substitute the inequality back into the marginal likelihood give with the optimization the hyperparameter the linear regression model section there are two approaches determining the the first approach recognize that the function defined integration over and can view latent variable and invoke the algorithm the second approach integrate over analytically and then perform direct maximization over let begin considering the approach the algorithm starts choosing some initial values for the parameters the step the algorithm which denote collectively old variational logistic regression then use these parameter values find the posterior distribution over which given the step then maximize the expected complete data log likelihood which given old where the expectation taken with respect the posterior distribution evaluated using old noting that does not depend and substituting for obtain old wwt const where const denotes terms that are independent now set the derivative with respect equal zero few lines algebra making use the definitions and then gives wwt now note that monotonic function for and that can restrict attention nonnegative values without loss generality due the symmetry the bound around thus and hence obtain the following estimation equations new wwt where have used let summarize the algorithm for finding the variational posterior distribution first initialize the variational parameters old the step evaluate the posterior distribution over given which the mean and covariance are defined and the step then use this variational posterior compute new value for given the and steps are repeated until suitable convergence criterion satisfied which practice typically requires only few iterations alternative approach obtaining estimation equations for note that the integral over the definition the lower bound the integrand has gaussian like form and the integral can evaluated analytically having evaluated the integral can then differentiate with respect turns out that this gives rise exactly the same estimation equations does the approach given have emphasized already the application variational methods useful able evaluate the lower bound given the integration over can performed analytically noting that gaussian and the exponential quadratic function thus completing the square and making use the standard result for the normalization coefficient gaussian distribution can obtain closed form solution which takes the form exercise exercise exercise approximate inference figure illustration the bayesian approach logistic regression for simple linearly separable data set the plot the left shows the predictive distribution obtained using variational inference see that the decision boundary lies roughly mid way between the clusters data points and that the contours the predictive distribution splay out away from the data reflecting the greater uncertainty the classification such regions the plot the right shows the decision boundaries corresponding five samples the parameter vector drawn from the posterior distribution this variational framework can also applied situations which the data this case maintain arriving sequentially jaakkola and jordan gaussian posterior distribution over which initialized using the prior each data point arrives the posterior updated making use the bound and then normalized give updated posterior distribution the predictive distribution obtained marginalizing over the posterior distribution and takes the same form for the laplace approximation discussed section figure shows the variational predictive distributions for synthetic data set this example provides interesting insights into the concept large margin which was discussed section and which has qualitatively similar behaviour the bayesian solution"}, "199": {"Section": "10.6.3", "Title": "Inference of hyperparameters", "Content": "far have treated the hyperparameter the prior distribution known constant now extend the bayesian logistic regression model allow the value this parameter inferred from the data set this can achieved combining the global and local variational approximations into single framework maintain lower bound the marginal likelihood each stage such combined approach was adopted bishop and svens the context bayesian treatment the hierarchical mixture experts model variational logistic regression specifically consider once again simple isotropic gaussian prior distribution the form our analysis readily extended more general gaussian priors for instance wish associate different hyperparameter with different subsets the parameters usual consider conjugate hyperprior over given gamma distribution governed the constants and the marginal likelihood for this model now takes the form gam where the joint distribution given are now faced with analytically intractable integration over and which shall tackle using both the local and global variational approaches the same model begin with introduce variational distribution and then apply the decomposition which this instance takes the form where the lower bound and the kullback leibler divergence are defined this point the lower bound still intractable due the form the likelihood factor therefore apply the local variational bound each the logistic sigmoid factors before this allows use the inequality and place lower bound which will therefore also lower bound the log marginal likelihood next assume that the variational distribution factorizes between parameters and hyperparameters that approximate inference with this factorization can appeal the general result find expressions for the optimal factors consider first the distribution discarding terms that are independent have const const now substitute for using and for using giving wtw const see that this quadratic function and the solution for will gaussian completing the square the usual way obtain where have defined similarly the optimal solution for the factor obtained from const substituting for using and for using obtain wtw const recognize this the log gamma distribution and obtain gam where wtw"}, "200": {"Section": "10.7", "Title": "Expectation Propagation", "Content": "also need optimize the variational parameters and this also done omitting terms that are independent and maximizing the lower bound integrating over have const note that this has precisely the same form and can again appeal our earlier result which can obtained direct optimization the marginal likelihood function leading estimation equations the form new appendix have obtained estimation equations for the three quantities and and after making suitable initializations can cycle through these quantities updating each turn the required moments are given wtw expectation propagation conclude this chapter discussing alternative form deterministic approximate inference known expectation propagation minka minka with the variational bayes methods discussed far this too based the minimization kullback leibler divergence but now the reverse form which gives the approximation rather different properties consider for moment the problem minimizing with respect when fixed distribution and member the exponential family and from can written the form exp function the kullback leibler divergence then becomes tep const where the constant terms are independent the natural parameters can minimize within this family distributions setting the gradient with respect zero giving however have already seen that the negative gradient given the expectation under the distribution equating these two results obtain approximate inference see that the optimum solution simply corresponds matching the expected sufficient statistics for instance gaussian then minimize the kullback leibler divergence setting the mean equal the mean the distribution and the covariance equal the covariance this sometimes called moment matching example this was seen figure now let exploit this result obtain practical algorithm for approximate inference for many probabilistic models the joint distribution data and hidden variables including parameters comprises product factors the form this would arise for example model for independent identically distributed data which there one factor for each data point along with factor corresponding the prior more generally would also apply any model defined directed probabilistic graph which each factor conditional distribution corresponding one the nodes undirected graph which each factor clique potential are interested evaluating the posterior distribution for the purpose making predictions well the model evidence for the purpose model comparison from the posterior given and the model evidence given here are considering continuous variables but the following discussion applies equally discrete variables with integrals replaced summations shall suppose that the marginalization over along with the marginalizations with respect the posterior distribution required make predictions are intractable that some form approximation required expectation propagation based approximation the posterior distribution which also given product factors the approximation corresponds one the factors which each factor the true posterior and the factor the normalizing constant needed ensure that the left hand side integrates unity order some way obtain practical algorithm need constrain the factors and particular shall assume that they come from the exponential family the product the factors will therefore also from the exponential family and can expectation propagation described finite set sufficient statistics for example each the gaussian then the overall approximation will also gaussian ideally would like determine the minimizing the kullback leibler divergence between the true posterior and the approximation given note that this the reverse form divergence compared with that used variational inference general this minimization will intractable because the divergence involves averaging with respect the true distribution rough approximation could instead minimize the divergences between the corresponding pairs and factors this represents much simpler problem solve and has the advantage that the algorithm noniterative however because each factor individually approximated the product the factors could well give poor approximation expectation propagation makes much better approximation optimizing each factor turn the context all the remaining factors starts initializing and then cycles through the factors refining them one time the factors this similar spirit the update factors the variational bayes framework first remove this considered earlier suppose wish refine factor conceptually will now determine factor from the product give revised form the factor ensuring that the product qnew close possible for this ensures that the which keep fixed all the factors approximation most accurate the regions high posterior probability defined the remaining factors shall see example this effect when apply from the the clutter problem achieve this first remove the factor current approximation the posterior defining the unnormalized distribution section note that could instead find from the product factors although practice division usually easier this now combined with the factor give distribution approximate inference figure illustration the expectation propagation approximation using gaussian distribution for the example considered earlier figures and the left hand plot shows the original distribution yellow along with the laplace red global variational green and blue approximations and the right hand plot shows the corresponding negative logarithms the distributions note that the distribution broader than that variational inference consequence the different form divergence where the normalization constant given now determine revised factor gence minimizing the kullback leibler diverkl qnew this easily solved because the approximating distribution qnew from the exponential family and can appeal the result which tells that the parameters qnew are obtained matching its expected sufficient statistics the corresponding moments shall assume that this tractable operation for example choose gaussian distribution then set equal the mean the unnormalized distribution and set its covariance more generally straightforward obtain the required expectations for any member the exponential family provided can normalized because the expected statistics can related the derivatives the normalization coefficient given the approximation illustrated figure from see that the revised factor qnew and dividing out the remaining factors that can found taking qnew where have used the coefficient determined multiplying both sides and integrating give expectation propagation where have used the fact that qnew normalized the value can therefore found matching zeroth order moments combining this with then see that and can found evaluating the integral practice several passes are made through the set factors revising each factor turn the posterior distribution then approximated using and the model evidence can approximated using with the factors replaced their approximations expectation propagation are given joint distribution over observed data and stochastic variables the form product factors and wish approximate the posterior distribution distribution the form also wish approximate the model evidence initialize all the approximating factors initialize the posterior approximation setting until convergence choose factor remove refine from the posterior division evaluate and store the new factor qnew evaluate the approximation the model evidence approximate inference evaluate the new posterior setting the sufficient statistics moments qnew equal those including evaluation the normalization constant special case known assumed density filtering adf moment matching maybeck lauritzen boyen and koller opper and winther obtained initializing all the approximating factors except the first unity and then making one pass through the factors updating each them once assumed density filtering can appropriate for line learning which data points are arriving sequence and need learn from each data point and then discard before considering the next point however batch setting have the opportunity use the data points many times order achieve improved accuracy and this idea that exploited expectation propagation furthermore apply adf batch data the results will have undesirable dependence the arbitrary order which the data points are considered which again can overcome one disadvantage expectation propagation that there guarantee that the iterations will converge however for approximations the exponential family the iterations converge the resulting solution will stationary point particular energy function minka although each iteration does not necessarily decrease the value this energy function this contrast variational bayes which iteratively maximizes lower bound the log marginal likelihood which each iteration guaranteed not decrease the bound possible optimize the cost function directly which case guaranteed converge although the resulting algorithms can slower and more complex implement another difference between variational bayes and arises from the form divergence that minimized the two algorithms because the former minimizes whereas the latter minimizes saw figure for distributions which are multimodal minimizing can lead poor approximations particular applied mixtures the results are not sensible because the approximation tries capture all the modes the posterior distribution conversely logistic type models often out performs both local variational methods and the laplace approximation kuss and rasmussen expectation propagation figure illustration the clutter problem for data space dimensionality training data points denoted the crosses are drawn from mixture two gaussians with components shown red and green the goal infer the mean the green gaussian from the observed data"}, "201": {"Section": "10.7.1", "Title": "Example: The clutter problem", "Content": "following minka illustrate the algorithm using simple example which the goal infer the mean multivariate gaussian distribution over variable given set observations drawn from that distribution make the problem more interesting the observations are embedded background clutter which itself also gaussian distributed illustrated figure the distribution observed values therefore mixture gaussians which take the form where the proportion background clutter and assumed known the prior over taken gaussian and minka chooses the parameter values and the joint distribution observations and given and the posterior distribution comprises mixture gaussians thus the computational cost solving this problem exactly would grow exponentially with the size the data set and exact solution intractable for moderately large apply the clutter problem first identify the factors and next select approximating distribution from the exponential family and for this example convenient choose spherical gaussian approximate inference the factor approximations will therefore take the form exponential quadratic functions the form snn vni where and set equal the prior note that the use does not imply that the right hand side well defined gaussian density fact shall see the variance parameter can negative but simply for can convenient shorthand notation the approximations initialized unity corresponding and where the dimensionality and hence the initial defined therefore equal the prior then iteratively refine the factors taking one factor time and applying and note that not need revise the term because update will leave this term unchanged here state the results and leave the reader fill the details first remove the current estimate from division using give which has mean and inverse variance given next evaluate the normalization constant using give similarly compute the mean and variance qnew finding the mean and variance give exercise exercise exercise where the quantity znn has simple interpretation the probability the point not being clutter then whose parameters are given use compute the refined factor vnew mnew this refinement process repeated until suitable termination criterion satisfied for instance that the maximum change parameter values resulting from complete expectation propagation figure examples the approximation specific factors for one dimensional version the clutter red and green notice that the current form for controls problem showing blue the range over which will good approximation pass through all factors less than some threshold finally use evaluate the approximation the model evidence given vnew exp where mnew tmnew nmn examples factor approximations for the clutter problem with one dimensional parameter space are shown figure note that the factor approximations can have infinite even negative values for the variance parameter this simply corresponds approximations that curve upwards instead downwards and are not necessarily problematic provided the overall approximate posterior has positive variance figure compares the performance with variational bayes mean field theory and the laplace approximation the clutter problem"}, "202": {"Section": "10.7.2", "Title": "Expectation propagation on graphs", "Content": "far our general discussion have allowed the factors the distribution functions all the components and similarly for the the approximating distribution now consider approximating factors situations which the factors depend only subsets the variables such restrictions can conveniently expressed using the framework probabilistic graphical models discussed chapter here use factor graph representation because this encompasses both directed and undirected graphs approximate inference posterior mean laplace flops laplace evidence flops figure comparison expectation propagation variational inference and the laplace approximation the clutter problem the left hand plot shows the error the predicted posterior mean versus the number floating point operations and the right hand plot shows the corresponding results for the model evidence section shall focus the case which the approximating distribution fully factorized and shall show that this case expectation propagation reduces loopy belief propagation minka start with show this the context simple example and then shall explore the general case first all recall from that minimize the kullback leibler divergence with respect factorized distribution then the optimal solution for each factor simply the corresponding marginal now consider the factor graph shown the left figure which was introduced earlier the context the sum product algorithm the joint distribution given seek approximation that has the same factorization that note that normalization constants have been omitted and these can instated the end local normalization generally done belief propagation now suppose restrict attention approximations which the factors themselves factorize with respect the individual variables that which corresponds the factor graph shown the right figure because the individual factors are factorized the overall distribution itself fully factorized now apply the algorithm using the fully factorized approximation suppose that have initialized all the factors and that choose refine factor expectation propagation figure the left simple factor graph from figure and reproduced here for convenience the right the corresponding factorized approximation distribution give first remove this factor from the approximating and then multiply this the exact factor give qnew now find qnew minimizing the kullback leibler divergence the result noted above that qnew comprises the product factors one for each variable which each factor given the corresponding marginal these four marginals are given and qnew obtained multiplying these marginals together see that the are those that involve only factors that change when update the variables namely and obtain the refined factor simply divide qnew which gives approximate inference section these are precisely the messages obtained using belief propagation which messages from variable nodes factor nodes have been folded into the messages from corresponds the message factor nodes variable nodes particular sent factor node variable node and given simifa correlarly substitute into obtain which sponds and corresponds giving the message which corresponds this result differs slightly from standard belief propagation that messages are passed both directions the same time can easily modify the procedure give the standard form the sum product algorithm updating just one the unchanged factors time for instance refine only again given definition while the refined version are refining only one term time then can choose the order which the refinements are done wish particular for tree structured graph can follow two pass update scheme corresponding the standard belief propagation schedule which will result exact inference the variable and factor marginals the initialization the approximation factors this case unimportant then now let consider general factor graph corresponding the distribution where represents the subset variables associated with factor approximate this using fully factorized distribution the form fik where corresponds individual variable node suppose that wish refine fjl keeping all other terms fixed first remove the term the particular term from give fik fjl and then multiply the exact factor determine the refined term need only consider the functional dependence and simply find the corresponding marginal multiplicative constant this involves taking the marginal multiplied any terms from that are functions any the variables terms that for will cancel between numerator and correspond other factors denominator when subsequently divide therefore obtain fjl fkm exercises recognize this the sum product rule the form which messages from variable nodes factor nodes have been eliminated illustrated the example shown fjm corresponds the message figure the quantity which factor node sends variable node and the product over over all factors that depend the variables that have variables other than variable common with factor other words compute the outgoing message from factor node take the product all the incoming messages from other factor nodes multiply the local factor and then marginalize thus the sum product algorithm arises special case expectation propagation use approximating distribution that fully factorized this suggests that more flexible approximating distributions corresponding partially disconnected graphs could used achieve higher accuracy another generalization group factors together into sets and refine all the factors set together each iteration both these approaches can lead improvements accuracy minka general the problem choosing the best combination grouping and disconnection open research issue have seen that variational message passing and expectation propagation optimize two different forms the kullback leibler divergence minka has shown that broad range message passing algorithms can derived from common framework involving minimization members the alpha family divergences given these include variational message passing loopy belief propagation and expectation propagation well range other algorithms which not have space discuss here such tree reweighted message passing wainwright fractional belief propagation wiegerinck and heskes and power minka"}, "203": {"Section": "11", "Title": "Sampling Methods", "Content": "for most probabilistic models practical interest exact inference intractable and have resort some form approximation chapter discussed inference algorithms based deterministic approximations which include methods such variational bayes and expectation propagation here consider approximate inference methods based numerical sampling also known monte carlo techniques although for some applications the posterior distribution over unobserved variables will direct interest itself for most situations the posterior distribution required primarily for the purpose evaluating expectations for example order make predictions the fundamental problem that therefore wish address this chapter involves finding the expectation some function with respect probability distribution here the components might comprise discrete continuous variables some combination the two thus the case continuous sampling methods figure schematic illustration function whose expectation evaluated with respect distribution variables wish evaluate the expectation where the integral replaced summation the case discrete variables this illustrated schematically for single continuous variable figure shall suppose that such expectations are too complex evaluated exactly using analytical techniques the general idea behind sampling methods obtain set samples where drawn independently from the distribution this allows the expectation approximated finite sum exercise long the samples are drawn from the distribution then and the estimator has the correct mean the variance the estimator given var the variance the function under the distribution worth emphasizing that the accuracy the estimator therefore does not depend the dimensionality and that principle high accuracy may achievable with relatively small number samples practice ten twenty independent samples may suffice estimate expectation sufficient accuracy the problem however that the samples might not independent and the effective sample size might much smaller than the apparent sample size also referring back figure note that small regions where large and vice versa then the expectation may dominated regions small probability implying that relatively large sample sizes will required achieve sufficient accuracy for many models the joint distribution conveniently specified terms graphical model the case directed graph with observed variables sampling methods straightforward sample from the joint distribution assuming that possible sample from the conditional distributions each node using the following ancestral sampling approach discussed briefly section the joint distribution specified pai where are the set variables associated with node and pai denotes the set variables associated with the parents node obtain sample from the joint distribution make one pass through the set variables the order sampling from the conditional distributions pai this always possible because each step all the parent values will have been instantiated after one pass through the graph will have obtained sample from the joint distribution now consider the case directed graph which some the nodes are instantiated with observed values can principle extend the above procedure least the case nodes representing discrete variables give the following logic sampling approach henrion which can seen special case importance sampling discussed section each step when sampled value obtained for variable whose value observed the sampled value compared the observed value and they agree then the sample value retained and the algorithm proceeds the next variable turn however the sampled value and the observed value disagree then the whole sample far discarded and the algorithm starts again with the first node the graph this algorithm samples correctly from the posterior distribution because corresponds simply drawing samples from the joint distribution hidden variables and data variables and then discarding those samples that disagree with the observed data with the slight saving not continuing with the sampling from the joint distribution soon one contradictory value observed however the overall probability accepting sample from the posterior decreases rapidly the number observed variables increases and the number states that those variables can take increases and this approach rarely used practice the case probability distributions defined undirected graph there one pass sampling strategy that will sample even from the prior distribution with observed variables instead computationally more expensive techniques must employed such gibbs sampling which discussed section well sampling from conditional distributions may also require samples from marginal distribution already have strategy for sampling from joint distribution then straightforward obtain samples from the marginal distribution simply ignoring the values for each sample there are numerous texts dealing with monte carlo methods those particular interest from the statistical inference perspective include chen gamerman gilks liu neal and robert and casella also there are review articles besag brooks diaconis and saloff coste jerrum and sinclair neal tierney and andrieu that provide additional information sampling"}, "204": {"Section": "11.1", "Title": "Basic Sampling Algorithms", "Content": "this section consider some simple strategies for generating random samples from given distribution because the samples will generated computer algorithm they will fact pseudo random numbers that they will deterministically calculated but must nevertheless pass appropriate tests for randomness generating such numbers raises several subtleties press that lie outside the scope this book here shall assume that algorithm has been provided that generates pseudo random numbers distributed uniformly over and indeed most software environments have such facility built"}, "205": {"Section": "11.1.1", "Title": "Standard distributions", "Content": "first consider how generate random numbers from simple nonuniform distributions assuming that already have available source uniformly distributed random numbers suppose that uniformly distributed over the interval and that transform the values using some function that the distribution will governed where this case our goal choose the function such that the resulting values have some specific desired distribution integrating obtain sampling methods methods for statistical inference diagnostic tests for convergence markov chain monte carlo algorithms are summarized robert and casella and some practical guidance the use sampling methods the context machine learning given bishop and nabney exercise which the indefinite integral thus and have transform the uniformly distributed random numbers using function which the inverse the indefinite integral the desired distribution this illustrated figure consider for example the exponential distribution exp where this case the lower limit the integral and exp thus transform our uniformly distributed variable using then will have exponential distribution basic sampling algorithms figure geometrical interpretation the transformation method for generating nonuniformly distributed random numbers the indefinite integral the desired distribution uniformly distributed random variable transformed using then will distributed according another example distribution which the transformation method can applied given the cauchy distribution exercise this case the inverse the indefinite integral can expressed terms the tan function the generalization multiple variables straightforward and involves the jacobian the change variables that final example the transformation method consider the box muller method for generating samples from gaussian distribution first suppose generate pairs uniformly distributed random numbers which can transforming variable distributed uniformly over using this leads uniform next discard each pair unless satisfies distribution points inside the unit circle with illustrated figure then for each pair evaluate the quantities figure the box muller method for generating gaussian distributed random numbers starts generating samples from uniform distribution inside the unit circle sampling methods exercise exercise where then the joint distribution and given exp exp and and are independent and each has gaussian distribution with zero mean and unit variance has gaussian distribution with zero mean and unit variance then will have gaussian distribution with mean and variance generate vectorvalued variables having multivariate gaussian distribution with mean and covariance can make use the cholesky decomposition which takes the form llt press then vector valued random variable whose components are independent and gaussian distributed with zero mean and unit variance then will have mean and covariance obviously the transformation technique depends for its success the ability calculate and then invert the indefinite integral the required distribution such operations will only feasible for limited number simple distributions and must turn alternative approaches search more general strategy here consider two techniques called rejection sampling and importance sampling although mainly limited univariate distributions and thus not directly applicable complex problems many dimensions they form important components more general strategies"}, "206": {"Section": "11.1.2", "Title": "Rejection sampling", "Content": "the rejection sampling framework allows sample from relatively complex distributions subject certain constraints begin considering univariate distributions and discuss the extension multiple dimensions subsequently suppose wish sample from distribution that not one the simple standard distributions considered far and that sampling directly from difficult furthermore suppose often the case that are easily able evaluate for any given value some normalizing constant that where can readily evaluated but unknown order apply rejection sampling need some simpler distribution sometimes called proposal distribution from which can readily draw samples basic sampling algorithms figure the rejection sampling method samples are drawn from simple distribution and rejected they fall the grey area between the unnormalized distribution and the scaled distribution the resulting samples are distributed according which the normalized version exercise next introduce constant whose value chosen such that for all values the function called the comparison function and illustrated for univariate distribution figure each step the rejection sampler involves generating two random numbers first generate number from the distribution next generate number from the uniform distribution over this pair random numbers has uniform distribution under the curve then the sample rejected otherwise the function finally retained thus the pair rejected lies the grey shaded region figp ure the remaining pairs then have uniform distribution under the curve and hence the corresponding values are distributed according desired the original values are generated from the distribution and these samp and the probability that ples are then accepted with probability sample will accepted given accept thus the fraction points that are rejected this method depends the ratio the area under the curve the area under the unnormalized distribution therefore see that the constant should small possible subject the limitation that must nowhere less than illustration the use rejection sampling consider the task sampling from the gamma distribution gam baza exp which for has bell shaped form shown figure suitable proposal distribution therefore the cauchy because this too bell shaped and because can use the transformation method discussed earlier sample from need generalize the cauchy slightly ensure that nowhere has smaller value than the gamma distribution this can achieved transforming uniform random variable using tan which gives random numbers distributed according exercise the minimum reject rate obtained setting and choosing the constant small possible while still satisfying the requirement the resulting comparison function also illustrated figure"}, "207": {"Section": "11.1.3", "Title": "Adaptive rejection sampling", "Content": "many instances where might wish apply rejection sampling proves difficult determine suitable analytic form for the envelope distribution alternative approach construct the envelope function the fly based measured values the distribution gilks and wild construction envelope function particularly straightforward for cases which log concave other words when has derivatives that are nonincreasing functions the construction suitable envelope function illustrated graphically figure the function and its gradient are evaluated some initial set grid points and the intersections the resulting tangent lines are used construct the envelope function next sample value drawn from the envelope distribution this straightforward because the log the envelope distribution succession exercise figure the case distributions that are log concave envelope function for use rejection sampling can constructed using the tangent lines computed set grid points sample point rejected added the set grid points and used refine the envelope distribution sampling methods figure plot showing the gamma distribution given the green curve with scaled cauchy proposal distribution shown the red curve samples from the gamma distribution can obtained sampling from the cauchy and then applying the rejection sampling criterion basic sampling algorithms figure illustrative example rejection sampling involving sampling from gaussian distribution shown the green curve using rejection sampling from proposal distribution that also gaussian and whose scaled version shown the red curve linear functions and hence the envelope distribution itself comprises piecewise exponential distribution the form exp once sample has been drawn the usual rejection criterion can applied the sample accepted then will draw from the desired distribution however the sample rejected then incorporated into the set grid points new tangent line computed and the envelope function thereby refined the number grid points increases the envelope function becomes better approximation the desired distribution and the probability rejection decreases variant the algorithm exists that avoids the evaluation derivatives gilks the adaptive rejection sampling framework can also extended distributions that are not log concave simply following each rejection sampling step with metropolis hastings step discussed section giving rise adaptive rejection metropolis sampling gilks clearly for rejection sampling practical value require that the comparison function close the required distribution that the rate rejection kept minimum now let examine what happens when try use rejection sampling spaces high dimensionality consider for the sake illustration somewhat artificial problem which wish sample from zero mean mulpi where the unit matrix tivariate gaussian distribution with covariance rejection sampling from proposal distribution that itself zero mean gaussian order that distribution having covariance there exists such that dimensions the optimum value given illustrated for figure the acceptance rate will the ratio volumes under and which because both distributions are normalized just thus the acceptance rate diminishes exponentially with dimensionality even exceeds just one percent for the acceptance ratio will approximately this illustrative example the comparison function close the required distribution for more practical examples where the desired distribution may multimodal and sharply peaked will extremely difficult find good proposal distribution and comparison function obviously must have sampling methods figure importance sampling addresses the problem evaluating the expectation function with respect distribution from which difficult draw samples diinstead samples are drawn rectly from simpler distribution and the corresponding terms the summation are weighted the ratios furthermore the exponential decrease acceptance rate with dimensionality generic feature rejection sampling although rejection can useful technique one two dimensions unsuited problems high dimensionality can however play role subroutine more sophisticated algorithms for sampling high dimensional spaces"}, "208": {"Section": "11.1.4", "Title": "Importance sampling", "Content": "one the principal reasons for wishing sample from complicated probability distributions able evaluate expectations the form the technique importance sampling provides framework for approximating expectations directly but does not itself provide mechanism for drawing samples from distribution the finite sum approximation the expectation given depends being able draw samples from the distribution suppose however that impractical sample directly from but that can evaluate easily for any given value one simplistic strategy for evaluating expectations would discretize space into uniform grid and evaluate the integrand sum the form obvious problem with this approach that the number terms the summation grows exponentially with the dimensionality furthermore have already noted the kinds probability distributions interest will often have much their mass confined relatively small regions space and uniform sampling will very inefficient because high dimensional problems only very small proportion the samples will make significant contribution the sum would really like choose the sample points fall regions where large ideally where the product large the case rejection sampling importance sampling based the use proposal distribution from which easy draw samples illustrated figure can then express the expectation the form finite sum over rlf samples drawn from basic sampling algorithms the quantities are known importance weights and they correct the bias introduced sampling from the wrong distribution note that unlike rejection sampling all the samples generated are retained will often the case that the distribution can only evaluated normalization constant that can evaluated easily whereas unknown similarly may wish use importance sampling distribution which has the same property then have where where with the result can use the same sample set evaluate the ratio and hence where have defined wlf with rejection sampling the success the importance sampling approach depends crucially how well the sampling distribution matches the desired sampling methods distribution often the case strongly varying and has significant proportion its mass concentrated over relatively small regions space then the set importance weights may dominated few weights having large values with the remaining weights being relatively insignificant thus the effective sample size can much smaller than the apparent sample size the problem even more severe none the samples falls the regions where large that case the apparent variances and rlf may small even though the estimate the expectation may severely wrong hence major drawback the importance sampling method the potential produce results that are arbitrarily error and with diagnostic indication this also highlights key requirement for the sampling distribution namely that should not small zero regions where may significant for distributions defined terms graphical model can apply the importance sampling technique various ways for discrete variables simple approach called uniform sampling the joint distribution for directed graph defined each sample from the joint distribution obtained first setting those variables that are the evidence set equal their observed values each the remaining variables then sampled independently from uniform distribution over the space possible instantiations determine the corresponding weight associq uniform over ated with sample note that the sampling distribution where denotes the subset the possible choices for and that variables that are observed and the equality follows from the fact that every sample that generated necessarily consistent with the evidence thus the weights are simply proportional note that the variables can sampled any order this approach can yield poor results the posterior distribution far from uniform often the case practice improvement this approach called likelihood weighted sampling fung and chang shachter and peot and based ancestral sampling the variables for each variable turn that variable the evidence set then just set its instantiated value not the evidence set then sampled from the conditional distribution pai which the conditioning variables are set their currently sampled values the weighting associated with the resulting sample then given pai pai pai pai this method can further extended using self importance sampling shachter and peot which the importance sampling distribution continually updated reflect the current estimated posterior distribution"}, "209": {"Section": "11.1.5", "Title": "Sampling-importance-resampling", "Content": "the rejection sampling method discussed section depends part for its success the determination suitable value for the constant for many pairs distributions and will impractical determine suitable basic sampling algorithms value for that any value that sufficiently large guarantee bound the desired distribution will lead impractically small acceptance rates the case rejection sampling the sampling importance resampling sir approach also makes use sampling distribution but avoids having determine the constant there are two stages the scheme the first stage samples are drawn from then the second stage weights are constructed using finally second set samples drawn from the discrete distribution with probabilities given the weights the resulting samples are only approximately distributed according but the distribution becomes correct the limit see this consider the univariate case and note that the cumulative distribution the resampled values given where the indicator function which equals its argument true and otherwise taking the limit and assuming suitable regularity the distributions can replace the sums integrals weighted according the original sampling distribution which the cumulative distribution function again see that the normalization not required for finite value and given initial sample set the resampled values will only approximately drawn from the desired distribution with rejection sampling the approximation improves the sampling distribution gets closer the desired distribution when the initial samples have the desired distribution and the weights that the resampled values also have the desired distribution moments with respect the distribution are required then they can sampling methods evaluated directly using the original samples together with the weights because wlf"}, "210": {"Section": "11.1.6", "Title": "Sampling and the EM algorithm", "Content": "addition providing mechanism for direct implementation the bayesian framework monte carlo methods can also play role the frequentist paradigm for example find maximum likelihood solutions particular sampling methods can used approximate the step the algorithm for models which the step cannot performed analytically consider model with hidden variables visible observed variables and parameters the function that optimized with respect the step the expected complete data log likelihood given old old can use sampling methods approximate this integral finite sum over samples which are drawn from the current estimate for the posterior distribution old that old the function then optimized the usual way the step this procedure called the monte carlo algorithm straightforward extend this the problem finding the mode the posterior distribution over the map estimate when prior distribution has been defined simply adding the function old before performing the step particular instance the monte carlo algorithm called stochastic arises consider finite mixture model and draw just one sample each step here the latent variable characterizes which the components the mixture responsible for generating each data point the step sample taken from the posterior distribution old where the data set this effectively makes hard assignment each data point one the components the mixture the step this sampled approximation the posterior distribution used update the model parameters the usual way"}, "211": {"Section": "11.2", "Title": "Markov Chain Monte Carlo", "Content": "now suppose move from maximum likelihood approach full bayesian treatment which wish sample from the posterior distribution over the parameter vector principle would like draw samples from the joint posterior but shall suppose that this computationally difficult suppose further that relatively straightforward sample from the complete data parameter posterior this inspires the data augmentation algorithm which alternates between two steps known the step imputation step analogous step and the step posterior step analogous step algorithm step wish sample from but cannot this directly therefore note the relation and hence for first draw sample from the current estimate for and then use this draw sample from step given the relation use the samples obtained from the step compute revised estimate the posterior distribution over given assumption will feasible sample from this approximation the step note that are making somewhat artificial distinction between parameters and hidden variables from now blur this distinction and focus simply the problem drawing samples from given posterior distribution markov chain monte carlo the previous section discussed the rejection sampling and importance sampling strategies for evaluating expectations functions and saw that they suffer from severe limitations particularly spaces high dimensionality therefore turn this section very general and powerful framework called markov chain monte carlo mcmc which allows sampling from large class distributions sampling methods section min and which scales well with the dimensionality the sample space markov chain monte carlo methods have their origins physics metropolis and ulam and was only towards the end the that they started have significant impact the field statistics with rejection and importance sampling again sample from proposal distribution this time however maintain record the current state and the proposal distribution depends this current state and the sequence samples forms markov chain again write can readily evaluated for any given value although will assume that the value may unknown the proposal distribution itself chosen sufficiently simple that straightforward draw samples from directly each cycle the algorithm generate candidate sample from the proposal distribution and then accept the sample according appropriate criterion the basic metropolis algorithm metropolis assume that the proposal distribution symmetric that for all values and the candidate sample then accepted with probability this can achieved choosing random number with uniform distribution over the unit interval and then accepting the sample note that the step from causes increase the value then the candidate point certain kept the candidate sample accepted then otherwise the candidate point discarded set and another candidate sample drawn from the distribution this contrast rejection sampling where rejected samples are simply discarded the metropolis algorithm when candidate point rejected the previous sample included instead the final list samples leading multiple copies samples course practical implementation only single copy each retained sample would kept along with integer weighting factor recording how many times that state appears shall see long positive for any values and this sufficient but not necessary condition the distribution tends should emphasized however that the sequence not set independent samples from because successive samples are highly correlated wish obtain independent samples then can discard most the sequence and just retain every sample for sufficiently large the retained samples will for all practical purposes independent figure shows simple illustrative example sampling from two dimensional gaussian distribution using the metropolis algorithm which the proposal distribution isotropic gaussian further insight into the nature markov chain monte carlo algorithms can gleaned looking the properties specific example namely simple random markov chain monte carlo figure simple illustration using metropolis algorithm sample from gaussian distribution whose one standard deviation contour shown the ellipse the proposal distribution isotropic gaussian distribution whose standard deviation steps that are accepted are shown green lines and rejected steps are shown red total candidate samples are generated which are rejected exercise walk consider state space consisting the integers with probabilities where denotes the state step the initial state then symmetry the expected state time will also zero and similarly easily seen that thus after steps the random walk has only travelled distance that average proportional the square root this square root dependence typical random walk behaviour and shows that random walks are very inefficient exploring the state space shall see central goal designing markov chain monte carlo methods avoid random walk behaviour"}, "212": {"Section": "11.2.1", "Title": "Markov chains", "Content": "before discussing markov chain monte carlo methods more detail useful study some general properties markov chains more detail particular ask under what circumstances will markov chain converge the desired distribution first order markov chain defined series random variables such that the following conditional independence property holds for this course can represented directed graph the form chain example which shown figure can then specify the markov chain giving the probability distribution for the initial variable together with the sampling methods conditional probabilities for subsequent variables the form transition probabilities markov chain called homogeneous the transition probabilities are the same for all the marginal probability for particular variable can expressed terms the marginal probability for the previous variable the chain the form distribution said invariant stationary with respect markov chain each step the chain leaves that distribution invariant thus for homogeneous markov chain with transition probabilities the distribution invariant note that given markov chain may have more than one invariant distribution for instance the transition probabilities are given the identity transformation then any distribution will invariant sufficient but not necessary condition for ensuring that the required distribution invariant choose the transition probabilities satisfy the property detailed balance defined for the particular distribution easily seen that transition probability that satisfies detailed balance with respect particular distribution will leave that distribution invariant because markov chain that respects detailed balance said reversible our goal use markov chains sample from given distribution can achieve this set markov chain such that the desired distribution invariant however must also require that for the distribution converges the required invariant distribution irrespective the choice initial distribution this property called ergodicity and the invariant distribution then called the equilibrium distribution clearly ergodic markov chain can have only one equilibrium distribution can shown that homogeneous markov chain will ergodic subject only weak restrictions the invariant distribution and the transition probabilities neal practice often construct the transition probabilities from set base transitions this can achieved through mixture distribution the form kbk markov chain monte carlo for some set mixing coefficients satisfying and alternatively the base transitions may combined through successive application that distribution invariant with respect each the base transitions then obviously will also invariant with respect either the given for the case the mixture each the base transitions satisfies detailed balance then the mixture transition will also satisfy detailed balance this does not hold for the transition probability constructed using although symmetrizing the order application the base transitions the form detailed balance can restored common example the use composite transition probabilities where each base transition changes only subset the variables"}, "213": {"Section": "11.2.2", "Title": "The Metropolis-Hastings algorithm", "Content": "earlier introduced the basic metropolis algorithm without actually demonstrating that samples from the required distribution before giving proof first discuss generalization known the metropolis hastings algorithm hastings the case where the proposal distribution longer symmetric function its arguments particular step the algorithm which the current state draw sample from the distribution and then accept with probability where min here labels the members the set possible transitions being considered again the evaluation the acceptance criterion does not require knowledge the normalizing constant the probability distribution for symmetric proposal distribution the metropolis hastings criterion reduces the standard metropolis criterion given can show that invariant distribution the markov chain defined the metropolis hastings algorithm showing that detailed balance defined satisfied using have min min required the specific choice proposal distribution can have marked effect the performance the algorithm for continuous state spaces common choice gaussian centred the current state leading important trade off determining the variance parameter this distribution the variance small then the sampling methods figure schematic illustration the use isotropic gaussian proposal distribution blue circle sample from correlated multivariate gaussian distribution red ellipse having very different standard deviations different directions using the metropolis hastings algorithm order keep the rejection rate low the scale the proposal distribution should the order the smallest standard deviation min which leads random walk behaviour which the number steps separating states that are approximately independent order max min where max the largest standard deviation min max proportion accepted transitions will high but progress through the state space takes the form slow random walk leading long correlation times however the variance parameter large then the rejection rate will high because the kind complex problems are considering many the proposed steps will states for which the probability low consider multivariate distribution having strong correlations between the components illustrated figure the scale the proposal distribution should large possible without incurring high rejection rates this suggests that should the same order the smallest length scale min the system then explores the distribution along the more extended direction means random walk and the number steps arrive state that more less independent the original state order max min fact two dimensions the increase rejection rate increases offset the larger steps sizes those transitions that are accepted and more generally for multivariate gaussian the number steps required obtain independent samples scales like max where the second smallest standard deviation neal these details aside remains the case that the length scales over which the distributions vary are very different different directions then the metropolis hastings algorithm can have very slow convergence"}, "214": {"Section": "11.3", "Title": "Gibbs Sampling", "Content": "gibbs sampling geman and geman simple and widely applicable markov chain monte carlo algorithm and can seen special case the metropolishastings algorithm consider the distribution from which wish sample and suppose that have chosen some initial state for the markov chain each step the gibbs sampling procedure involves replacing the value one the variables value drawn from the distribution that variable conditioned the values the remaining variables thus replace value drawn from the distribution where denotes the ith component and denotes but with omitted this procedure repeated either cycling through the variables gibbs sampling some particular order choosing the variable updated each step random from some distribution for example suppose have distribution over three variables first obtained sampling from the conditional distriand step the algorithm have selected values replace bution new value and obtained sampling from the conditional next replace distribution value that the new value for used straight away subsequent sampling steps then update with sample drawn from and cycling through the three variables turn gibbs sampling initialize for sample sample sample sample josiah willard gibbs gibbs spent almost his entire life living house built his father new haven connecticut gibbs was granted the first phd engineering the united states and was appointed the first chair mathematical physics the united states yale post for which received salary because the time had publications developed the field vector analysis and made contributions crystallography and planetary orbits his most famous work entitled ontheequilibriumofheterogeneous substances laid the foundations for the science physical chemistry sampling methods show that this procedure samples from the required distribution first all note that the distribution invariant each the gibbs sampling steps individually and hence the whole markov chain this follows from the fact that when sample from the marginal distribution clearly invariant because the value unchanged also each step definition samples from the correct conditional distribution because these conditional and marginal distributions together specify the joint distribution see that the joint distribution itself invariant the second requirement satisfied order that the gibbs sampling procedure samples from the correct distribution that ergodic sufficient condition for ergodicity that none the conditional distributions anywhere zero this the case then any point space can reached from any other point finite number steps involving one update each the component variables this requirement not satisfied that some the conditional distributions have zeros then ergodicity applies must proven explicitly the distribution initial states must also specified order complete the algorithm although samples drawn after many iterations will effectively become independent this distribution course successive samples from the markov chain will highly correlated and obtain samples that are nearly independent will necessary subsample the sequence can obtain the gibbs sampling procedure particular instance the metropolis hastings algorithm follows consider metropolis hastings sampling step involving the variable which the remaining variables remain fixed and for which the transition probability from given because these components are unchanged the sampling note that step also thus the factor that determines the acceptance probability the metropolis hastings given where have used accepted thus the metropolis hastings steps are always with the metropolis algorithm can gain some insight into the behaviour gibbs sampling investigating its application gaussian distribution consider correlated gaussian two variables illustrated figure having conditional distributions width and marginal distributions width the typical step size governed the conditional distributions and will order because the state evolves according random walk the number steps needed obtain independent samples from the distribution will order course the gaussian distribution were uncorrelated then the gibbs sampling procedure would optimally efficient for this simple problem could rotate the coordinate system order decorrelate the variables however practical applications will generally infeasible find such transformations one approach reducing random walk behaviour gibbs sampling called over relaxation adler its original form this applies problems for which figure illustration gibbs sampling alternate updates two variables whose distribution correlated gaussian the step size governed the standard deviation the conditional distribution green curve and leading slow progress the direction elongation the joint distribution red ellipse the number steps needed obtain independent sample from the distribution gibbs sampling the conditional distributions are gaussian which represents more general class distributions than the multivariate gaussian because for example the non gaussian distribution exp has gaussian conditional distributions each step the gibbs sampling algorithm the conditional distribution for particular component has some mean and some variance the over relaxation framework the value replaced with where gaussian random variable with zero mean and unit variance and parameter such that for the method equivalent standard gibbs sampling and for the step biased the opposite side the mean this step leaves the desired distribution invariant because has mean then too does the effect over relaxation encourage and variance directed motion through state space when the variables are highly correlated the framework ordered over relaxation neal generalizes this approach nongaussian distributions the practical applicability gibbs sampling depends the ease with which samples can drawn from the conditional distributions the case probability distributions specified using graphical models the conditional distributions for individual nodes depend only the variables the corresponding markov blankets illustrated figure for directed graphs wide choice conditional distributions for the individual nodes conditioned their parents will lead conditional distributions for gibbs sampling that are log concave the adaptive rejection sampling methods discussed section therefore provide framework for monte carlo sampling from directed graphs with broad applicability the graph constructed using distributions from the exponential family and the parent child relationships preserve conjugacy then the full conditional distributions arising gibbs sampling will have the same functional form the orig sampling methods figure the gibbs sampling method requires samples drawn from the conditional distribution variable conditioned the remaining variables for graphical models this conditional distribution function only the states the nodes the markov blanket for undirected graph this comprises the set neighbours shown the left while for directed graph the markov blanket comprises the parents the children and the parents shown the right inal conditional distributions conditioned the parents defining each node and standard sampling techniques can employed general the full conditional distributions will complex form that does not permit the use standard sampling algorithms however these conditionals are log concave then sampling can done efficiently using adaptive rejection sampling assuming the corresponding variable scalar each stage the gibbs sampling algorithm instead drawing sample from the corresponding conditional distribution make point estimate the variable given the maximum the conditional distribution then obtain the iterated conditional modes icm algorithm discussed section thus icm can seen greedy approximation gibbs sampling because the basic gibbs sampling technique considers one variable time there are strong dependencies between successive samples the opposite extreme could draw samples directly from the joint distribution operation that are supposing intractable then successive samples would independent can hope improve the simple gibbs sampler adopting intermediate strategy which sample successively from groups variables rather than individual variables this achieved the blocking gibbs sampling algorithm choosing blocks variables not necessarily disjoint and then sampling jointly from the variables each block turn conditioned the remaining variables jensen"}, "215": {"Section": "11.4", "Title": "Slice Sampling", "Content": "have seen that one the difficulties with the metropolis algorithm the sensitivity step size this too small the result slow decorrelation due random walk behaviour whereas too large the result inefficiency due high rejection rate the technique slice sampling neal provides adaptive step size that automatically adjusted match the characteristics the distribution again requires that are able evaluate the unnormalized distribution consider first the univariate case slice sampling involves augmenting with additional variable and then drawing samples from the joint space shall see another example this approach when discuss hybrid monte carlo section the goal sample uniformly from the area under the distribution given slice sampling zmin zmax for given value value chosen uniformly which then defines slice through the distribution shown the solid horizontal because infeasible sample directly from slice new sample drawn from region figure illustration slice sampling the region lines zmin zmax which contains the previous value otherwise where the marginal distribution over given and then sample uniformly the range this illustrated figure and can sample from sampling from and then ignoring the values this can achieved alternately sampling and given the value which evaluate straightforward then fix and sample uniformly from the slice through the distribution defined practice can difficult sample directly from slice through the distribution and instead define sampling scheme that leaves the uniform distribution invariant which can achieved ensuring that detailed balance under satisfied suppose the current value denoted and that have obtained corresponding sample the next value obtained considering region zmin zmax that contains the choice this region that the adaptation the characteristic length scales the distribution takes place want the region encompass much the slice possible allow large moves space while having little possible this region lying outside the slice because this makes the sampling less efficient one approach the choice region involves starting with region containing having some width and then testing each the end points see they lie within the slice either end point does not then the region extended that direction increments value until the end point lies outside the region candidate value then chosen uniformly from this region and lies within the slice then forms lies outside the slice then the region shrunk such that forms end point and such that the region still contains then another sampling methods candidate point drawn uniformly from this reduced region and until value found that lies within the slice slice sampling can applied multivariate distributions repeatedly sampling each variable turn the manner gibbs sampling this requires that are able compute for each component function that proportional"}, "216": {"Section": "11.5", "Title": "The Hybrid Monte Carlo Algorithm", "Content": "have already noted one the major limitations the metropolis algorithm that can exhibit random walk behaviour whereby the distance traversed through the state space grows only the square root the number steps the problem cannot resolved simply taking bigger steps this leads high rejection rate this section introduce more sophisticated class transitions based analogy with physical systems and that has the property being able make large changes the system state while keeping the rejection probability small applicable distributions over continuous variables for which can readily evaluate the gradient the log probability with respect the state variables will discuss"}, "217": {"Section": "11.5.1", "Title": "Dynamical systems", "Content": "explain how this may combined with the metropolis algorithm yield the powerful hybrid monte carlo algorithm background physics not required this section self contained and the key results are all derived from first principles dynamical systems the dynamical approach stochastic sampling has its origins algorithms for simulating the behaviour physical systems evolving under hamiltonian dynamics markov chain monte carlo simulation the goal sample from given probability distribution the framework hamiltonian dynamics exploited casting the probabilistic simulation the form hamiltonian system order remain keeping with the literature this area make use the relevant dynamical systems terminology where appropriate which will defined along the dynamics that consider corresponds the evolution the state variable under continuous time which denote classical dynamics described newton second law motion which the acceleration object proportional the applied force corresponding second order differential equation over time can decompose second order equation into two coupled firstorder equations introducing intermediate momentum variables corresponding the rate change the state variables having components dzi where the can regarded position variables this dynamics perspective thus where interpreted the potential energy the system when state the system acceleration the rate change momentum and given the applied force which itself the negative gradient the potential energy dri the hybrid monte carlo algorithm for each position variable there corresponding momentum variable and the joint space position and momentum variables called phase space without loss generality can write the probability distribution the form exp convenient reformulate this dynamical system using the hamiltonian framework this first define the kinetic energy the total energy the system then the sum its potential and kinetic energies exercise where the hamiltonian function using and can now express the dynamics the system terms the hamiltonian equations given dzi dri william hamilton william rowan hamilton was irish mathematician and physicist and child prodigy who was appointed professor astronomy trinity college dublin before had even graduated one hamilton most important contributions was new formulation dynamics which played significant role the later development quantum mechanics his other great achievement was the development quaternions which generalize the concept complex numbers introducing three distinct square roots minus one which satisfy ijk said that these equations occurred him while walking along the royal canal dublin with his wife october and promptly carved the equations into the side broome bridge although there longer any evidence the carving there now stone plaque the bridge commemorating the discovery and displaying the quaternion equations sampling methods during the evolution this dynamical system the value the hamiltonian constant easily seen differentiation dzi dri second important property hamiltonian dynamical systems known liouville theorem that they preserve volume phase space other words consider region within the space variables then this region evolves under the equations hamiltonian dynamics its shape may change but its volume will not this can seen noting that the flow field rate change location phase space given and that the divergence this field vanishes div dzi dri now consider the joint distribution over phase space whose total energy the hamiltonian the distribution given exp using the two results conservation volume and conservation follows that the hamiltonian dynamics will leave invariant this can seen considering small region phase space over which approximately constant follow the evolution the hamiltonian equations for finite time then the volume this region will remain unchanged will the value this region and hence the probability density which function only will also unchanged although invariant the values and will vary and integrating the hamiltonian dynamics over finite time duration becomes possible make large changes systematic way that avoids random walk behaviour evolution under the hamiltonian dynamics will not however sample ergodically from because the value constant order arrive ergodic sampling scheme can introduce additional moves phase space that change the value while also leaving the distribution invariant the simplest way achieve this replace the value with one drawn from its distribution conditioned this can regarded gibbs sampling step and hence from exercise the hybrid monte carlo algorithm section see that this also leaves the desired distribution invariant noting that and are independent the distribution see that the conditional distribution gaussian from which straightforward sample practical application this approach have address the problem performing numerical integration the hamiltonian equations this will necessarily introduce numerical errors and should devise scheme that minimizes the impact such errors fact turns out that integration schemes can devised for which liouville theorem still holds exactly this property will important"}, "218": {"Section": "11.5.2", "Title": "Hybrid Monte Carlo", "Content": "for achieving this called the leapfrog discretization and involves alternately updatr the position and momentum variables ing discrete time approximations using and see that this takes the form half step update the momentum variables with step size followed full step update the position variables with step size followed second half step update the momentum variables several leapfrog steps are applied succession can seen that half step updates the momentum variables can combined into full step updates with step size the successive updates position and momentum variables then leapfrog over each other order advance the dynamics time interval need take steps the error involved the discretized approximation the continuous time dynamics will zero assuming smooth function the limit however for nonzero used practice some residual error will remain shall see section how the effects such errors can eliminated the hybrid monte carlo algorithm summary then the hamiltonian dynamical approach involves alternating between series leapfrog updates and resampling the momentum variables from their marginal distribution note that the hamiltonian dynamics method unlike the basic metropolis algorithm able make use information about the gradient the log probability distribution well about the distribution itself analogous situation familiar from the domain function optimization most cases where gradient information available highly advantageous make use informally this follows from the fact that space dimension the additional computational cost evaluating gradient compared with evaluating the function itself will typically fixed factor independent whereas the dimensional gradient vector conveys pieces information compared with the one piece information given the function itself sampling methods hybrid monte carlo discussed the previous section for nonzero step size the discretization the leapfrog algorithm will introduce errors into the integration the hamiltonian dynamical equations hybrid monte carlo duane neal combines hamiltonian dynamics with the metropolis algorithm and thereby removes any bias associated with the discretization specifically the algorithm uses markov chain consisting alternate stochastic updates the momentum variable and hamiltonian dynamical updates using the leapfrog algorithm after each application the leapfrog algorithm the resulting candidate state accepted rejected according the metropolis criterion based the value the hamiltonian thus the initial state and the state after the leapfrog integration then this candidate state accepted with probability min exp the leapfrog integration were simulate the hamiltonian dynamics perfectly then every such candidate step would automatically accepted because the value would unchanged due numerical errors the value may sometimes decrease and would like the metropolis criterion remove any bias due this effect and ensure that the resulting samples are indeed drawn from the required distribution order for this the case need ensure that the update equations corresponding the leapfrog integration satisfy detailed balance this easily achieved modifying the leapfrog scheme follows before the start each leapfrog integration sequence choose random with equal probability whether integrate forwards time using step size backwards time using step size first note that the leapfrog integration scheme and time reversible that integration for steps using step size will exactly undo the effect integration for steps using step size next show that the leapfrog integration preserves phase space volume exactly this follows from the fact that each step the leapfrog scheme updates either variable variable amount that function only the other variable shown figure this has the effect shearing region phase space while not altering its volume finally use these results show that detailed balance holds consider small region phase space that under sequence leapfrog iterations step size maps region using conservation volume under the leapfrog iteration see that has volume then too will choose initial point from the distribution and then update using leapfrog interactions the probability the transition going from given exp min exp where the factor arises from the probability choosing integrate with positive step size rather than negative one similarly the probability starting the hybrid monte carlo algorithm exercise figure each step the leapfrog algorithm modifies either position variable momentum variable because the change one variable function only the other any region phase space will sheared without change volume region and integrating backwards time end region given exp min exp easily seen that the two probabilities and are equal and hence detailed balance holds note that this proof ignores any overlap between the regions and but easily generalized allow for such overlap not difficult construct examples for which the leapfrog algorithm returns its starting position after finite number iterations such cases the random replacement the momentum values before each leapfrog integration will not sufficient ensure ergodicity because the position variables will never updated such phenomena are easily avoided choosing the magnitude the step size random from some small interval before each leapfrog integration can gain some insight into the behaviour the hybrid monte carlo algorithm considering its application multivariate gaussian for convenience consider gaussian distribution with independent components for which the hamiltonian given our conclusions will equally valid for gaussian distribution having correlated components because the hybrid monte carlo algorithm exhibits rotational isotropy during the leapfrog integration each pair phase space variables evolves independently however the acceptance rejection the candidate point based the value which depends the values all the variables thus significant integration error any one the variables could lead high probability rejection order that the discrete leapfrog integration reasonably sampling methods good approximation the true continuous time dynamics necessary for the leapfrog integration scale smaller than the shortest length scale over which the potential varying significantly this governed the smallest value which denote min recall that the goal the leapfrog integration hybrid monte carlo move substantial distance through phase space new state that relatively independent the initial state and still achieve high probability acceptance order achieve this the leapfrog integration must continued for number iterations order max min contrast consider the behaviour simple metropolis algorithm with isotropic gaussian proposal distribution variance considered earlier order avoid high rejection rates the value must order min the exploration state space then proceeds random walk and takes order max min steps arrive roughly independent state"}, "219": {"Section": "11.6", "Title": "Estimating the Partition Function", "Content": "have seen most the sampling algorithms considered this chapter require only the functional form the probability distribution multiplicative constant thus write exp then the value the normalization constant also known the partition function not needed order draw samples from however knowledge the value can useful for bayesian model comparison since represents the model evidence the probability the observed data given the model and interest consider how its value might obtained assume that direct evaluation summing integrating the function exp over the state space intractable for model comparison actually the ratio the partition functions for two models that required multiplication this ratio the ratio prior probabilities gives the ratio posterior probabilities which can then used for model selection model averaging one way estimate ratio partition functions use importance sampling from distribution with energy function exp exp exp exp exp exp exp where are samples drawn from the distribution defined the distribution one for which the partition function can evaluated analytically for example gaussian then the absolute value can obtained this approach will only yield accurate results the importance sampling distribution closely matched the distribution that the ratio does not have wide variations practice suitable analytically specified importance sampling distributions cannot readily found for the kinds complex models considered this book alternative approach therefore use the samples obtained from markov chain define the importance sampling distribution the transition probability for the markov chain given and the sample set given then the sampling distribution can written exp which can used directly estimating the partition function methods for estimating the ratio two partition functions require for their success that the two corresponding distributions reasonably closely matched this especially problematic wish find the absolute value the partition function for complex distribution because only for relatively simple distributions that the partition function can evaluated directly and attempting estimate the ratio partition functions directly unlikely successful this problem can tackled using technique known chaining neal barber and bishop which involves introducing succession intermediate distributions that interpolate between simple distribution for which can evaluate the normalization coefficient and the desired complex distribution then have which the intermediate ratios can determined using monte carlo methods discussed above one way construct such sequence intermediate systems use energy function containing continuous parameter that interpolates between the two distributions the intermediate ratios are found using monte carlo may more efficient use single markov chain run than restart the markov chain for each ratio this case the markov chain run initially for the system and then after some suitable number steps moves the next distribution the sequence note however that the system must remain close the equilibrium distribution each stage"}, "220": {"Section": "12", "Title": "Continuous Latent Variables", "Content": "chapter discussed probabilistic models having discrete latent variables such the mixture gaussians now explore models which some all the latent variables are continuous important motivation for such models that many data sets have the property that the data points all lie close manifold much lower dimensionality than that the original data space see why this might arise consider arti cial data set constructed taking one the off line digits represented pixel grey level image and embedding larger image size padding with pixels having the value zero corresponding white pixels which the location and orientation the digit varied random illustrated figure each the resulting images represented point the dimensional data space however across data set such images there are only three degrees freedom variability corresponding the vertical and horizontal translations and the rotations the data points will therefore live subspace the data space whose intrinsic dimensionality three note continuous latent variables figure synthetic data set obtained taking one the off line digit images and creating multiple copies each which the digit has undergone random displacement and rotation within some larger image eld the resulting images each have pixels that the manifold will nonlinear because for instance translate the digit past particular pixel that pixel value will from zero white one black and back zero again which clearly nonlinear function the digit position this example the translation and rotation parameters are latent variables because observe only the image vectors and are not told which values the translation rotation variables were used create them for real digit image data there will further degree freedom arising from scaling moreover there will multiple additional degrees freedom associated with more complex deformations due the variability individual writing well the differences writing styles between individuals nevertheless the number such degrees freedom will small compared the dimensionality the data set another example provided the oil data set which for given geometrical con guration the gas water and oil phases there are only two degrees freedom variability corresponding the fraction oil the pipe and the fraction water the fraction gas then being determined although the data space comprises measurements data set points will lie close two dimensional manifold embedded within this space this case the manifold comprises several distinct segments corresponding different regimes each such segment being noisy continuous two dimensional manifold our goal data compression density modelling then there can bene exploiting this manifold structure practice the data points will not con ned precisely smooth lowdimensional manifold and can interpret the departures data points from the manifold noise this leads naturally generative view such models which rst select point within the manifold according some latent variable distribution and then generate observed data point adding noise drawn from some conditional distribution the data variables given the latent variables the simplest continuous latent variable model assumes gaussian distributions for both the latent and observed variables and makes use linear gaussian dependence the observed variables the state the latent variables this leads probabilistic formulation the well known technique principal component analysis pca well related model called factor analysis this chapter will begin with standard nonprobabilistic treatment pca and then show how pca arises naturally the maximum likelihood solution appendix section section"}, "221": {"Section": "12.1", "Title": "Principal Component Analysis", "Content": "figure principal component analysis seeks space lower dimensionality known the principal subspace and denoted the magenta line such that the orthogonal projection the data points red dots onto this subspace maximizes the variance the projected points green dots alternative nition pca based minimizing the sum squares the projection errors indicated the blue lines cid section section particular form linear gaussian latent variable model this probabilistic reformulation brings many advantages such the use for parameter estimation principled extensions mixtures pca models and bayesian formulations that allow the number principal components determined automatically from the data finally discuss brie several generalizations the latent variable concept that beyond the linear gaussian assumption including non gaussian latent variables which leads the framework independent component analysis well models having nonlinear relationship between latent and observed variables principal component analysis principal component analysis pca technique that widely used for applications such dimensionality reduction lossy data compression feature extraction and data visualization jolliffe also known the karhunen eve transform there are two commonly used nitions pca that give rise the same algorithm pca can ned the orthogonal projection the data onto lower dimensional linear space known the principal subspace such that the variance the projected data maximized hotelling equivalently can ned the linear projection that minimizes the average projection cost ned the mean squared distance between the data points and their projections pearson the process orthogonal projection illustrated figure consider each these nitions turn"}, "222": {"Section": "12.1.1", "Title": "Maximum variance formulation", "Content": "consider data set observations where and euclidean variable with dimensionality our goal project the data onto space having dimensionality while maximizing the variance the projected data for the moment shall assume that the value given later this continuous latent variables chapter shall consider techniques determine appropriate value from the data begin with consider the projection onto one dimensional space can the direction this space using dimensional vector which for convenience and without loss generality shall choose unit vector note that are only interested the direction ned that not the magnitude itself each data point then projected onto scalar value where the sample set mean given the mean the projected data cid cid cid cid cid and the variance the projected data given where the data covariance matrix ned appendix now maximize the projected variance with respect clearly this has constrained maximization prevent cid cid the appropriate constraint enforce this constraint comes from the normalization condition introduce lagrange multiplier that shall denote and then make unconstrained maximization cid cid setting the derivative with respect equal zero see that this quantity will have stationary point when which says that must eigenvector left multiply use see that the variance given and make and the variance will maximum when set equal the eigenvector having the largest eigenvalue this eigenvector known the rst principal component can additional principal components incremental fashion choosing each new direction that which maximizes the projected variance principal component analysis amongst all possible directions orthogonal those already considered consider the general case dimensional projection space the optimal linear projection for which the variance the projected data maximized now ned the eigenvectors the data covariance matrix corresponding the largest eigenvalues this easily shown using proof induction summarize principal component analysis involves evaluating the mean and the covariance matrix the data set and then nding the eigenvectors corresponding the largest eigenvalues algorithms for nding eigenvectors and eigenvalues well additional theorems related eigenvector decomposition can found golub and van loan note that the computational cost computing the full eigenvector decomposition for matrix size plan project our data onto the rst principal components then only need the rst eigenvalues and eigenvectors this can done with more cient techniques such the power method golub and van loan that scale like alternatively can make use the algorithm"}, "223": {"Section": "12.1.2", "Title": "Minimum-error formulation", "Content": "now discuss alternative formulation pca based projection error minimization this introduce complete orthonormal set dimensional basis vectors where that satisfy exercise section appendix because this basis complete each data point can represented exactly linear combination the basis vectors niui where the coef cients will different for different data points this simply corresponds rotation the coordinate system new system ned the and the original components xnd are replaced equivalent set taking the inner product with and making use the orthonormality property obtain nuj and without loss generality can write nui our goal however approximate this data point using representation involving restricted number variables corresponding projection onto lower dimensional subspace the dimensional linear subspace can represented without loss generality the rst the basis vectors and approximate each data point cid cid cid cid cid cid cid zniui biui continuous latent variables cid cid cid cid where the zni depend the particular data point whereas the are constants that are the same for all data points are free choose the the zni and the minimize the distortion introduced the reduction dimensionality our distortion measure shall use the squared distance between the original data point and its approximation cid averaged over the data set that our goal minimize consider rst all the minimization with respect the quantities zni substituting for cid setting the derivative with respect znj zero and making use the orthonormality conditions obtain znj nuj where similarly setting the derivative with respect zero and again making use the orthonormality relations gives xtuj where substitute for zni and and make use the general expansion obtain cid cid cid cid tui from which see that the displacement vector from cid lies the space projected points cid must lie within the principal subspace but can move them orthogonal the principal subspace because linear combination for illustrated figure this expected because the therefore obtain expression for the distortion measure function freely within that subspace and the minimum error given the orthogonal projection purely the the form cid cid cid cid cid nui xtui sui there remains the task minimizing with respect the which must constrained minimization otherwise will obtain the vacuous result the constraints arise from the orthonormality conditions and shall see the solution will expressed terms the eigenvector expansion the covariance matrix before considering formal solution let try obtain some intuition about the result considering the case two dimensional data space and onedimensional principal subspace have choose direction principal component analysis cid cid cid minimize lagrange multiplier enforce the constraint consider the minimization subject the normalization constraint using setting the derivative with respect zero obtain that eigenvector with eigenvalue thus any eigenvector will stationary point the distortion measure the value the minimum back substitute the solution for into the distortion measure give therefore obtain the minimum value choosing the eigenvector corresponding the smaller the two eigenvalues thus should choose the principal subspace aligned with the eigenvector having the larger eigenvalue this result accords with our intuition that order minimize the average squared projection distance should choose the principal component subspace pass through the mean the data points and aligned with the directions maximum variance for the case when the eigenvalues are equal any choice principal direction will give rise the same value the general solution the minimization for arbitrary and arbitrary obtained choosing the eigenvectors the covariance matrix given where and usual the eigenvectors are chosen orthonormal the corresponding value the distortion measure then given sui iui cid which simply the sum the eigenvalues those eigenvectors that are orthogonal the principal subspace therefore obtain the minimum value selecting these eigenvectors those having the smallest eigenvalues and hence the eigenvectors ning the principal subspace are those corresponding the largest eigenvalues although have considered the pca analysis still holds which case there dimensionality reduction but simply rotation the coordinate axes align with principal components finally worth noting that there exists closely related linear dimensionality reduction technique called canonical correlation analysis cca hotelling bach and jordan whereas pca works with single random variable cca considers two more variables and tries corresponding pair linear subspaces that have high cross correlation that each component within one the subspaces correlated with single component from the other subspace its solution can expressed terms generalized eigenvector problem"}, "224": {"Section": "12.1.3", "Title": "Applications of PCA", "Content": "can illustrate the use pca for data compression considering the offline digits data set because each eigenvector the covariance matrix vector exercise appendix continuous latent variables mean figure the mean vector along with the rst four pca eigenvectors for the off line digits data set together with the corresponding eigenvalues the original dimensional space can represent the eigenvectors images the same size the data points the rst eigenvectors along with the corresponding eigenvalues are shown figure plot the complete spectrum eigenvalues sorted into decreasing order shown figure the distortion measure associated with choosing particular value given the sum the eigenvalues from and plotted for different values figure substitute and into can write the pca approximation data vector the form cid cid cid cid cid nui xtui cid nui xtui figure plot the eigenvalue spectrum for the off line digits data set plot the sum the discarded eigenvalues which represents the sum squares distortion introduced projecting the data onto principal component subspace dimensionality principal component analysis original figure original example from the off line digits data set together with its pca reconstructions obtained retaining principal components for various values increases the reconstruction becomes more accurate and would become perfect when cid cid where have made use the relation cid cid cid xtui which follows from the completeness the this represents compression the data set because for each data point have replaced the dimensional vector with dimensional vector having components the smaller the value the greater the degree compression examples pca reconstructions data points for the digits data set are shown figure nui xtui another application principal component analysis data pre processing this case the goal not dimensionality reduction but rather the transformation data set order standardize certain its properties this can important allowing subsequent pattern recognition algorithms applied successfully the data set typically done when the original variables are measured various different units have signi cantly different variability for instance the old faithful data set the time between eruptions typically order magnitude greater than the duration eruption when applied the means algorithm this data set rst made separate linear scaling the individual variables such that each variable had zero mean and unit variance this known standardizing the data and the covariance matrix for the standardized data has components cid xni xnj where the variance this known the correlation matrix the original data and has the property that two components and the data are perfectly correlated then and they are uncorrelated then however using pca can make more substantial normalization the data give zero mean and unit covariance that different variables become decorrelated this rst write the eigenvector equation the form appendix section continuous latent variables figure illustration the effects linear pre processing applied the old faithful data set the plot the left shows the original data the centre plot shows the result standardizing the individual variables zero mean and unit variance also shown are the principal axes this normalized data set plotted over the range the plot the right shows the result whitening the data give zero mean and unit covariance where diagonal matrix with elements and orthogonal matrix with columns given then for each data point transformed value given where the sample mean ned clearly the set has zero mean and its covariance given the identity matrix because cid cid ynyt tul utsul appendix appendix this operation known whitening sphereing the data and illustrated for the old faithful data set figure interesting compare pca with the fisher linear discriminant which was discussed section both methods can viewed techniques for linear dimensionality reduction however pca unsupervised and depends only the values whereas fisher linear discriminant also uses class label information this difference highlighted the example figure another common application principal component analysis data visualization here each data point projected onto two dimensional principal subspace that data point plotted cartesian coordinates given and where and are the eigenvectors corresponding the largest and second largest eigenvalues example such plot for the oil data set shown figure principal component analysis figure comparison principal component analysis with fisher linear discriminant for linear dimensionality reduction here the data two dimensions belonging two classes shown red and blue projected onto single dimension pca chooses the direction maximum variance shown the magenta curve which leads strong class overlap whereas the fisher linear discriminant takes account the class labels and leads projection onto the green curve giving much better class separation figure visualization the oil data set obtained projecting the data onto the rst two principal components the red blue and green points correspond the laminar homogeneous and annular con gurations respectively"}, "225": {"Section": "12.1.4", "Title": "PCA for high-dimensional data", "Content": "some applications principal component analysis the number data points smaller than the dimensionality the data space for example might want apply pca data set few hundred images each which corresponds vector space potentially several million dimensions corresponding three colour values for each the pixels the image note that dimensional space set points where nes linear subspace whose dimensionality most and there little point applying pca for values that are greater than indeed perform pca will that least the eigenvalues are zero corresponding eigenvectors along whose directions the data set has zero variance furthermore typical algorithms for nding the eigenvectors matrix have computational cost that scales like and for applications such the image example direct application pca will computationally infeasible can resolve this problem follows first let the continuous latent variables dimensional centred data matrix whose nth row given the covari xtx and the corresponding ance matrix can then written eigenvector equation becomes xtxui iui now pre multiply both sides give xxt xui xui now xui obtain xxtvi ivi which eigenvector equation for the matrix xxt see that this has the same eigenvalues the original covariance matrix which itself has additional eigenvalues value zero thus can solve the eigenvector problem spaces lower dimensionality with computational cost instead order determine the eigenvectors multiply both sides give cid xtx xtvi xtvi cid from which see that xtvi eigenvector with eigenvalue note however that these eigenvectors need not normalized determine the appropriate normalization scale xtvi constant such that cid cid which assuming has been normalized unit length gives xtvi summary apply this approach rst evaluate xxt and then its eigenvectors and eigenvalues and then compute the eigenvectors the original data space using"}, "226": {"Section": "12.2", "Title": "Probabilistic PCA", "Content": "the formulation pca discussed the previous section was based linear projection the data onto subspace lower dimensionality than the original data space now show that pca can also expressed the maximum likelihood solution probabilistic latent variable model this reformulation pca known probabilistic pca brings several advantages compared with conventional pca probabilistic pca represents constrained form the gaussian distribution which the number free parameters can restricted while still allowing the model capture the dominant correlations data set section section section section probabilistic pca can derive algorithm for pca that computationally cient situations where only few leading eigenvectors are required and that avoids having evaluate the data covariance matrix intermediate step the combination probabilistic model and allows deal with missing values the data set mixtures probabilistic pca models can formulated principled way and trained using the algorithm probabilistic pca forms the basis for bayesian treatment pca which the dimensionality the principal subspace can found automatically from the data the existence likelihood function allows direct comparison with other probabilistic density models contrast conventional pca will assign low reconstruction cost data points that are close the principal subspace even they lie arbitrarily far from the training data probabilistic pca can used model class conditional densities and hence applied classi cation problems the probabilistic pca model can run generatively provide samples from the distribution this formulation pca probabilistic model was proposed independently tipping and bishop and roweis shall see later closely related factor analysis basilevsky probabilistic pca simple example the linear gaussian framework which all the marginal and conditional distributions are gaussian can formulate probabilistic pca rst introducing explicit latent variable corresponding the principal component subspace next gaussian prior distribution over the latent variable together with gaussian conditional distribution for the observed variable conditioned the value the latent variable speci cally the prior distribution over given zero mean unit covariance gaussian similarly the conditional distribution the observed variable conditioned the value the latent variable again gaussian the form which the mean general linear function governed the matrix and the dimensional vector note that this factorizes with respect the elements other words this example the naive bayes model shall see shortly the columns span linear subspace within the data space that corresponds the principal subspace the other parameter this model the scalar governing the variance the conditional distribution note that there continuous latent variables figure illustration the generative view the probabilistic pca model for two dimensional data space and one dimensional latent space observed data point generated rst drawing value for the latent variable from its prior distribution and then drawing value for from isotropic gaussian distribution illustrated the red circles having mean wbz and covariance the green ellipses show the density contours for the marginal distribution exercise loss generality assuming zero mean unit covariance gaussian for the latent distribution because more general gaussian distribution would give rise equivalent probabilistic model can view the probabilistic pca model from generative viewpoint which sampled value the observed variable obtained rst choosing value for the latent variable and then sampling the observed variable conditioned this latent value speci cally the dimensional observed variable ned linear transformation the dimensional latent variable plus additive gaussian noise that where dimensional gaussian latent variable and dimensional zero mean gaussian distributed noise variable with covariance this generative process illustrated figure note that this framework based mapping from latent space data space contrast the more conventional view pca discussed above the reverse mapping from data space the latent space will obtained shortly using bayes theorem suppose wish determine the values the parameters and using maximum likelihood write down the likelihood function need expression for the marginal distribution the observed variable this expressed from the sum and product rules probability the form cid exercise because this corresponds linear gaussian model this marginal distribution again gaussian and given probabilistic pca cid where the covariance matrix ned wwt this result can also derived more directly noting that the predictive distribution will gaussian and then evaluating its mean and covariance using this gives cid cid cov wzztwt cid where have used the fact that and are independent random variables and hence are uncorrelated wwt intuitively can think the distribution being ned taking isotropic gaussian spray can and moving across the principal subspace spraying gaussian ink with density determined and weighted the prior distribution the accumulated ink density gives rise pancake shaped distribution representing the marginal density orthogonal matrix using the orthogonality property rrt see that the the predictive distribution governed the parameters and however there redundancy this parameterization corresponding rotations the latent space coordinates see this consider matrix where quantity that appears the covariance matrix takes the form and hence independent thus there whole family matrices all wrrtwt wwt which give rise the same predictive distribution this invariance can understood terms rotations within the latent space shall return discussion the number independent parameters this model later when evaluate the predictive distribution require which involves the inversion matrix the computation required this can reduced making use the matrix inversion identity give where the matrix ned exercise wtw because invert rather than inverting directly the cost evaluating reduced from well the predictive distribution will also require the posterior distribution which can again written down directly using the result for linear gaussian models give note that the posterior mean depends whereas the posterior covariance independent cid cid continuous latent variables figure the probabilistic pca model for data set observations can expressed directed graph which each observation associated with value the latent variable"}, "227": {"Section": "12.2.1", "Title": "Maximum likelihood PCA", "Content": "next consider the determination the model parameters using maximum likelihood given data set observed data points the probabilistic pca model can expressed directed graph shown figure the corresponding log likelihood function given from cid cid setting the derivative the log likelihood with respect equal zero gives the expected result where the data mean ned back substituting can then write the log likelihood function the form cid cid cid cid where the data covariance matrix ned because the log likelihood quadratic function this solution represents the unique maximum can con rmed computing second derivatives maximization with respect and more complex but nonetheless has exact closed form solution was shown tipping and bishop that all the stationary points the log likelihood function can written wml where matrix whose columns are given any subset size the eigenvectors the data covariance matrix the diagonal matrix has elements given the corresponding eigenvalues and arbitrary orthogonal matrix furthermore tipping and bishop showed that the maximum the likelihood function obtained when the eigenvectors are chosen those whose eigenvalues are the largest all other solutions being saddle points similar result was conjectured independently roweis although proof was given probabilistic pca again shall assume that the eigenvectors have been arranged order decreasing values the corresponding eigenvalues that the principal eigenvectors are this case the columns the principal subspace standard pca the corresponding maximum likelihood solution for then given cid that the average variance associated with the discarded dimensions because orthogonal can interpreted rotation matrix the latent space substitute the solution for into the expression for and make use the orthogonality property rrt see that independent this simply says that the predictive density unchanged rotations the latent space discussed earlier for the particular case see that the columns are the principal component eigenvectors scaled the variance parameters the interpretation these scaling factors clear once recognize that for convolution independent gaussian distributions this case the latent space distribution and the noise model the variances are additive thus the variance the direction eigenvector composed the sum contribution from the projection the unit variance latent space distribution into data space through the corresponding column plus isotropic contribution variance which added all directions the noise model worth taking moment study the form the covariance matrix given consider the variance the predictive distribution along some direction speci the unit vector where vtv which given vtcv first suppose that orthogonal the principal subspace other words given some linear combination the discarded eigenvectors then vtu and hence vtcv thus the model predicts noise variance orthogonal the principal subspace which from just the average the discarded eigenvalues now suppose that where one the retained eigenvectors ning the principal subspace then vtcv other words this model correctly captures the variance the data along the principal axes and approximates the variance all remaining directions with single average value one way construct the maximum likelihood density model would simply the eigenvectors and eigenvalues the data covariance matrix and then evaluate and using the results given above this case would choose for convenience however the maximum likelihood solution found numerical optimization the likelihood function for instance using algorithm such conjugate gradients fletcher nocedal and wright bishop and nabney through the algorithm then the resulting value essentially arbitrary this implies that the columns need not orthogonal orthogonal basis required the matrix can post processed appropriately golub and van loan alternatively the algorithm can modi such way yield orthonormal principal directions sorted descending order the corresponding eigenvalues directly ahn and section continuous latent variables the rotational invariance latent space represents form statistical nonidenti ability analogous that encountered for mixture models the case discrete latent variables here there continuum parameters all which lead the same predictive density contrast the discrete nonidenti ability associated with component labelling the mixture setting consider the case that there reduction dimensionality then and making use the orthogonality properties uut and rrt see that the covariance the marginal distribution for becomes rrt ulut and obtain the standard maximum likelihood solution for unconstrained gaussian distribution which the covariance matrix given the sample covariance conventional pca generally formulated projection points from the ddimensional data space onto dimensional linear subspace probabilistic pca however most naturally expressed mapping from the latent space into the data space via for applications such visualization and data compression can reverse this mapping using bayes theorem any point data space can then summarized its posterior mean and covariance latent space from the mean given where given this projects point data space given section note that this takes the same form the equations for regularized linear regression and consequence maximizing the likelihood function for linear gaussian model similarly the posterior covariance given from and independent take the limit then the posterior mean reduces mlwml exercise exercise section which represents orthogonal projection the data point onto the latent space and recover the standard pca model the posterior covariance this limit zero however and the density becomes singular for the latent projection shifted towards the origin relative the orthogonal projection finally note that important role for the probabilistic pca model ning multivariate gaussian distribution which the number degrees freedom other words the number independent parameters can controlled whilst still allowing the model capture the dominant correlations the data recall that general gaussian distribution has independent parameters its covariance matrix plus another parameters its mean thus the number parameters scales quadratically with and can become excessive spaces high probabilistic pca dimensionality restrict the covariance matrix diagonal then has only independent parameters and the number parameters now grows linearly with dimensionality however now treats the variables they were independent and hence can longer express any correlations between them probabilistic pca provides elegant compromise which the most signi cant correlations can captured while still ensuring that the total number parameters grows only linearly with can see this evaluating the number degrees freedom the ppca model follows the covariance matrix depends the parameters which has size and giving total parameter count however have seen that there some redundancy this parameterization associated with rotations the coordinate system the latent space the orthogonal matrix that expresses these rotations has size the rst column this matrix there are independent parameters because the column vector must normalized unit length the second column there are independent parameters because the column must normalized and also must orthogonal the previous column and summing this arithmetic series see that has total independent parameters thus the number degrees freedom the covariance matrix given the number independent parameters this model therefore only grows linearly with for xed take then recover the standard result for full covariance gaussian this case the variance along linearly independent directions controlled the columns and the variance along the remaining direction given the model equivalent the isotropic covariance case"}, "228": {"Section": "12.2.2", "Title": "EM algorithm for PCA", "Content": "have seen the probabilistic pca model can expressed terms marginalization over continuous latent space which for each data point there corresponding latent variable can therefore make use the algorithm maximum likelihood estimates the model parameters this may seem rather pointless because have already obtained exact closed form solution for the maximum likelihood parameter values however spaces high dimensionality there may computational advantages using iterative procedure rather than working directly with the sample covariance matrix this procedure can also extended the factor analysis model for which there closed form solution finally allows missing data handled principled way can derive the algorithm for probabilistic pca following the general framework for thus write down the complete data log likelihood and take its expectation with respect the posterior distribution the latent distribution evaluated using old parameter values maximization this expected completedata log likelihood then yields the new parameter values because the data points exercise section section continuous latent variables are assumed independent the complete data log likelihood function takes the form cid cid cid where the nth row the matrix given already know that the exact maximum likelihood solution for given the sample mean ned and convenient substitute for this stage making use the expressions and for the latent and conditional distributions respectively and taking the expectation with respect the posterior distribution over the latent variables obtain cid cid znzt cid cid cid cid cid cid cid cid cid twt znzt wtw exercise note that this depends the posterior distribution only through the suf cient statistics the gaussian thus the step use the old parameter values evaluate znzt which follow directly from the posterior distribution together with the standard result znzt cov here ned the step maximize with respect and keeping the posterior statistics xed maximization with respect straightforward for the maximization with respect make use and obtain the step equations cid cid cid cid cid twt znzt cid cid cid cid cid cid znzt new newwnew wnew new the algorithm for probabilistic pca proceeds initializing the parameters and then alternately computing the suf cient statistics the latent space posterior distribution using and the step and revising the parameter values using and the step one the bene the algorithm for pca computational ciency for large scale applications roweis unlike conventional pca based probabilistic pca eigenvector decomposition the sample covariance matrix the approach iterative and might appear less attractive however each cycle the algorithm can computationally much more cient than conventional pca spaces high dimensionality see this note that the eigendecomposition the covariance matrix requires computation often are interested only the rst eigenvectors and their corresponding eigenvalues which case can use algorithms that are however the evaluation the covariance matrix itself takes computations where the number data points algorithms such the snapshot method sirovich which assume that the eigenvectors are linear combinations the data vectors avoid direct evaluation the covariance matrix but are and hence unsuited large data sets the algorithm described here also does not construct the covariance matrix explicitly instead the most computationally demanding steps are those involving sums over the data set that are for large and cid this can signi cant saving compared and can offset the iterative nature the algorithm note that this algorithm can implemented line form which each dimensional data point read and processed and then discarded before the next data point considered see this note that the quantities evaluated the step dimensional vector and matrix can computed for each data point separately and the step need accumulate sums over data points which can incrementally this approach can advantageous both and are large because now have fully probabilistic model for pca can deal with missing data provided that missing random marginalizing over the distribution the unobserved variables again these missing values can treated using the algorithm give example the use this approach for data visualization figure another elegant feature the approach that can take the limit corresponding standard pca and still obtain valid like algorithm roweis from see that the only quantity need compute the step furthermore the step simpli because wtw emphasize nth row given the vector and similarly matrix size whose nth row given the vector the step the algorithm for pca then becomes the simplicity the algorithm let cid matrix size whose and the step takes the form old oldwold cid wnew cid again these can implemented line form these equations have simple interpretation follows from our earlier discussion see that the step involves orthogonal projection the data points onto the current estimate for the principal subspace correspondingly the step represents estimation the principal continuous latent variables figure probabilistic pca visualization portion the oil data set for the rst data points the left hand plot shows the posterior mean projections the data points the principal subspace the right hand plot obtained rst randomly omitting the variable values and then using handle the missing values note that each data point then has least one missing measurement but that the plot very similar the one obtained without missing values exercise subspace minimize the squared reconstruction error which the projections are xed can give simple physical analogy for this algorithm which easily visualized for and consider collection data points two dimensions and let the one dimensional principal subspace represented solid rod now attach each data point the rod via spring obeying hooke law stored energy proportional the square the spring length the step keep the rod xed and allow the attachment points slide and down the rod minimize the energy this causes each attachment point independently position itself the orthogonal projection the corresponding data point onto the rod the step keep the attachment points xed and then release the rod and allow move the minimum energy position the and steps are then repeated until suitable convergence criterion satis illustrated figure"}, "229": {"Section": "12.2.3", "Title": "Bayesian PCA", "Content": "far our discussion pca have assumed that the value for the dimensionality the principal subspace given practice must choose suitable value according the application for visualization generally choose whereas for other applications the appropriate choice for may less clear one approach plot the eigenvalue spectrum for the data set analogous the example figure for the off line digits data set and look see the eigenvalues naturally form two groups comprising set small values separated signi cant gap from set relatively large values indicating natural choice for practice such gap often not seen probabilistic pca figure synthetic data illustrating the algorithm for pca ned and data set with the data points shown green together with the true principal components shown eigenvectors scaled the square roots the eigenvalues initial con guration the principal subspace ned shown red together with the projections the latent points into the data space given zwt shown cyan after one step the latent space has been updated with held xed after the successive step the values have been updated giving orthogonal projections with held xed after the second step after the second step section because the probabilistic pca model has well ned likelihood function could employ cross validation determine the value dimensionality selecting the largest log likelihood validation data set such approach however can become computationally costly particularly consider probabilistic mixture pca models tipping and bishop which seek determine the appropriate dimensionality separately for each component the mixture given that have probabilistic formulation pca seems natural seek bayesian approach model selection this need marginalize out the model parameters and with respect appropriate prior distributions this can done using variational framework approximate the analytically intractable marginalizations bishop the marginal likelihood values given the variational lower bound can then compared for range different values and the value giving the largest marginal likelihood selected here consider simpler approach introduced based the evidence continuous latent variables figure probabilistic graphical model for bayesian pca which the distribution over the parameter matrix governed vector hyperparameters proximation which appropriate when the number data points relatively large and the corresponding posterior distribution tightly peaked bishop involves speci choice prior over that allows surplus dimensions the principal subspace pruned out the model this corresponds example automatic relevance determination ard discussed section speci cally independent gaussian prior over each column which represent the vectors ning the principal subspace each such gaussian has independent variance governed precision hyperparameter that cid cid cid cid cid exp iwt where the ith column the resulting model can represented using the directed graph shown figure the values for will found iteratively maximizing the marginal likelihood function which has been integrated out result this optimization some the may driven nity with the corresponding parameters vector being driven zero the posterior distribution becomes delta function the origin giving sparse solution the effective dimensionality the principal subspace then determined the number nite values and the corresponding vectors can thought relevant for modelling the data distribution this way the bayesian approach automatically making the trade off between improving the the data using larger number vectors with their corresponding eigenvalues each tuned the data and reducing the complexity the model suppressing some the vectors the origins this sparsity were discussed earlier the context relevance vector machines the values are estimated during training maximizing the log marginal likelihood given where the log given note that for simplicity also treat and parameters estimated rather than ning priors over these parameters cid section probabilistic pca section section because this integration intractable make use the laplace approximation assume that the posterior distribution sharply peaked will occur for suf ciently large data sets then the estimation equations obtained maximizing the marginal likelihood with respect take the simple form new which follows from noting that the dimensionality these reestimations are interleaved with the algorithm updates for determining and the step equations are again given and similarly the mstep equation for again given the only change the step equation for which modi give cid cid cid cid wnew znzt where diag the value given the sample mean before choose then all values are nite the model represents full covariance gaussian while all the nity the model equivalent isotropic gaussian and the model can encompass all permissible values for the effective dimensionality the principal subspace also possible consider smaller values which will save computational cost but which will limit the maximum dimensionality the subspace comparison the results this algorithm with standard probabilistic pca shown figure bayesian pca provides opportunity illustrate the gibbs sampling algorithm discussed section figure shows example the samples from the hyperparameters for data set dimensions which the dimensionality the latent space but which the data set generated from probabilistic pca model having one direction high variance with the remaining directions comprising low variance noise this result shows clearly the presence three distinct modes the posterior distribution each step the iteration one the hyperparameters has small value and the remaining two have large values that two the three latent variables are suppressed during the course the gibbs sampling the solution makes sharp transitions between the three modes the model described here involves prior only over the matrix fully bayesian treatment pca including priors over and and solved using variational methods described bishop for discussion various bayesian approaches determining the appropriate dimensionality for pca model see minka"}, "230": {"Section": "12.2.4", "Title": "Factor analysis", "Content": "factor analysis linear gaussian latent variable model that closely related probabilistic pca its nition differs from that probabilistic pca only that the conditional distribution the observed variable given the latent variable continuous latent variables figure hinton diagrams the matrix which each element the matrix depicted square white for positive and black for negative values whose area proportional the magnitude that element the synthetic data set comprises data points dimensions sampled from gaussian distribution having standard deviation directions and standard deviation the remaining directions for data set dimensions having directions with larger variance than the remaining directions the left hand plot shows the result from maximum likelihood probabilistic pca and the left hand plot shows the corresponding result from bayesian pca see how the bayesian model able discover the appropriate dimensionality suppressing the surplus degrees freedom taken have diagonal rather than isotropic covariance that where diagonal matrix note that the factor analysis model common with probabilistic pca assumes that the observed variables are independent given the latent variable essence the factor analysis model explaining the observed covariance structure the data representing the independent variance associated with each coordinate the matrix and capturing the covariance between variables the matrix the factor analysis literature the columns which capture the correlations between observed variables are called factor loadings and the diagonal elements which represent the independent noise variances for each the variables are called uniquenesses the origins factor analysis are old those pca and discussions factor analysis can found the books everitt bartholomew and basilevsky links between factor analysis and pca were investigated lawley and anderson who showed that stationary points the likelihood function for factor analysis model with the columns are scaled eigenvectors the sample covariance matrix and the average the discarded eigenvalues later tipping and bishop showed that the maximum the log likelihood function occurs when the eigenvectors comprising are chosen the principal eigenvectors making use see that the marginal distribution for the observed probabilistic pca figure gibbs sampling for bayesian pca showing plots for versus iteration number three values showing transitions the three modes the posterior distribution between exercise section exercise variable given where now wwt with probabilistic pca this model invariant rotations the latent space historically factor analysis has been the subject controversy when attempts have been made place interpretation the individual factors the coordinates space which has proven problematic due the nonidenti ability factor analysis associated with rotations this space from our perspective however shall view factor analysis form latent variable density model which the form the latent space interest but not the particular choice coordinates used describe wish remove the degeneracy associated with latent space rotations must consider non gaussian latent variable distributions giving rise independent component analysis ica models can determine the parameters and the factor analysis model maximum likelihood the solution for again given the sample mean however unlike probabilistic pca there longer closed form maximum likelihood solution for which must therefore found iteratively because factor analysis latent variable model this can done using algorithm rubin and thayer that analogous the one used for probabilistic pca speci cally the step equations are given gwt znzt where have ned note that this expressed form that involves inversion matrices size rather than except for the diagonal matrix whose inverse trivial continuous latent variables exercise exercise compute steps which convenient because often cid similarly the step equations take the form wnew new diag wnew znzt cid cid cid cid cid cid cid where the diag operator sets all the nondiagonal elements matrix zero bayesian treatment the factor analysis model can obtained straightforward application the techniques discussed this book another difference between probabilistic pca and factor analysis concerns their different behaviour under transformations the data set for pca and probabilistic pca rotate the coordinate system data space then obtain exactly the same the data but with the matrix transformed the corresponding rotation matrix however for factor analysis the analogous property that make component wise scaling the data vectors then this absorbed into corresponding scaling the elements"}, "231": {"Section": "12.3", "Title": "Kernel PCA", "Content": "chapter saw how the technique kernel substitution allows take algorithm expressed terms scalar products the form xtx cid and generalize that algorithm replacing the scalar products with nonlinear kernel here apply this technique kernel substitution principal component analysis thereby obtaining nonlinear generalization called kernel pca sch olkopf consider data set observations where space cid dimensionality order keep the notation uncluttered shall assume that have already subtracted the sample mean from each the vectors that the rst step express conventional pca such form that the data vectors appear only the form the scalar products nxm recall that the principal components are ned the eigenvectors the covariance matrix where here the sample covariance matrix ned sui iui cid xnxt and the eigenvectors are normalized such that now consider nonlinear transformation into dimensional feature space that each data point thereby projected onto point can kernel pca figure schematic illustration kernel pca data set the original data space left hand plot projected nonlinear transformation into feature space right hand plot performing pca the feature space obtain the principal components which the rst shown blue and denoted the vector the green lines feature space indicate the linear projections onto the rst principal component which correspond nonlinear projections the original data space note that general not possible represent the nonlinear principal component vector space cid now perform standard pca the feature space which implicitly nes nonlinear principal component model the original data space illustrated figure for the moment let assume that the projected data set also has zero mean shall return this point shortly the sample that covariance matrix feature space given cid and its eigenvector expansion ned our goal solve this eigenvalue problem without having work explicitly the feature space from the nition the eigenvector equations tells that satis cvi ivi tvi ivi and see that provided the vector given linear combination the and can written the form cid cid cid cid ain continuous latent variables exercise substituting this expansion back into the eigenvector equation obtain aim ain the key step now express this terms the kernel function which multiplying both sides give cid cid cid cid cid cid aimk aink this can written matrix notation inkai where dimensional column vector with elements ani for can solutions for solving the following eigenvalue problem kai inai which have removed factor from both sides note that the solutions and differ only eigenvectors having zero eigenvalues that not affect the principal components projection the normalization condition for the coef cients obtained requiring that the eigenvectors feature space normalized using and have ainaim kai inat cid cid having solved the eigenvector problem the resulting principal component projections can then also cast terms the kernel function that using the projection point onto eigenvector given cid cid tvi ain aink and again expressed terms the kernel function the original dimensional space there are orthogonal eigenvectors and hence can most linear principal components the dimensionality the feature space however can much larger than even nite and thus can number nonlinear principal components that can exceed note however that the number nonzero eigenvalues cannot exceed the number data points because even the covariance matrix feature space has rank most equal this ected the fact that kernel pca involves the eigenvector expansion the matrix kernel pca far have assumed that the projected data set given has zero mean which general will not the case cannot simply compute and then subtract off the mean since wish avoid working directly feature space and again formulate the algorithm purely terms the kernel function the projected data points after centralizing denoted cid are given and the corresponding elements the gram matrix are given cid cid cid knm cid cid cid cid cid cid cid cid cid cid this can expressed matrix notation cid where denotes the matrix which every element takes the value thus can evaluate cid using only the kernel function and then use cid determine exercise the eigenvalues and eigenvectors note that the standard pca algorithm recovered special case use linear kernel cid xtx cid figure shows example kernel pca applied synthetic data set sch olkopf here gaussian kernel the form cid exp cid cid cid applied synthetic data set the lines correspond contours along which the projection onto the corresponding principal component ned cid tvi aink constant continuous latent variables figure example kernel pca with gaussian kernel applied synthetic data set two dimensions showing the rst eight eigenfunctions along with their eigenvalues the contours are lines along which the projection onto the corresponding principal component constant note how the rst two eigenvectors separate the three clusters the next three eigenvectors split each the cluster into halves and the following three eigenvectors again split the clusters into halves along directions orthogonal the previous splits one obvious disadvantage kernel pca that involves nding the eigenvectors the matrix cid rather than the matrix conventional linear cid onto the dimensional principal subspace ned finally note that standard linear pca often retain some reduced number eigenvectors and then approximate data vector its projection pca and practice for large data sets approximations are often used nui kernel pca this will general not possible see this note that the mapping maps the dimensional space into dimensional manifold the dimensional feature space the vector known the pre image the corresponding point however the projection points feature space onto the linear pca subspace that space will typically not lie the nonlinear ddimensional manifold and will not have corresponding pre image data space techniques have therefore been proposed for nding approximate pre images bakir cid cid cid cid"}, "232": {"Section": "12.4", "Title": "Nonlinear Latent Variable Models", "Content": "nonlinear latent variable models this chapter have focussed the simplest class models having continuous latent variables namely those based linear gaussian distributions well having great practical importance these models are relatively easy analyse and data and can also used components more complex models here consider brie some generalizations this framework models that are either nonlinear non gaussian both fact the issues nonlinearity and non gaussianity are related because general probability density can obtained from simple xed reference density such gaussian making nonlinear change variables this idea forms the basis several practical latent variable models shall see shortly exercise"}, "233": {"Section": "12.4.1", "Title": "Independent component analysis", "Content": "begin considering models which the observed variables are related linearly the latent variables but for which the latent distribution non gaussian important class such models known independent component analysis ica arises when consider distribution over the latent variables that factorizes that cid understand the role such models consider situation which two people are talking the same time and record their voices using two microphones ignore effects such time delay and echoes then the signals received the microphones any point time will given linear combinations the amplitudes the two voices the coef cients this linear combination will constant and can infer their values from sample data then can invert the mixing process assuming nonsingular and thereby obtain two clean signals each which contains the voice just one person this example problem called blind source separation which blind refers the fact that are given only the mixed data and neither the original sources nor the mixing coef cients are observed cardoso this type problem sometimes addressed using the following approach mackay which ignore the temporal nature the signals and treat the successive samples consider generative model which there are two latent variables corresponding the unobserved speech signal amplitudes and there are two observed variables given the signal values the microphones the latent variables have joint distribution that factorizes above and the observed variables are given linear combination the latent variables there need include noise distribution because the number latent variables equals the number observed variables and therefore the marginal distribution the observed variables will not general singular the observed variables are simply deterministic linear combinations the latent variables given data set observations the continuous latent variables exercise likelihood function for this model function the coef cients the linear combination the log likelihood can maximized using gradient based optimization giving rise particular version independent component analysis the success this approach requires that the latent variables have non gaussian distributions see this recall that probabilistic pca and factor analysis the latent space distribution given zero mean isotropic gaussian the model therefore cannot distinguish between two different choices for the latent variables where these differ simply rotation latent space this can veri directly noting that the marginal density and hence the likelihood function unchanged make the transformation where orthogonal matrix satisfying rrt because the matrix given itself invariant extending the model allow more general gaussian latent distributions does not change this conclusion because have seen such model equivalent the zero mean isotropic gaussian latent variable model another way see why gaussian latent variable distribution linear model insuf cient independent components note that the principal components represent rotation the coordinate system data space such diagonalize the covariance matrix that the data distribution the new coordinates then uncorrelated although zero correlation necessary condition for independence not however suf cient practice common choice for the latent variable distribution given cosh ezj which has heavy tails compared gaussian ecting the observation that many real world distributions also exhibit this property the original ica model bell and sejnowski was based the optimization objective function ned information maximization one advantage probabilistic latent variable formulation that helps motivate and formulate generalizations basic ica for instance independent factor analysis attias considers model which the number latent and observed variables can differ the observed variables are noisy and the individual latent variables have exible distributions modelled mixtures gaussians the log likelihood for this model maximized using and the reconstruction the latent variables approximated using variational approach many other types model have been considered and there now huge literature ica and its applications jutten and herault comon amari pearlmutter and parra hyv arinen and oja hinton miskin and mackay hojen sorensen choudrey and roberts chan stone"}, "234": {"Section": "12.4.2", "Title": "Autoassociative neural networks", "Content": "chapter considered neural networks the context supervised learning where the role the network predict the output variables given values nonlinear latent variable models figure autoassociative multilayer perceptron having two layers weights such network trained map input vectors onto themselves minimization sum squares error even with nonlinear units the hidden layer such network equivalent linear principal component analysis links representing bias parameters have been omitted for clarity inputs outputs for the input variables however neural networks have also been applied unsupervised learning where they have been used for dimensionality reduction this achieved using network having the same number outputs inputs and optimizing the weights minimize some measure the reconstruction error between inputs and outputs with respect set training data consider rst multilayer perceptron the form shown figure having inputs output units and hidden units with the targets used train the network are simply the input vectors themselves that the network attempting map each input vector onto itself such network said form autoassociative mapping since the number hidden units smaller than the number inputs perfect reconstruction all input vectors not general possible therefore determine the network parameters minimizing error function which captures the degree mismatch between the input vectors and their reconstructions particular shall choose sum squares error the form cid cid cid the hidden units have linear activations functions then can shown that the error function has unique global minimum and that this minimum the network performs projection onto the dimensional subspace which spanned the rst principal components the data bourlard and kamp baldi and hornik thus the vectors weights which lead into the hidden units figure form basis set which spans the principal subspace note however that these vectors need not orthogonal normalized this result unsurprising since both principal component analysis and the neural network are using linear dimensionality reduction and are minimizing the same sum squares error function might thought that the limitations linear dimensionality reduction could overcome using nonlinear sigmoidal activation functions for the hidden units the network figure however even with nonlinear hidden units the minimum error solution again given the projection onto the principal component subspace bourlard and kamp there therefore advantage using twolayer neural networks perform dimensionality reduction standard techniques for principal component analysis based singular value decomposition are guaranteed give the correct solution nite time and they also generate ordered set eigenvalues with corresponding orthonormal eigenvectors continuous latent variables figure addition extra hidden layers nonlinear units gives autoassociative network which can perform nonlinear dimensionality reduction inputs non linear outputs the situation different however additional hidden layers are permitted the network consider the four layer autoassociative network shown figure again the output units are linear and the units the second hidden layer can also linear however the rst and third hidden layers have sigmoidal nonlinear activation functions the network again trained minimization the error function can view this network two successive functional mappings and indicated figure the rst mapping projects the original ddimensional data onto dimensional subspace ned the activations the units the second hidden layer because the presence the rst hidden layer nonlinear units this mapping very general and particular not restricted being linear similarly the second half the network nes arbitrary functional mapping from the dimensional space back into the original dimensional input space this has simple geometrical interpretation indicated for the case and figure such network effectively performs nonlinear principal component analysis figure geometrical interpretation the mappings performed the network figure for the case inputs and units the middle hidden layer the function maps from dimensional space into dimensional space and therefore nes the way which the space embedded within the original space since the mapping can nonlinear the embedding can nonplanar indicated the gure the mapping then nes projection points the original dimensional space into the dimensional subspace nonlinear latent variable models has the advantage not being limited linear transformations although contains standard principal component analysis special case however training the network now involves nonlinear optimization problem since the error function longer quadratic function the network parameters computationally intensive nonlinear optimization techniques must used and there the risk nding suboptimal local minimum the error function also the dimensionality the subspace must speci before training the network"}, "235": {"Section": "12.4.3", "Title": "Modelling nonlinear manifolds", "Content": "have already noted many natural sources data correspond lowdimensional possibly noisy nonlinear manifolds embedded within the higher dimensional observed data space capturing this property explicitly can lead improved density modelling compared with more general methods here consider brie range techniques that attempt this one way model the nonlinear structure through combination linear models that make piece wise linear approximation the manifold this can obtained for instance using clustering technique such means based euclidean distance partition the data set into local groups with standard pca applied each group better approach use the reconstruction error for cluster assignment kambhatla and leen hinton then common cost function being optimized each stage however these approaches still suffer from limitations due the absence overall density model using probabilistic pca straightforward fully probabilistic model simply considering mixture distribution which the components are probabilistic pca models tipping and bishop such model has both discrete latent variables corresponding the discrete mixture well continuous latent variables and the likelihood function can maximized using the algorithm fully bayesian treatment based variational inference bishop and winn allows the number components the mixture well the effective dimensionalities the individual models inferred from the data there are many variants this model which parameters such the matrix the noise variances are tied across components the mixture which the isotropic noise distributions are replaced diagonal ones giving rise mixture factor analysers ghahramani and hinton ghahramani and beal the mixture probabilistic pca models can also extended hierarchically produce interactive data visualization algorithm bishop and tipping alternative considering mixture linear models consider single nonlinear model recall that conventional pca nds linear subspace that passes close the data least squares sense this concept can extended onedimensional nonlinear surfaces the form principal curves hastie and stuetzle can describe curve dimensional data space using vector valued function which vector each whose elements function the scalar there are many possible ways parameterize the curve which natural choice the arc length along the curve for any given point cid data space can the point the curve that closest euclidean distance denote this point continuous latent variables because depends the particular curve for continuous data density principal curve ned one for which every point the curve the mean all those points data space that project that for given continuous density there can many principal curves practice are interested nite data sets and also wish restrict attention smooth curves hastie and stuetzle propose two stage iterative procedure for nding such principal curves somewhat reminiscent the algorithm for pca the curve initialized using the rst principal component and then the algorithm alternates between data projection step and curve estimation step the projection step each data point assigned value corresponding the closest point the curve then the estimation step each point the curve given weighted average those points that project nearby points the curve with points closest the curve given the greatest weight the case where the subspace constrained linear the procedure converges the rst principal component and equivalent the power method for nding the largest eigenvector the covariance matrix principal curves can generalized multidimensional manifolds called principal surfaces although these have found limited use due the dif culty data smoothing higher dimensions even for two dimensional manifolds pca often used project data set onto lower dimensional space for example two dimensional for the purposes visualization another linear technique with similar aim multidimensional scaling mds cox and cox nds low dimensional projection the data such preserve closely possible the pairwise distances between data points and involves nding the eigenvectors the distance matrix the case where the distances are euclidean gives equivalent results pca the mds concept can extended wide variety data types speci terms similarity matrix giving nonmetric mds two other nonprobabilistic methods for dimensionality reduction and data visualization are worthy mention locally linear embedding lle roweis and saul rst computes the set coef cients that best reconstructs each data point from its neighbours these coef cients are arranged invariant rotations translations and scalings that data point and its neighbours and hence they characterize the local geometrical properties the neighbourhood lle then maps the high dimensional data points down lower dimensional space while preserving these neighbourhood coef cients the local neighbourhood for particular data point can considered linear then the transformation can achieved using combination translation rotation and scaling such preserve the angles formed between the data points and their neighbours because the weights are invariant these transformations expect the same weight values reconstruct the data points the low dimensional space the high dimensional data space spite the nonlinearity the optimization for lle does not exhibit local minima isometric feature mapping isomap tenenbaum the goal project the data lower dimensional space using mds but where the dissimilarities are ned terms the geodesic distances measured along the mani nonlinear latent variable models fold for instance two points lie circle then the geodesic the arc length distance measured around the circumference the circle not the straight line distance measured along the chord connecting them the algorithm rst nes the neighbourhood for each data point either nding the nearest neighbours nding all points within sphere radius graph then constructed linking all neighbouring points and labelling them with their euclidean distance the geodesic distance between any pair points then approximated the sum the arc lengths along the shortest path connecting them which itself found using standard algorithms finally metric mds applied the geodesic distance matrix the low dimensional projection our focus this chapter has been models for which the observed variables are continuous can also consider models having continuous latent variables together with discrete observed variables giving rise latent trait models bartholomew this case the marginalization over the continuous latent variables even for linear relationship between latent and observed variables cannot performed analytically and more sophisticated techniques are required tipping uses variational inference model with two dimensional latent space allowing binary data set visualized analogously the use pca visualize continuous data note that this model the dual the bayesian logistic regression problem discussed section the case logistic regression have observations the feature vector which are parameterized single parameter vector whereas the latent space visualization model there single latent space variable analogous and copies the latent variable generalization probabilistic latent variable models general exponential family distributions described collins have already noted that arbitrary distribution can formed taking gaussian random variable and transforming through suitable nonlinearity this exploited general latent variable model called density network mackay mackay and gibbs which the nonlinear function governed multilayered neural network the network has enough hidden units can approximate given nonlinear function any desired accuracy the downside having such exible model that the marginalization over the latent variables required order obtain the likelihood function longer analytically tractable instead the likelihood approximated using monte carlo techniques drawing samples from the gaussian prior the marginalization over the latent variables then becomes simple sum with one term for each sample however because large number sample points may required order give accurate representation the marginal this procedure can computationally costly consider more restricted forms for the nonlinear function and make appropriate choice the latent variable distribution then can construct latent variable model that both nonlinear and cient train the generative topographic mapping gtm bishop bishop bishop uses latent distribution that ned nite regular grid delta functions over the typically two dimensional latent space marginalization over the latent space then simply involves summing over the contributions from each the grid locations chapter chapter continuous latent variables figure plot the oil data set visualized using pca the left and gtm the right for the gtm model each data point plotted the mean its posterior distribution latent space the nonlinearity the gtm model allows the separation between the groups data points seen more clearly chapter section the nonlinear mapping given linear regression model that allows for general nonlinearity while being linear function the adaptive parameters note that the usual limitation linear regression models arising from the curse dimensionality does not arise the context the gtm since the manifold generally has two dimensions irrespective the dimensionality the data space consequence these two choices that the likelihood function can expressed analytically closed form and can optimized ciently using the algorithm the resulting gtm model two dimensional nonlinear manifold the data set and evaluating the posterior distribution over latent space for the data points they can projected back the latent space for visualization purposes figure shows comparison the oil data set visualized with linear pca and with the nonlinear gtm the gtm can seen probabilistic version earlier model called the self organizing map som kohonen kohonen which also represents two dimensional nonlinear manifold regular array discrete points the som somewhat reminiscent the means algorithm that data points are assigned nearby prototype vectors that are then subsequently updated initially the prototypes are distributed random and during the training process they self organize approximate smooth manifold unlike means however the som not optimizing any well ned cost function erwin making dif cult set the parameters the model and assess convergence there also guarantee that the self organization will take place this dependent the choice appropriate parameter values for any particular data set contrast gtm optimizes the log likelihood function and the resulting model nes probability density data space fact corresponds constrained mixture gaussians which the components share common variance and the means are constrained lie smooth two dimensional manifold this proba"}, "236": {"Section": "13", "Title": "Sequential Data", "Content": "far this book have focussed primarily sets data points that were assumed independent and identically distributed this assumption allowed express the likelihood function the product over all data points the probability distribution evaluated each data point for many applications however the assumption will poor one here consider particularly important class such data sets namely those that describe sequential data these often arise through measurement time series for example the rainfall measurements successive days particular location the daily values currency exchange rate the acoustic features successive time frames used for speech recognition example involving speech data shown figure sequential data can also arise contexts other than time series for example the sequence nucleotide base pairs along strand dna the sequence characters english sentence for convenience shall sometimes refer past and future observations sequence however the models explored this chapter are equally applicable all sequential data figure example spectrogram the spoken words bayes theorem showing plot the intensity the spectral coefficients versus time index forms sequential data not just temporal sequences useful distinguish between stationary and nonstationary sequential distributions the stationary case the data evolves time but the distribution from which generated remains the same for the more complex nonstationary situation the generative distribution itself evolving with time here shall focus the stationary case for many applications such financial forecasting wish able predict the next value time series given observations the previous values intuitively expect that recent observations are likely more informative than more historical observations predicting future values the example figure shows that successive observations the speech spectrum are indeed highly correlated furthermore would impractical consider general dependence future observations all previous observations because the complexity such model would grow without limit the number observations increases this leads consider markov models which assume that future predictions are inden pendent all but the most recent observations although such models are tractable they are also severely limited can obtain more general framework while still retaining tractability the introduction latent variables leading state space models chapters and shall see that complex models can thereby constructed from simpler components particular from distributions belonging the exponential family and can readily characterized using the framework probabilistic graphical models here focus the two most important examples state space models namely the hidden markov model which the latent variables are discrete and linear dynamical systems which the latent variables are gaussian both models are described directed graphs having tree structure loops for which inference can performed efficiently using the sum product algorithm"}, "237": {"Section": "13.1", "Title": "Markov Models", "Content": "figure the simplest approach modelling sequence observations treat them independent corresponding graph without links markov models the easiest way treat sequential data would simply ignore the sequential aspects and treat the observations corresponding the graph figure such approach however would fail exploit the sequential patterns the data such correlations between observations that are close the sequence suppose for instance that observe binary variable denoting whether particular day rained not given time series recent observations this variable wish predict whether will rain the next day treat the data then the only information can glean from the data the relative frequency rainy days however know practice that the weather often exhibits trends that may last for several days observing whether not rains today therefore significant help predicting will rain tomorrow express such effects probabilistic model need relax the assumption and one the simplest ways this consider markov model first all note that without loss generality can use the product rule express the joint distribution for sequence observations the form now assume that each the conditional distributions the right hand side independent all previous observations except the most recent obtain the first order markov chain which depicted graphical model figure the sequential data figure first order markov chain observations which the distribution particular observation conditioned the value the previous observation section exercise joint distribution for sequence observations under this model given from the separation property see that the conditional distribution for observation given all the observations time given which easily verified direct evaluation starting from and using the product rule probability thus use such model predict the next observation sequence the distribution predictions will depend only the value the immediately preceding observation and will independent all earlier observations most applications such models the conditional distributions that define the model will constrained equal corresponding the assumption stationary time series the model then known homogeneous markov chain for instance the conditional distributions depend adjustable parameters whose values might inferred from set training data then all the conditional distributions the chain will share the same values those parameters although this more general than the independence model still very restrictive for many sequential observations anticipate that the trends the data over several successive observations will provide important information predicting the next value one way allow earlier observations have influence move higher order markov chains allow the predictions depend also the previous but one value obtain second order markov chain represented the graph figure the joint distribution now given again using separation direct evaluation see that the conditional distribution given and independent all observations figure second order markov chain which the conditional distribution particular observation depends the values the two previous observations and figure can represent sequential data using markov chain latent variables with each observation conditioned the state the corresponding latent variable this important graphical structure forms the foundation both for the hidden markov model and for linear dynamical systems markov models each observation now influenced two previous observations can similarly consider extensions order markov chain which the conditional distribution for particular variable depends the previous variables however have paid price for this increased flexibility because the number parameters the model now much larger suppose the observations are discrete variables having states then the conditional distribution first order markov chain will specified set parameters for each the states giving total parameters now suppose extend the model order markov chain that the joint distribution built from conditionals the variables are discrete and the conditional distributions are represented general conditional probability tables then the number parameters such model will have parameters because this grows exponentially with will often render this approach impractical for larger values for continuous variables can use linear gaussian conditional distributions which each node has gaussian distribution whose mean linear function its parents this known autoregressive model box thiesson alternative approach use parametric model for such neural network this technique sometimes called tapped delay line because corresponds storing delaying the previous values the observed variable order predict the next value the number parameters can then much smaller than completely general model for example may grow linearly with although this achieved the expense restricted family conditional distributions suppose wish build model for sequences that not limited the markov assumption any order and yet that can specified using limited number free parameters can achieve this introducing additional latent variables permit rich class models constructed out simple components did with mixture distributions chapter and with continuous latent variable models chapter for each observation introduce corresponding latent variable which may different type dimensionality the observed variable now assume that the latent variables that form markov chain giving rise the graphical structure known state space model which shown figure satisfies the key conditional independence property that and are independent given that sequential data the joint distribution for this model given using the separation criterion see that there always path connecting any two observed variables and via the latent variables and that this path never blocked thus the predictive distribution for observation given all previous observations does not exhibit any conditional independence properties and our predictions for depends all previous observations the observed variables however not satisfy the markov property any order shall discuss how evaluate the predictive distribution later sections this chapter there are two important models for sequential data that are described this graph the latent variables are discrete then obtain the hidden markov model hmm elliott note that the observed variables hmm may discrete continuous and variety different conditional distributions can used model them both the latent and the observed variables are gaussian with linear gaussian dependence the conditional distributions their parents then obtain the linear dynamical system section section"}, "238": {"Section": "13.2", "Title": "Hidden Markov Models", "Content": "the hidden markov model can viewed specific instance the state space model figure which the latent variables are discrete however examine single time slice the model see that corresponds mixture distribution with component densities given can therefore also interpreted extension mixture model which the choice mixture component for each observation not selected independently but depends the choice component for the previous observation the hmm widely used speech recognition jelinek rabiner and juang natural language modelling manning and sch utze line handwriting recognition nag and for the analysis biological sequences such proteins and dna krogh durbin baldi and brunak the case standard mixture model the latent variables are the discrete multinomial variables describing which component the mixture responsible for generating the corresponding observation again convenient use coding scheme used for mixture models chapter now allow the probability distribution depend the state the previous latent variable through conditional distribution because the latent variables are dimensional binary variables this conditional distribution corresponds table numbers that denote the elements which are known transition probabilities they are given ajk znk and because they are probabilities they satisfy ajk with ajk that the matrix figure transition diagram showing model whose latent variables have three possible states corresponding the three boxes the black lines denote the elements the transition matrix ajk hidden markov models has independent parameters can then write the conditional distribution explicitly the form azn znk the initial latent node special that does not have parent node and has marginal distribution represented vector probabilities with elements that where the transition matrix sometimes illustrated diagrammatically drawing the states nodes state transition diagram shown figure for the case note that this does not represent probabilistic graphical model because the nodes are not separate variables but rather states single variable and have shown the states boxes rather than circles section sometimes useful take state transition diagram the kind shown figure and unfold over time this gives alternative representation the transitions between latent states known lattice trellis diagram and which shown for the case the hidden markov model figure the specification the probabilistic model completed defining the conditional distributions the observed variables where set parameters governing the distribution these are known emission probabilities and might for example given gaussians the form the elements are continuous variables conditional probability tables discrete because observed the distribution consists for given value vector numbers corresponding the possible states the binary vector sequential data figure unfold the state transition diagram figure over time obtain lattice trellis representation the latent states each column this diagram corresponds one the latent variables can represent the emission probabilities the form znk shall focuss attention homogeneous models for which all the conditional distributions governing the latent variables share the same parameters and similarly all the emission distributions share the same parameters the extension more general cases straightforward note that mixture model for data set corresponds the special case which the parameters ajk are the same for all values that the conditional distribution independent this corresponds deleting the horizontal links the graphical model shown figure the joint probability distribution over both latent and observed variables then given where and denotes the set parameters governing the model most our discussion the hidden markov model will independent the particular choice the emission probabilities indeed the model tractable for wide range emission distributions including discrete tables gaussians and mixtures gaussians also possible exploit discriminative models such neural networks these can used model the emission density directly provide representation for that can converted into the required emission density using bayes theorem bishop can gain better understanding the hidden markov model considering from generative point view recall that generate samples from mixture exercise hidden markov models figure illustration sampling from hidden markov model having state latent variable and gaussian emission model where dimensional contours constant probability density for the emission distributions corresponding each the three states the latent variable sample points drawn from the hidden markov model colour coded according the component that generated them and with lines connecting the successive observations here the transition matrix was fixed that any state there probability making transition each the other states and consequently probability remaining the same state gaussians first chose one the components random with probability given the mixing coefficients and then generate sample vector from the corresponding gaussian component this process repeated times generate data set independent samples the case the hidden markov model this procedure modified follows first choose the initial latent variable with probabilities governed the parameters and then sample the corresponding observation now choose the state the variable according the transition probabilities using the already instantiated value thus suppose that the sample for corresponds state then choose the state with probabilities ajk for once know can draw sample for and also sample the next latent variable and this example ancestral sampling for directed graphical model for instance have model which the diagonal transition elements akk are much larger than the off diagonal elements then typical data sequence will have long runs points generated from single component with infrequent transitions from one component another the generation samples from hidden markov model illustrated figure there are many variants the standard hmm model obtained for instance imposing constraints the form the transition matrix rabiner here mention one particular practical importance called the left right hmm which obtained setting the elements ajk zero illustrated the section sequential data figure example the state transition diagram for state left right hidden markov model note that once state has been vacated cannot later entered state transition diagram for state hmm figure typically for such models the initial state probabilities for are modified that and for other words every sequence constrained start state the transition matrix may further constrained ensure that large changes the state index not occur that ajk this type model illustrated using lattice diagram figure many applications hidden markov models for example speech recognition line character recognition make use left right architectures illustration the left right hidden markov model consider example involving handwritten digits this uses line data meaning that each digit represented the trajectory the pen function time the form sequence pen coordinates contrast the off line digits data discussed appendix which comprises static two dimensional pixellated images the ink examples the online digits are shown figure here train hidden markov model subset data comprising examples the digit there are states each which can generate line segment fixed length having one possible angles and the emission distribution simply table probabilities associated with the allowed angle values for each state index value transition probabilities are all set zero except for those that keep the state index the same that increment and the model parameters are optimized using iterations can gain some insight into the resulting model running generatively shown figure figure lattice diagram for state leftto right hmm which the state index allowed increase most each transition hidden markov models figure top row examples line handwritten digits bottom row synthetic digits sampled generatively from left right hidden markov model that has been trained data set handwritten digits one the most powerful properties hidden markov models their ability exhibit some degree invariance local warping compression and stretching the time axis understand this consider the way which the digit written the line handwritten digits example typical digit comprises two distinct sections joined cusp the first part the digit which starts the top left has sweeping arc down the cusp loop the bottom left followed second moreor less straight sweep ending the bottom right natural variations writing style will cause the relative sizes the two sections vary and hence the location the cusp loop within the temporal sequence will vary from generative perspective such variations can accommodated the hidden markov model through changes the number transitions the same state versus the number transitions the successive state note however that digit written the reverse order that starting the bottom right and ending the top left then even though the pen tip coordinates may identical example from the training set the probability the observations under the model will extremely small the speech recognition context warping the time axis associated with natural variations the speed speech and again the hidden markov model can accommodate such distortion and not penalize too heavily"}, "239": {"Section": "13.2.1", "Title": "Maximum likelihood for the HMM", "Content": "have observed data set can determine the parameters hmm using maximum likelihood the likelihood function obtained from the joint distribution marginalizing over the latent variables because the joint distribution does not factorize over contrast the mixture distribution considered chapter cannot simply treat each the summations over independently nor can perform the summations explicitly because there are variables summed over each which has states resulting total terms thus the number terms the summation grows sequential data section exponentially with the length the chain fact the summation corresponds summing over exponentially many paths through the lattice diagram figure have already encountered similar difficulty when considered the inference problem for the simple chain variables figure there were able make use the conditional independence properties the graph order the summations order obtain algorithm whose cost scales linearly instead exponentially with the length the chain shall apply similar technique the hidden markov model further difficulty with the expression for the likelihood function that because corresponds generalization mixture distribution represents summation over the emission models for different settings the latent variables direct maximization the likelihood function will therefore lead complex expressions with closed form solutions was the case for simple mixture models recall that mixture model for data special case the hmm therefore turn the expectation maximization algorithm find efficient framework for maximizing the likelihood function hidden markov models the algorithm starts with some initial selection for the model parameters which denote old the step take these parameter values and find the posterior distribution the latent variables old then use this posterior distribution evaluate the expectation the logarithm the complete data likelihood function function the parameters give the function old defined old old this point convenient introduce some notation shall use denote the marginal posterior distribution latent variable and denote the joint posterior distribution two successive latent variables that old old for each value can store using set nonnegative numbers that sum unity and similarly can store using matrix nonnegative numbers that again sum unity shall also use znk denote the conditional probability znk with similar use notation for znk and for other probabilistic variables introduced later because the expectation binary random variable just the probability that takes the value have znk znk znk znk jznk jznk substitute the joint distribution given into hidden markov models and make use the definitions and obtain old znk ajk znk the goal the step will evaluate the quantities and efficiently and shall discuss this detail shortly the step maximize old with respect the parameters which treat and constant maximization with respect and easily achieved using appropriate lagrange multipliers with the results ajk znk znl the algorithm must initialized choosing starting values for and which should course respect the summation constraints associated with their probabilistic interpretation note that any elements that are set zero initially will remain zero subsequent updates typical initialization procedure would involve selecting random starting values for these parameters subject the summation and non negativity constraints note that particular modification the results are required for the case left right models beyond choosing initial values for the elements ajk which the appropriate elements are set zero because these will remain zero throughout maximize old with respect notice that only the final term depends and furthermore this term has exactly the same form the data dependent term the corresponding function for standard mixture distribution for data can seen comparison with for the case gaussian mixture here the quantities znk are playing the role the responsibilities the parameters are independent for the different components then this term decouples into sum terms one for each value each which can maximized independently are then simply maximizing the weighted log likelihood function for the emission density with weights znk here shall suppose that this maximization can done efficiently for instance the case exercise exercise znk znk znk znk for the case discrete multinomial observed variables the conditional distribution the observations takes the form xizk and the corresponding step equations are given znk xni znk analogous result holds for bernoulli observed variables the algorithm requires initial values for the parameters the emission distribution one way set these first treat the data initially and fit the emission density maximum likelihood and then use the resulting values initialize the parameters for"}, "240": {"Section": "13.2.2", "Title": "The forward-backward algorithm", "Content": "next seek efficient procedure for evaluating the quantities znk and znk corresponding the step the algorithm the graph for the hidden markov model shown figure tree and know that the posterior distribution the latent variables can obtained efficiently using twostage message passing algorithm the particular context the hidden markov model this known the forward backward algorithm rabiner the baum welch algorithm baum there are fact several variants the basic algorithm all which lead the exact marginals according the precise form sequential data gaussian emission densities have and maximization the function old then gives exercise section hidden markov models the messages that are propagated along the chain jordan shall focus the most widely used these known the alpha beta algorithm well being great practical importance its own right the forwardbackward algorithm provides with nice illustration many the concepts introduced earlier chapters shall therefore begin this section with conventional derivation the forward backward equations making use the sum and product rules probability and exploiting conditional independence properties which shall obtain from the corresponding graphical model using separation then section shall see how the forward backward algorithm can obtained very simply specific example the sum product algorithm introduced section worth emphasizing that evaluation the posterior distributions the latent variables independent the form the emission density indeed whether the observed variables are continuous discrete all require the values the quantities for each value for every also this section and the next shall omit the explicit dependence the model parameters old because these fixed throughout therefore begin writing down the following conditional independence properties jordan where these relations are most easily proved using separation for instance the first these results note that every path from any one the nodes the node passes through the node which observed because all such paths are head tail follows that the conditional independence property must hold the reader should take few moments verify each these properties turn exercise the application separation these relations can also proved directly though with significantly greater effort from the joint distribution for the hidden markov model using the sum and product rules probability let begin evaluating znk recall that for discrete multinomial random variable the expected value one its components just the probability that component having the value thus are interested finding the posterior distribution given the observed data set this exercise sequential data represents vector length whose entries correspond the expected values znk using bayes theorem have note that the denominator implicitly conditioned the parameters old the hmm and hence represents the likelihood function using the conditional independence property together with the product rule probability obtain where have defined the quantity represents the joint probability observing all the given data time and the value whereas represents the conditional probability all future data from time given the value again and each represent set numbers one for each the possible settings the coded binary vector shall use the notation znk denote the value when znk with analogous interpretation znk now derive recursion relations that allow and evaluated efficiently again shall make use conditional independence properties particular and together with the sum and product rules allowing express terms follows making use the definition for then obtain figure illustration the forward recursion for evaluation the variables this fragment the lattice see that the quantity obtained taking the elements step and summing them with weights given corresponding the values and then multiplying the data contribution hidden markov models worth taking moment study this recursion relation some detail note that there are terms the summation and the right hand side has evaluated for each the values each step the recursion has computational cost that scaled like the forward recursion equation for illustrated using lattice diagram figure order start this recursion need initial condition that given which tells that for takes the value starting the first node the chain can then work along the chain and evaluate for every latent node because each step the recursion involves multiplying matrix the overall cost evaluating these quantities for the whole chain can similarly find recursion relation for the quantities making use the conditional independence properties and giving sequential data figure illustration the backward recursion for evaluation the variables this fragment the lattice see that the quantity obtained taking the components step and summing them with weights given the products corresponding the values and the corresponding values the emission density making use the definition for then obtain note that this case have backward message passing algorithm that evaluates terms each step absorb the effect observation through the emission probability multiply the transition matrix and then marginalize out this illustrated figure again need starting condition for the recursion namely value for this can obtained setting and replacing with its definition give which see will correct provided take for all settings the step equations the quantity will cancel out can seen for instance the step equation for given which takes the form znk znk znk znk znk znk however the quantity represents the likelihood function whose value typically wish monitor during the optimization and useful able evaluate sum both sides over and use the fact that the left hand side normalized distribution obtain hidden markov models thus can evaluate the likelihood function computing this sum for any convenient choice for instance only want evaluate the likelihood function then can this running the recursion from the start the end the chain and then use this result for making use the fact that vector this case recursion required and simply have let take moment interpret this result for recall that compute the likelihood should take the joint distribution and sum over all possible values each such value represents particular choice hidden state for every time step other words every term the summation path through the lattice diagram and recall that there are exponentially many such paths expressing the likelihood function the form have reduced the computational cost from being exponential the length the chain being linear swapping the order the summation and multiplications that each time step sum the contributions from all paths passing through each the states znk give the intermediate quantities next consider the evaluation the quantities which correspond the values the conditional probabilities for each the settings for using the definition and applying bayes theorem have where have made use the conditional independence property together with the definitions and given and thus can calculate the directly using the results the and recursions let summarize the steps required train hidden markov model using the algorithm first make initial selection the parameters old where the and parameters are often initialized either uniformly randomly from uniform distribution respecting their non negativity and summation constraints initialization the parameters will depend the form the distribution for instance the case gaussians the parameters might initialized applying the means algorithm the data and might initialized the covariance matrix the corresponding means cluster then run both the forward recursion and the backward recursion and use the results evaluate and this stage can also evaluate the likelihood function which can evaluated first running forward recursion and then computing the final summations over and the result the first summation over can stored and used once the value observed order run the recursion forward the next step order predict the subsequent value sequential data this completes the step and use the results find revised set parameters new using the step equations from section then continue alternate between and steps until some convergence criterion satisfied for instance when the change the likelihood function below some threshold note that these recursion relations the observations enter through conditional distributions the form the recursions are therefore independent the type dimensionality the observed variables the form this conditional distribution long its value can computed for each the possible states since the observed variables are fixed the quantities can pre computed functions the start the algorithm and remain fixed throughout have seen earlier chapters that the maximum likelihood approach most effective when the number data points large relation the number parameters here note that hidden markov model can trained effectively using maximum likelihood provided the training sequence sufficiently long alternatively can make use multiple shorter sequences which requires straightforward modification the hidden markov model algorithm the case left right models this particularly important because given observation sequence given state transition corresponding nondiagonal element will seen most once another quantity interest the predictive distribution which the observed data and wish predict which would important for real time applications such financial forecasting again make use the sum and product rules together with the conditional independence properties and giving exercise figure fragment the factor graph representation for the hidden markov model hidden markov models section section note that the influence all data from summarized the values thus the predictive distribution can carried forward indefinitely using fixed amount storage may required for real time applications here have discussed the estimation the parameters hmm using maximum likelihood this framework easily extended regularized maximum likelihood introducing priors over the model parameters and whose values are then estimated maximizing their posterior probability this can again done using the algorithm which the step the same discussed above and the step involves adding the log the prior distribution the function old before maximization and represents straightforward application the techniques developed various points this book furthermore can use variational methods give fully bayesian treatment the hmm which marginalize over the parameter distributions mackay with maximum likelihood this leads two pass forward backward recursion compute posterior probabilities"}, "241": {"Section": "13.2.3", "Title": "The sum-product algorithm for the HMM", "Content": "the directed graph that represents the hidden markov model shown figure tree and can solve the problem finding local marginals for the hidden variables using the sum product algorithm not surprisingly this turns out equivalent the forward backward algorithm considered the previous section and the sum product algorithm therefore provides with simple way derive the alpha beta recursion formulae begin transforming the directed graph figure into factor graph which representative fragment shown figure this form the factor graph shows all variables both latent and observed explicitly however for the purpose solving the inference problem shall always conditioning the variables and can simplify the factor graph absorbing the emission probabilities into the transition probability factors this leads the simplified factor graph representation figure which the factors are given sequential data figure simplified form factor graph describe the hidden markov model derive the alpha beta algorithm denote the final hidden variable the root node and first pass messages from the leaf node the root from the general results and for message propagation see that the messages which are propagated the hidden markov model take the form these equations represent the propagation messages forward along the chain and are equivalent the alpha recursions derived the previous section shall now show note that because the variable nodes have only two neighbours they perform computation can eliminate from using give recursion for the messages the form now recall the definition and define then obtain the alpha recursion given also need verify that the quantities are themselves equivalent those defined previously this easily done using the initial condition and noting that given which identical because the initial the same and because they are iteratively computed using the same equation all subsequent quantities must the same next consider the messages that are propagated from the root node back the leaf node these take the form where before have eliminated the messages the type since the variable nodes perform computation using the definition substitute for and defining hidden markov models obtain the beta recursion given again can verify that the beta variables themselves are equivalent noting that implies that the initial message send the root variable node which identical the initialization given section the sum product algorithm also specifies how evaluate the marginals once all the messages have been evaluated particular the result shows that the local marginal the node given the product the incoming messages because have conditioned the variables are computing the joint distribution dividing both sides then obtain exercise agreement with the result can similarly derived from"}, "242": {"Section": "13.2.4", "Title": "Scaling factors", "Content": "there important issue that must addressed before can make use the forward backward algorithm practice from the recursion relation note that each step the new value obtained from the previous value multiplying quantities and because these probabilities are often significantly less than unity work our way forward along the chain the values can zero exponentially quickly for moderate lengths chain say the calculation the will soon exceed the dynamic range the computer even double precision floating point used the case data implicitly circumvented this problem with the evaluation likelihood functions taking logarithms unfortunately this will not help here because are forming sums products small numbers are fact implicitly summing over all possible paths through the lattice diagram figure therefore work with scaled versions and whose values remain order unity shall see the corresponding scaling factors cancel out when use these scaled quantities the algorithm defined representing the joint distribution all the observations and the latent variable now define normalized version given which expect well behaved numerically because probability distribution over variables for any value order relate the scaled and original alpha variables introduce scaling factors defined conditional distributions over the observed variables sequential data from the product rule then have and can then turn the recursion equation for into one for given note that each stage the forward message passing phase used evaluate have evaluate and store which easily done because the coefficient that normalizes the right hand side give can similarly define scaled variables using which will again remain within machine precision because from the quantities are simply the ratio two conditional probabilities the recursion result for then gives the following recursion for the scaled variables applying this recursion relation make use the scaling factors that were previously computed the phase from see that the likelihood function can found using exercise similarly using and together with see that the required marginals are given section hidden markov models instead using finally note that there alternative formulation the forward backward algorithm jordan which the backward pass defined recursion based the quantities this recursion requires that the forward pass completed first that all the quantities are available for the backward pass whereas the forward and backward passes the algorithm can done independently although these two algorithms have comparable computational cost the version the most commonly encountered one the case hidden markov models whereas for linear dynamical systems recursion analogous the form more usual"}, "243": {"Section": "13.2.5", "Title": "The Viterbi algorithm", "Content": "many applications hidden markov models the latent variables have some meaningful interpretation and often interest find the most probable sequence hidden states for given observation sequence for instance speech recognition might wish find the most probable phoneme sequence for given series acoustic observations because the graph for the hidden markov model directed tree this problem can solved exactly using the max sum algorithm recall from our discussion section that the problem finding the most probable sequence latent states not the same that finding the set states that are individually the most probable the latter problem can solved first running the forward backward sum product algorithm find the latent variable marginals and then maximizing each these individually duda however the set such states will not general correspond the most probable sequence states fact this set states might even represent sequence having zero probability happens that two successive states which isolation are individually the most probable are such that the transition matrix element connecting them zero practice are usually interested finding the most probable sequence states and this can solved efficiently using the max sum algorithm which the context hidden markov models known the viterbi algorithm viterbi note that the max sum algorithm works with log probabilities and there need use scaled variables was done with the forward backward algorithm figure shows fragment the hidden markov model expanded lattice diagram have already noted the number possible paths through the lattice grows exponentially with the length the chain the viterbi algorithm searches this space paths efficiently find the most probable path with computational cost that grows only linearly with the length the chain with the sum product algorithm first represent the hidden markov model factor graph shown figure again treat the variable node the root and pass messages the root starting with the leaf nodes using the results and see that the messages passed the max sum algorithm are given max sequential data figure fragment the hmm lattice showing two possible paths the viterbi algorithm efficiently determines the most probable path from amongst the exponentially many possibilities for any given path the corresponding probability given the product the elements the transition matrix ajk corresponding the probabilities for each segment the path along with the emission densities associated with each node the path eliminate between these two equations and make use obtain recursion for the messages the form max where have introduced the notation from and these messages are initialized using where have used note that keep the notation uncluttered omit the dependence the model parameters that are held fixed when finding the most probable sequence exercise the viterbi algorithm can also derived directly from the definition the joint distribution taking the logarithm and then exchanging maximizations and summations easily seen that the quantities have the probabilistic interpretation max once have completed the final maximization over will obtain the value the joint distribution corresponding the most probable path also wish find the sequence latent variable values that corresponds this path this simply make use the back tracking procedure discussed section specifically note that the maximization over must performed for each the possible values suppose keep record the values that correspond the maxima for each value the values let denote this function where once have passed messages the end the chain and found the most probable state can then use this function backtrack along the chain applying recursively kmax kmax hidden markov models intuitively can understand the viterbi algorithm follows naively could consider explicitly all the exponentially many paths through the lattice evaluate the probability for each and then select the path having the highest probability however notice that can make dramatic saving computational cost follows suppose that for each path evaluate its probability summing products transition and emission probabilities work our way forward along each path through the lattice consider particular time step and particular state that time step there will many possible paths converging the corresponding node the lattice diagram however need only retain that particular path that far has the highest probability because there are states time step need keep track such paths time step there will possible paths consider comprising possible paths leading out each the current states but again need only retain these corresponding the best path for each state time when reach the final time step will discover which state corresponds the overall most probable path because there unique path coming into that state can trace the path back step see what state occupied that time and back through the lattice the state"}, "244": {"Section": "13.2.6", "Title": "Extensions of the hidden Markov model", "Content": "the basic hidden markov model along with the standard training algorithm based maximum likelihood has been extended numerous ways meet the requirements particular applications here discuss few the more important examples see from the digits example figure that hidden markov models can quite poor generative models for the data because many the synthetic digits look quite unrepresentative the training data the goal sequence classification there can significant benefit determining the parameters hidden markov models using discriminative rather than maximum likelihood techniques suppose have training set observation sequences where each which labelled according its class where for each class have separate hidden markov model with its own parameters and treat the problem determining the parameter values standard classification problem which optimize the cross entropy where the prior probability class optimization this cost function more complex than for maximum likelihood kapadia and particular using bayes theorem this can expressed terms the sequence probabilities associated with the hidden markov models sequential data figure section autoregressive hidden markov model which the distribution the observation depends subset the previous observations well the hidden state this example the distribution depends the two previous observations and requires that every training sequence evaluated under each the models order compute the denominator hidden markov models coupled with discriminative training methods are widely used speech recognition kapadia significant weakness the hidden markov model the way which represents the distribution times for which the system remains given state see the problem note that the probability that sequence sampled from given hidden markov model will spend precisely steps state and then make transition different state given akk akk exp akk and exponentially decaying function for many applications this will very unrealistic model state duration the problem can resolved modelling state duration directly which the diagonal coefficients akk are all set zero and each state explicitly associated with probability distribution possible duration times from generative point view when state entered value representing the number time steps that the system will remain state then drawn from the model then emits values the observed variable which are generally assumed independent that the corresponding emist this approach requires some straightforward sion density simply modifications the optimization procedure rabiner another limitation the standard hmm that poor capturing longrange correlations between the observed variables between variables that are separated many time steps because these must mediated via the first order markov chain hidden states longer range effects could principle included adding extra links the graphical model figure one way address this generalize the hmm give the autoregressive hidden markov model ephraim example which shown figure for discrete observations this corresponds expanded tables conditional probabilities for the emission distributions the case gaussian emission density can use the lineargaussian framework which the conditional distribution for given the values the previous observations and the value gaussian whose mean linear combination the values the conditioning variables clearly the number additional links the graph must limited avoid excessive the number free parameters the example shown figure each observation depends figure example input output hidden markov model this case both the emission probabilities and the transition probabilities depend the values sequence observations hidden markov models the two preceding observed variables well the hidden state although this graph looks messy can again appeal separation see that fact still has simple probabilistic structure particular imagine conditioning see that with the standard hmm the values and are independent corresponding the conditional independence property this easily verified noting that every path from node node passes through least one observed node that head tail with respect that path consequence can again use forward backward recursion the step the algorithm determine the posterior distributions the latent variables computational time that linear the length the chain similarly the step involves only minor modification the standard step equations the case gaussian emission densities this involves estimating the parameters using the standard linear regression equations discussed chapter have seen that the autoregressive hmm appears natural extension the standard hmm when viewed graphical model fact the probabilistic graphical modelling viewpoint motivates plethora different graphical structures based the hmm another example the input output hidden markov model bengio and frasconi which have sequence observed variables addition the output variables whose values influence either the distribution latent variables output variables both example shown figure this extends the hmm framework the domain supervised learning for sequential data again easy show through the use the separation criterion that the markov property for the chain latent variables still holds verify this simply note that there only one path from node node and this head tail with respect the observed node this conditional independence property again allows the formulation computationally efficient learning algorithm particular can determine the parameters the model maximizing the likelihood function where matrix whose rows are given consequence the conditional independence property this likelihood function can maximized efficiently using algorithm which the step involves forward and backward recursions another variant the hmm worthy mention the factorial hidden markov model ghahramani and jordan which there are multiple independent exercise sequential data figure factorial hidden markov model comprising two markov chains latent variables for continuous observed variables one possible choice emission model linear gaussian density which the mean the gaussian linear combination the states the corresponding latent variables markov chains latent variables and the distribution the observed variable given time step conditional the states all the corresponding latent variables that same time step figure shows the corresponding graphical model the motivation for considering factorial hmm can seen noting that order represent say bits information given time step standard hmm would need latent states whereas factorial hmm could make use binary latent chains the primary disadvantage factorial hmms however lies the additional complexity training them the step for the factorial hmm model straightforward however observation the variables introduces dependencies between the latent chains leading difficulties with the step this can seen noting that figure the variables are connected path which head head node and hence they are not separated the exact step for this model does not correspond running forward and backward recursions along the markov chains independently this confirmed noting that the key conditional independence property not satisfied for the individual markov chains the factorial hmm model shown using separation figure now suppose that there are chains hidden nodes and for simplicity suppose that all latent variables have the same number states then one approach would note that there are combinations latent variables given time step and figure example path highlighted green which head head the observed nodes and and head tail the unobserved nodes and thus the path not blocked and the conditional independence property does not hold for the individual latent chains the factorial hmm model consequence there efficient exact step for this model section"}, "245": {"Section": "13.3", "Title": "Linear Dynamical Systems", "Content": "and can transform the model into equivalent standard hmm having single chain latent variables each which has latent states can then run the standard forward backward recursions the step this has computational complexity that exponential the number latent chains and will intractable for anything other than small values one solution would use sampling methods discussed chapter elegant deterministic alternative ghahramani and jordan exploited variational inference techniques obtain tractable algorithm for approximate inference this can done using simple variational posterior distribution that fully factorized with respect the latent variables alternatively using more powerful approach which the variational distribution described independent markov chains corresponding the chains latent variables the original model the latter case the variational inference algorithms involves running independent forward and backward recursions along each chain which computationally efficient and yet also able capture correlations between variables within the same chain clearly there are many possible probabilistic structures that can constructed according the needs particular applications graphical models provide general technique for motivating describing and analysing such structures and variational methods provide powerful framework for performing inference those models for which exact solution intractable linear dynamical systems order motivate the concept linear dynamical systems let consider the following simple problem which often arises practical settings suppose wish measure the value unknown quantity using noisy sensor that returns observation representing the value plus zero mean gaussian noise given single measurement our best guess for assume that however can improve our estimate for taking lots measurements and averaging them because the random noise terms will tend cancel each other now let make the situation more complicated assuming that wish measure quantity that changing over time can take regular measurements that some point time have obtained and wish find the corresponding values simply average the measurements the error due random noise will reduced but unfortunately will just obtain single averaged estimate which have averaged over the changing value thereby introducing new source error intuitively could imagine doing bit better follows estimate the value take only the most recent few measurements say and just average these changing slowly and the random noise level the sensor high would make sense choose relatively long window observations average conversely the signal changing quickly and the noise levels are small might better just use directly our estimate perhaps could even better take weighted average which more recent measurements sequential data make greater contribution than less recent ones although this sort intuitive argument seems plausible does not tell how form weighted average and any sort hand crafted weighing hardly likely optimal fortunately can address problems such this much more systematically defining probabilistic model that captures the time evolution and measurement processes and then applying the inference and learning methods developed earlier chapters here shall focus widely used model known linear dynamical system have seen the hmm corresponds the state space model shown figure which the latent variables are discrete but with arbitrary emission probability distributions this graph course describes much broader class probability distributions all which factorize according now consider extensions other distributions for the latent variables particular consider continuous latent variables which the summations the sum product algorithm become integrals the general form the inference algorithms will however the same for the hidden markov model interesting note that historically hidden markov models and linear dynamical systems were developed independently once they are both expressed graphical models however the deep relationship between them immediately becomes apparent one key requirement that retain efficient algorithm for inference which linear the length the chain this requires that for instance when take representing the posterior probability given observations quantity and multiply the transition probability and the emission probability and then marginalize over obtain distribution over that say the that the same functional form that over distribution must not become more complex each stage but must only change its parameter values not surprisingly the only distributions that have this property being closed under multiplication are those belonging the exponential family here consider the most important example from practical perspective which the gaussian particular consider linear gaussian state space model that the latent variables well the observed variables are multivariate gaussian distributions whose means are linear functions the states their parents the graph have seen that directed graph linear gaussian units equivalent joint gaussian distribution over all the variables furthermore are also gaussian that the functional form the mesmarginals such sages preserved and will obtain efficient inference algorithm contrast suppose that the emission densities comprise mixture gaussians gaussian the each which has mean that linear then even will mixture quantity gaussians and and exact inference will not practical value will mixture gaussians have seen that the hidden markov model can viewed extension the mixture models chapter allow for sequential correlations the data similar way can view the linear dynamical system generalization the continuous latent variable models chapter such probabilistic pca and factor analysis each pair nodes represents linear gaussian latent variable linear dynamical systems model for that particular observation however the latent variables are longer treated independent but now form markov chain because the model represented tree structured directed graph inference problems can solved efficiently using the sum product algorithm the forward recursions analogous the messages the hidden markov model are known the kalman filter equations kalman zarchan and musoff and the backward recursions analogous the messages are known the kalman smoother equations the rauch tung striebel rts equations rauch the kalman filter widely used many real time tracking applications because the linear dynamical system linear gaussian model the joint distribution over all variables well all marginals and conditionals will gaussian follows that the sequence individually most probable latent variable values the same the most probable latent sequence there thus need consider the analogue the viterbi algorithm for the linear dynamical system because the model has linear gaussian conditional distributions can write the transition and emission distributions the general form azn czn the initial latent variable also has gaussian distribution which write exercise exercise note that order simplify the notation have omitted additive constant terms from the means the gaussians fact straightforward include them desired traditionally these distributions are more commonly expressed equivalent form terms noisy linear equations given azn czn where the noise terms have the distributions the parameters the model denoted can determined using maximum likelihood through the algorithm the step need solve the inference problem determining the local posterior marginals for the latent variables which can solved efficiently using the sum product algorithm discuss the next section sequential data"}, "246": {"Section": "13.3.1", "Title": "Inference in LDS", "Content": "now turn the problem finding the marginal distributions for the latent variables conditional the observation sequence for given parameter settings also wish make predictions the next latent state and the next observation conditioned the observed data for use real time applications these inference problems can solved efficiently using the sum product algorithm which the context the linear dynamical system gives rise the kalman filter and kalman smoother equations worth emphasizing that because the linear dynamical system lineargaussian model the joint distribution over all latent and observed variables simply gaussian and principle could solve inference problems using the standard results derived previous chapters for the marginals and conditionals multivariate gaussian the role the sum product algorithm provide more efficient way perform such computations linear dynamical systems have the identical factorization given hidden markov models and are again described the factor graphs figures and inference algorithms therefore take precisely the same form except that summations over latent variables are replaced integrations begin considering the forward equations which treat the root node and propagate messages from the leaf node the root from the initial message will gaussian and because each the factors gaussian all subsequent messages will also gaussian convention shall propagate messages that are normalized marginal distributions corresponding which denote given this precisely analogous the propagation scaled variables the discrete case the hidden markov model and the recursion equation now takes the form dzn substituting for the conditionals and using and respectively and making use see that becomes cnn czn azn dzn here are supposing that and are known and evaluating the integral wish determine values for and the integral easily evaluated making use the result from which follows that azn dzn linear dynamical systems where have defined avn can now combine this result with the first factor the right hand side making use and give knc cpn here have made use the matrix inverse identities and and also defined the kalman gain matrix thus given the values and together with the new observation can evaluate the gaussian marginal for having mean and covariance well the normalization coefficient cpn the initial conditions for these recursion equations are obtained from because given and given can again make use calculate and calculate and giving where similarly the likelihood function for the linear dynamical system given which the factors are found using the kalman filtering equations can interpret the steps involved going from the posterior marginal over the posterior marginal over follows can view the quantity the prediction the mean over obtained simply taking the mean over and projecting forward one step using the transition probability matrix this predicted mean would give predicted observation for given cazn obtained applying the emission probability matrix the predicted hidden state mean can view the update equation for the mean the hidden variable distribution taking the predicted mean and then adding correction that proportional the error cazn between the predicted observation and the actual observation the coefficient this correction given the kalman gain matrix thus can view the kalman filter process making successive predictions and then correcting these predictions the light the new observations this illustrated graphically figure sequential data figure the linear dynamical system can viewed sequence steps which increasing uncertainty the state variable due diffusion compensated the arrival new data the left hand plot the blue curve shows the distribution which incorporates all the data step the diffusion arising from the nonzero variance the transition probability gives the distribution shown red the centre plot note that this broader and shifted relative the blue curve which shown dashed the centre plot for comparison the next data observation contributes through the emission density which shown function green the right hand plot note that this not density with respect and not normalized one inclusion this new data point leads revised distribution for the state density shown blue see that observation the data has shifted and narrowed the distribution compared which shown dashed the right hand plot for comparison exercise exercise consider situation which the measurement noise small compared the rate which the latent variable evolving then find that the posterior distribution for depends only the current measurement accordance with the intuition from our simple example the start the section similarly the latent variable evolving slowly relative the observation noise level find that the posterior mean for obtained averaging all the measurements obtained that time one the most important applications the kalman filter tracking and this illustrated using simple example object moving two dimensions figure far have solved the inference problem finding the posterior marginal for node given observations from next turn the problem finding the marginal for node given all observations for temporal data this corresponds the inclusion future well past observations although this cannot used for real time prediction plays key role learning the parameters the model analogy with the hidden markov model this problem can solved propagating messages from node back node and combining this information with that obtained during the forward message passing stage used compute the the lds literature usual formulate this backward recursion terms because must also rather than terms gaussian write the form derive the required recursion start from the backward recursion for linear dynamical systems figure illustration linear dynamical system being used track moving object the blue points indicate the true positions the object two dimensional space successive time steps the green points denote noisy measurements the positions and the red crosses indicate the means the inferred posterior distributions the positions obtained running the kalman filtering equations the covariances the inferred positions are indicated the red ellipses which correspond contours having one standard deviation which for continuous latent variables can written the form dzn and substitute for now multiply both sides and using and then make use and together with and after some manipulation obtain where have defined and have made use avn pnjt note that these recursions require that the forward pass completed first that the quantities and will available for the backward pass vnat for the algorithm also require the pairwise posterior marginals which can obtained from the form azn czn using and rearranging see that substituting for gaussian with mean given with components and and covariance between and given cov exercise exercise sequential data"}, "247": {"Section": "13.3.2", "Title": "Learning in LDS", "Content": "far have considered the inference problem for linear dynamical systems assuming that the model parameters are known next consider the determination these parameters using maximum likelihood ghahramani and hinton because the model has latent variables this can addressed using the algorithm which was discussed general terms chapter can derive the algorithm for the linear dynamical system follows let denote the estimated parameter values some particular cycle the algorithm old for these parameter values can run the inference algorithm determine the posterior distribution the latent variables old more precisely those local posterior marginals that are required the step particular shall require the following expectations znzt znzt where have used now consider the complete data log likelihood function which obtained taking the logarithm and therefore given which have made the dependence the parameters explicit now take the expectation the complete data log likelihood with respect the posterior distribution old which defines the function old old the step this function maximized with respect the components consider first the parameters and substitute for using and then take the expectation with respect obtain old old const where all terms not dependent have been absorbed into the additive constant maximization with respect and easily performed making use the maximum likelihood solution for gaussian distribution discussed section giving exercise new vnew xne xne znzt linear dynamical systems similarly optimize and substitute for using giving old old azn azn const which the constant comprises terms that are independent and maximizing with respect these parameters then gives anew new znzt znzt anewe anew anewe anew note that anew must evaluated first and the result can then used determine new finally order determine the new values and substitute for using giving old old czn czn const maximizing with respect and then gives cnew new znzt xnxt cnewe cnew cnewe znzt cnew exercise exercise chapter sequential data have approached parameter learning the linear dynamical system using maximum likelihood inclusion priors give map estimate straightforward and fully bayesian treatment can found applying the analytical approximation techniques discussed chapter though detailed treatment precluded here due lack space"}, "248": {"Section": "13.3.3", "Title": "Extensions of LDS", "Content": "with the hidden markov model there considerable interest extending the basic linear dynamical system order increase its capabilities although the assumption linear gaussian model leads efficient algorithms for inference and learning also implies that the marginal distribution the observed variables simply gaussian which represents significant limitation one simple extension the linear dynamical system use gaussian mixture the initial distribution for this mixture has components then the forward recursion equations will lead mixture gaussians over each hidden variable and the model again tractable for many applications the gaussian emission density poor approximation instead try use mixture gaussians the emission density then the will also mixture gaussians however from the posterior will comprise mixture gaussians and with posterior being given mixture gaussians thus the number components grows exponentially with the length the chain and this model impractical more generally introducing transition emission models that depart from the linear gaussian other exponential family model leads intractable inference problem can make deterministic approximations such assumed density filtering expectation propagation can make use sampling methods discussed section one widely used approach make gaussian approximation linearizing around the mean the predicted distribution which gives rise the extended kalman filter zarchan and musoff with hidden markov models can develop interesting extensions the basic linear dynamical system expanding its graphical representation for example the switching state space model ghahramani and hinton can viewed combination the hidden markov model with set linear dynamical systems the model has multiple markov chains continuous linear gaussian latent variables each which analogous the latent chain the linear dynamical system discussed earlier together with markov chain discrete variables the form used hidden markov model the output each time step determined stochastically choosing one the continuous latent chains using the state the discrete latent variable switch and then emitting observation from the corresponding conditional output distribution exact inference this model intractable but variational methods lead efficient inference scheme involving forward backward recursions along each the continuous and discrete markov chains independently note that consider multiple chains discrete latent variables and use one the switch select from the remainder obtain analogous model having only discrete latent variables known the switching hidden markov model chapter linear dynamical systems"}, "249": {"Section": "13.3.4", "Title": "Particle filters", "Content": "for dynamical systems which not have linear gaussian for example they use non gaussian emission density can turn sampling methods order find tractable inference algorithm particular can apply the samplingimportance resampling formalism section obtain sequential monte carlo algorithm known the particle filter consider the class distributions represented the graphical model figure and suppose are given the observed values and wish draw samples from the posterior distribution using bayes theorem have dzn dzn dzn dzn where set samples drawn from and have made use the conditional independence property which follows from the graph figure the sampling weights are defined where the same samples are used the numerator the denominator thus the posterior distribution represented the set samples together note that these weights satisfy with the corresponding weights and because wish find sequential sampling scheme shall suppose that set samples and weights have been obtained time step and that have subsequently observed the value and wish find the weights and samples time step first sample from the distribution this dzn dzn dzn dzn dzn sequential data straightforward since again using bayes theorem where have made use the conditional independence properties which follow from the application the separation criterion the graph figure the distribution given mixture distribution and samples can drawn choosing component with probability given the mixing coefficients and then drawing sample from the corresponding component summary can view each step the particle filter algorithm comprising two stages time step have sample representation the posterior disn with corresponding weights tribution expressed samples this can viewed mixture representation the form obtain the corresponding representation for the next time step first draw samples from the mixture distribution and then for each sample use the new observation evaluate the corresponding weights this illustrated for the case single variable figure the particle filtering sequential monte carlo approach has appeared the literature under various names including the bootstrap filter gordon survival the fittest kanazawa and the condensation algorithm isard and blake"}, "250": {"Section": "14", "Title": "Combining Models", "Content": "earlier chapters have explored range different models for solving classification and regression problems often found that improved performance can obtained combining multiple models together some way instead just using single model isolation for instance might train different models and then make predictions using the average the predictions made each model such combinations models are sometimes called committees section discuss ways apply the committee concept practice and also give some insight into why can sometimes effective procedure one important variant the committee method known boosting involves training multiple models sequence which the error function used train particular model depends the performance the previous models this can produce substantial improvements performance compared the use single model and discussed section instead averaging the predictions set models alternative form model combination select one the models make the prediction which the choice model function the input variables thus different models become responsible for making predictions different regions input space one widely used framework this kind known decision tree which the selection process can described sequence binary selections corresponding the traversal tree structure and discussed section this case the individual models are generally chosen very simple and the overall flexibility the model arises from the input dependent selection process decision trees can applied both classification and regression problems one limitation decision trees that the division input space based hard splits which only one model responsible for making predictions for any given value the input variables the decision process can softened moving probabilistic framework for combining models discussed section for example have set models for conditional distribution where the input variable the target variable and indexes the model then can form probabilistic mixture the form which represent the input dependent mixing coefficients such models can viewed mixture distributions which the component densities well the mixing coefficients are conditioned the input variables and are known mixtures experts they are closely related the mixture density network model discussed section combining models"}, "251": {"Section": "14.1", "Title": "Bayesian Model Averaging", "Content": "section important distinguish between model combination methods and bayesian model averaging the two are often confused understand the difference consider the example density estimation using mixture gaussians which several gaussian components are combined probabilistically the model contains binary latent variable that indicates which component the mixture responsible for generating the corresponding data point thus the model specified terms joint distribution and the corresponding density over the observed variable obtained marginalizing over the latent variable the case our gaussian mixture example this leads distribution the form"}, "252": {"Section": "14.2", "Title": "Committees", "Content": "with the usual interpretation the symbols this example model combination for independent identically distributed data can use write the marginal probability data set the form thus see that each observed data point has corresponding latent variable now suppose have several different models indexed with prior probabilities for instance one model might mixture gaussians and another model might mixture cauchy distributions the marginal distribution over the data set given this example bayesian model averaging the interpretation this summation over that just one model responsible for generating the whole data set and the probability distribution over simply reflects our uncertainty which model that the size the data set increases this uncertainty reduces and the posterior probabilities become increasingly focussed just one the models this highlights the key difference between bayesian model averaging and model combination because bayesian model averaging the whole data set generated single model contrast when combine multiple models see that different data points within the data set can potentially generated from different values the latent variable and hence different components although have considered the marginal probability the same considerations apply for the predictive density for conditional distributions such exercise committees section the simplest way construct committee average the predictions set individual models such procedure can motivated from frequentist perspective considering the trade off between bias and variance which decomposes the error due model into the bias component that arises from differences between the model and the true function predicted and the variance component that represents the sensitivity the model the individual data points recall from figure combining models that when trained multiple polynomials using the sinusoidal data and then averaged the resulting functions the contribution arising from the variance term tended cancel leading improved predictions when averaged set low bias models corresponding higher order polynomials obtained accurate predictions for the underlying sinusoidal function from which the data were generated practice course have only single data set and have find way introduce variability between the different models within the committee one approach use bootstrap data sets discussed section consider regression problem which are trying predict the value single continuous variable and suppose generate bootstrap data sets and then use each train separate copy predictive model where the committee prediction given ycom this procedure known bootstrap aggregation bagging breiman suppose the true regression function that are trying predict given that the output each the models can written the true value plus error the form the average sum squares error then takes the form where denotes frequentist expectation with respect the distribution the input vector the average error made the models acting individually therefore eav similarly the expected error from the committee given ecom assume that the errors have zero mean and are uncorrelated that exercise then obtain"}, "253": {"Section": "14.3", "Title": "Boosting", "Content": "ecom eav this apparently dramatic result suggests that the average error model can reduced factor simply averaging versions the model unfortunately depends the key assumption that the errors due the individual models are uncorrelated practice the errors are typically highly correlated and the reduction overall error generally small can however shown that the expected committee error will not exceed the expected error the constituent models that ecom eav order achieve more significant improvements turn more sophisticated technique for building committees known boosting exercise boosting boosting powerful technique for combining multiple base classifiers produce form committee whose performance can significantly better than that any the base classifiers here describe the most widely used form boosting algorithm called adaboost short for adaptive boosting developed freund and schapire boosting can give good results even the base classifiers have performance that only slightly better than random and hence sometimes the base classifiers are known weak learners originally designed for solving classification problems boosting can also extended regression friedman the principal difference between boosting and the committee methods such bagging discussed above that the base classifiers are trained sequence and each base classifier trained using weighted form the data set which the weighting coefficient associated with each data point depends the performance the previous classifiers particular points that are misclassified one the base classifiers are given greater weight when used train the next classifier the sequence once all the classifiers have been trained their predictions are then combined through weighted majority voting scheme illustrated schematically figure consider two class classification problem which the training data comprises input vectors along with corresponding binary target variables where each data point given associated weighting parameter which initially set for all data points shall suppose that have procedure available for training base classifier using weighted data give function each stage the algorithm adaboost trains new classifier using data set which the weighting coefficients are adjusted according the performance the previously trained classifier give greater weight the misclassified data points finally when the desired number base classifiers have been trained they are combined form committee using coefficients that give different weight different base classifiers the precise form the adaboost algorithm given below mym combining models figure schematic illustration the boosting framework each base classifier trained weighted form the training set blue arrows which the weights depend the performance the previous base classifier green arrows once all base classifiers have been trained they are combined give the final classifier red arrows adaboost sign initialize the data weighting coefficients setting for for fit classifier the training data minimizing the weighted error function where the indicator function and equals when and otherwise evaluate the quantities and then use these evaluate update the data weighting coefficients exp boosting make predictions using the final model which given sign mym see that the first base classifier trained using weighting coefficients that are all equal which therefore corresponds the usual procedure for training single classifier from see that subsequent iterations the weighting coefficients are increased for data points that are misclassified and decreased for data points that are correctly classified successive classifiers are therefore forced place greater emphasis points that have been misclassified previous classifiers and data points that continue misclassified successive classifiers receive ever greater weight the quantities represent weighted measures the error rates each the base classifiers the data set therefore see that the weighting coefficients defined give greater weight the more accurate classifiers when computing the overall output given the adaboost algorithm illustrated figure using subset data points taken from the toy classification data set shown figure here each base learners consists threshold one the input variables this simple classifier corresponds form decision tree known decision stumps decision tree with single node thus each base learner classifies input according whether one the input features exceeds some threshold and therefore simply partitions the space into two regions separated linear decision surface that parallel one the axes"}, "254": {"Section": "14.3.1", "Title": "Minimizing exponential error", "Content": "boosting was originally motivated using statistical learning theory leading upper bounds the generalization error however these bounds turn out too loose have practical value and the actual performance boosting much better than the bounds alone would suggest friedman gave different and very simple interpretation boosting terms the sequential minimization exponential error function consider the exponential error function defined exp tnfm where classifier defined terms linear combination base classifiers the form lyl and are the training set target values our goal minimize with respect both the weighting coefficients and the parameters the base classifiers section combining models figure illustration boosting which the base learners consist simple thresholds applied one other the axes each figure shows the number base learners trained far along with the decision boundary the most recent base learner dashed black line and the combined decision boundary the ensemble solid green line each data point depicted circle whose radius indicates the weight assigned that data point when training the most recently added base learner thus for instance see that points that are misclassified the base learner are given greater weight when training the base learner instead doing global error function minimization however shall suppose that the base classifiers are fixed are their coefficients and are minimizing only with respect and separating off the contribution from base classifier can then write the error function the form exp tnfm mym exp mym where the coefficients exp tnfm can viewed constants because are optimizing only and denote the set data points that are correctly classified and denote the remaining misclassified points then can turn rewrite the error function the form boosting when minimize this with respect see that the second term constant and this equivalent minimizing because the overall multiplicative factor front the summation does not affect the location the minimum similarly minimizing with respect obtain which defined from see that having found and the weights the data points are updated using making use the fact that exp mym tnym see that the weights are updated the next iteration using exp exp because the term exp independent see that weights all data points the same factor and can discarded thus obtain finally once all the base classifiers are trained new data points are classified evaluating the sign the combined function defined according because the factor does not affect the sign can omitted giving"}, "255": {"Section": "14.3.2", "Title": "Error functions for boosting", "Content": "the exponential error function that minimized the adaboost algorithm differs from those considered previous chapters gain some insight into the nature the exponential error function first consider the expected error given exp exp perform variational minimization with respect all possible functions obtain exercise exercise combining models figure plot the exponential green and rescaled cross entropy red error functions along with the hinge error blue used support vector machines and the misclassification for large error black note that negative values the cross entropy gives linearly increasing penalty whereas the exponential loss gives exponentially increasing penalty which half the log odds thus the adaboost algorithm seeking the best approximation the log odds ratio within the space functions represented the linear combination base classifiers subject the constrained minimization resulting from the sequential optimization strategy this result motivates the use the sign function arrive the final classification decision have already seen that the minimizer the cross entropy error for two class classification given the posterior class probability the case target variable have seen that the error function given exp this compared with the exponential error function figure where have divided the cross entropy error constant factor that passes through the point for ease comparison see that both can seen continuous approximations the ideal misclassification error function advantage the exponential error that its sequential minimization leads the simple adaboost scheme one drawback however that penalizes large negative values much more strongly than cross entropy particular see that for large negative values the cross entropy grows linearly with whereas the exponential error function grows exponentially with thus the exponential error function will much less robust outliers misclassified data points another important difference between cross entropy and the exponential error function that the latter cannot interpreted the log likelihood function any well defined probabilistic model furthermore the exponential error does not generalize classification problems having classes again contrast the cross entropy for probabilistic model which easily generalized give the interpretation boosting the sequential optimization additive model under exponential error friedman opens the door wide range boosting like algorithms including multiclass extensions altering the choice error function also motivates the extension regression problems friedman consider sum squares error function for regression then sequential minimization additive model the form simply involves fitting each new base classifier the residual errors from the previous model have noted however the sum squares error not robust outliers and this section exercise section exercise figure comparison the squared error green with the absolute error red showing how the latter places much less emphasis large errors and hence more robust outliers and mislabelled data points"}, "256": {"Section": "14.4", "Title": "Tree-based Models", "Content": "can addressed basing the boosting algorithm the absolute deviation instead these two error functions are compared figure tree based models there are various simple but widely used models that work partitioning the input space into cuboid regions whose edges are aligned with the axes and then assigning simple model for example constant each region they can viewed model combination method which only one model responsible for making predictions any given point input space the process selecting specific model given new input can described sequential decision making process corresponding the traversal binary tree one that splits into two branches each node here focus particular tree based framework called classification and regression trees cart breiman although there are many other variants going such names and quinlan quinlan figure shows illustration recursive binary partitioning the input space along with the corresponding tree structure this example the first step figure illustration two dimensional input space that has been partitioned into five regions using axis aligned boundaries combining models figure binary tree corresponding the parinput space shown figtitioning ure divides the whole the input space into two regions according whether where parameter the model this creates two subregions each which can then subdivided independently for instance the region further subdivided according whether giving rise the regions denoted and the recursive subdivision can described the traversal the binary tree shown figure for any new input determine which region falls into starting the top the tree the root node and following path down specific leaf node according the decision criteria each node note that such decision trees are not probabilistic graphical models within each region there separate model predict the target variable for instance regression might simply predict constant over each region classification might assign each region specific class key property treebased models which makes them popular fields such medical diagnosis for example that they are readily interpretable humans because they correspond sequence binary decisions applied the individual input variables for instance predict patient disease might first ask their temperature greater than some threshold the answer yes then might next ask their blood pressure less than some threshold each leaf the tree then associated with specific diagnosis order learn such model from training set have determine the structure the tree including which input variable chosen each node form the split criterion well the value the threshold parameter for the split also have determine the values the predictive variable within each region consider first regression problem which the goal predict single target variable from dimensional vector input variables the training data consists input vectors along with the corresponding continuous labels the partitioning the input space given and minimize the sum squares error function then the optimal value the predictive variable within any given region just given the average the values for those data points that fall that region now consider how determine the structure the decision tree even for fixed number nodes the tree the problem determining the optimal structure including choice input variable for each split well the corresponding threshexercise the pruning criterion then given the regularization parameter determines the trade off between the overall residual sum squares error and the complexity the model measured the number leaf nodes and its value chosen cross validation for classification problems the process growing and pruning the tree similar except that the sum squares error replaced more appropriate measure tree based models olds minimize the sum squares error usually computationally infeasible due the combinatorially large number possible solutions instead greedy optimization generally done starting with single root node corresponding the whole input space and then growing the tree adding nodes one time each step there will some number candidate regions input space that can split corresponding the addition pair leaf nodes the existing tree for each these there choice which the input variables split well the value the threshold the joint optimization the choice region split and the choice input variable and threshold can done efficiently exhaustive search noting that for given choice split variable and threshold the optimal choice predictive variable given the local average the data noted earlier this repeated for all possible choices variable split and the one that gives the smallest residual sum squares error retained given greedy strategy for growing the tree there remains the issue when stop adding nodes simple approach would stop when the reduction residual error falls below some threshold however found empirically that often none the available splits produces significant reduction error and yet after several more splits substantial error reduction found for this reason common practice grow large tree using stopping criterion based the number data points associated with the leaf nodes and then prune back the resulting tree the pruning based criterion that balances residual error against measure model complexity denote the starting tree for pruning then define subtree can obtained pruning nodes from other words collapsing internal nodes combining the corresponding regions suppose the leaf nodes are indexed with leaf node representing region input space having data points and denoting the total number leaf nodes the optimal prediction for region then given and the corresponding contribution the residual sum squares then exercise these both vanish for and and have maximum they encourage the formation regions which high proportion the data points are assigned one class the cross entropy and the gini index are better measures than the misclassification rate for growing the tree because they are more sensitive the node probabilities also unlike misclassification rate they are differentiable and hence better suited gradient based optimization methods for subsequent pruning the tree the misclassification rate generally used the human interpretability tree model such cart often seen its major strength however practice found that the particular tree structure that learned very sensitive the details the data set that small change the training data can result very different set splits hastie there are other problems with tree based methods the kind considered this section one that the splits are aligned with the axes the feature space which may very suboptimal for instance separate two classes whose optimal decision boundary runs degrees the axes would need large number axis parallel splits the input space compared single non axis aligned split furthermore the splits decision tree are hard that each region input space associated with one and only one leaf node model the last issue particularly problematic regression where are typically aiming model smooth functions and yet the tree model produces piecewise constant predictions with discontinuities the split boundaries"}, "257": {"Section": "14.5", "Title": "Conditional Mixture Models", "Content": "have seen that standard decision trees are restricted hard axis aligned splits the input space these constraints can relaxed the expense interpretability allowing soft probabilistic splits that can functions all the input variables not just one them time also give the leaf models probabilistic interpretation arrive fully probabilistic tree based model called the hierarchical mixture experts which consider section alternative way motivate the hierarchical mixture experts model start with standard probabilistic mixtures unconditional density models such gaussians and replace the component densities with conditional distributions here"}, "258": {"Section": "14.5.1", "Title": "Mixtures of linear regression models", "Content": "chapter combining models performance define the proportion data points region assigned class where then two commonly used choices are the cross entropy and the gini index conditional mixture models logistic regression models section the simplest case the mixing coefficients are independent the input variables make further generalization allow the mixing coefficients also depend the inputs then obtain mixture experts model finally allow each component the mixture model itself mixture experts model then obtain hierarchical mixture experts mixtures linear regression models one the many advantages giving probabilistic interpretation the linear regression model that can then used component more complex probabilistic models this can done for instance viewing the conditional distribution representing the linear regression model node directed probabilistic graph here consider simple example corresponding mixture linear regression models which represents straightforward extension the gaussian mixture model discussed section the case conditional gaussian distributions therefore consider linear regression models each governed its own weight parameter many applications will appropriate use common noise variance governed precision parameter for all components and this the case consider here will once again restrict attention single target variable though the extension multiple outputs straightforward denote the mixing coefficients then the mixture distribution can written where denotes the set all adaptive parameters the model namely and the log likelihood function for this model given data set observations then takes the form where denotes the vector target variables order maximize this likelihood function can once again appeal the algorithm which will turn out simple extension the algorithm for unconditional gaussian mixtures section can therefore build our experience with the unconditional mixture and introduce set binary latent variables where znk which for each data point all the elements are zero except for single value indicating which component the mixture was responsible for generating that data point the joint distribution over latent and observed variables can represented the graphical model shown figure the complete data log likelihood function then takes the form znk exercise exercise combining models figure probabilistic directed graph representing mixture linear regression models defined the algorithm begins first choosing initial value old for the model parameters the step these parameter values are then used evaluate the posterior probabilities responsibilities each component for every data point given znk old the responsibilities are then used determine the expectation with respect the posterior distribution old the complete data log likelihood which takes the form old lnn the step maximize the function old with respect keeping the fixed for the optimization with respect the mixing coefficients need which can done with the aid take account the constraint lagrange multiplier leading step estimation equation for the form exercise note that this has exactly the same form the corresponding result for simple mixture unconditional gaussians given next consider the maximization with respect the parameter vector the kth linear regression model substituting for the gaussian distribution see that the function old function the parameter vector takes the form old const where the constant term includes the contributions from other weight vectors for note that the quantity are maximizing similar the negative the standard sum squares error for single linear regression model but with the inclusion the responsibilities this represents weighted least squares conditional mixture models problem which the term corresponding the nth data point carries weighting coefficient given which could interpreted effective precision for each data point see that each component linear regression model the mixture governed its own parameter vector fitted separately the whole data set the step but with each data point weighted the responsibility that model takes for that data point setting the derivative with respect equal zero gives which can write matrix notation trk where diag diagonal matrix size solving for obtain this represents set modified normal equations corresponding the weighted least squares problem the same form found the context logistic regression note that after each step the matrix will change and will have solve the normal equations afresh the subsequent step trkt trk finally maximize old with respect keeping only terms that depend the function old can written old setting the derivative with respect equal zero and rearranging obtain the step equation for the form figure illustrate this algorithm using the simple example fitting mixture two straight lines data set having one input variable and one target variable the predictive density plotted figure using the converged parameter values obtained from the algorithm corresponding the right hand plot figure also shown this figure the result fitting single linear regression model which gives unimodal predictive density see that the mixture model gives much better representation the data distribution and this reflected the higher likelihood value however the mixture model also assigns significant probability mass regions where there data because its predictive distribution bimodal for all values this problem can resolved extending the model allow the mixture coefficients themselves functions leading models such the mixture density networks discussed section and hierarchical mixture experts discussed section combining models figure example synthetic data set shown the green points having one input variable and one target variable together with mixture two linear regression models whose mean functions where are shown the blue and red lines the upper three plots show the initial configuration left the result running iterations centre and the result after iterations right here was initialized the reciprocal the true variance the set target values the lower three plots show the corresponding responsibilities plotted vertical line for each data point which the length the blue segment gives the posterior probability the blue line for that data point and similarly for the red segment"}, "259": {"Section": "14.5.2", "Title": "Mixtures of logistic models", "Content": "because the logistic regression model defines conditional distribution for the target variable given the input vector straightforward use the component distribution mixture model thereby giving rise richer family conditional distributions compared single logistic regression model this example involves straightforward combination ideas encountered earlier sections the book and will help consolidate these for the reader the conditional distribution the target variable for probabilistic mixture logistic regression models given kyt where the feature vector denotes the adjustable parameters namely and now suppose are given data set the corresponding likelihood the output component and conditional mixture models figure the left plot shows the predictive conditional density corresponding the converged solution figure this gives log likelihood value vertical slice through one these plots particular value represents the corresponding conditional distribution which see bimodal the plot the right shows the predictive density for single linear regression model fitted the same data set using maximum likelihood this model has smaller log likelihood function then given kytn where ynk and can maximize this likelihood function iteratively making use the algorithm this involves introducing latent variables znk that correspond coded binary indicator variable for each data point the complete data likelihood function then given ynk kytn ynk znk where the matrix latent variables with elements znk initialize the algorithm choosing initial value old for the model parameters the step then use these parameter values evaluate the posterior probabilities the components for each data point which are given znk old kytn jytn ynk ynj these responsibilities are then used find the expected complete data log likelihood function given old ynk ynk combining models section section exercise the step involves maximization this function with respect keeping old and hence fixed maximization with respect can done the usual way giving with lagrange multiplier enforce the summation constraint the familiar result determine the note that the old function comprises sum over terms indexed each which depends only one the vectors that the different vectors are decoupled the step the algorithm other words the different components interact only via the responsibilities which are fixed during the step note that the step does not have closed form solution and must solved iteratively using for instance the iterative reweighted least squares irls algorithm the gradient and the hessian for the vector are given ynk nkynk ynk where denotes the gradient with respect for fixed these are independent for and can solve for each separately using the irls algorithm thus the step equations for component correspond simply fitting single logistic regression model weighted data set which data point carries weight figure shows example the mixture logistic regression models applied simple classification problem the extension this model mixture softmax models for more than two classes straightforward"}, "260": {"Section": "14.5.3", "Title": "Mixtures of experts", "Content": "section considered mixture linear regression models and section discussed the analogous mixture linear classifiers although these simple mixtures extend the flexibility linear models include more complex multimodal predictive distributions they are still very limited can further increase the capability such models allowing the mixing coefficients themselves functions the input variable that this known mixture experts model jacobs which the mixing coefficients are known gating functions and the individual component densities are called experts the notion behind the terminology that different components can model the distribution different regions input space they are experts making predictions their own regions and the gating functions determine which components are dominant which region the gating functions must satisfy the usual constraints for mixing coefficients namely and they can therefore represented for example linear softmax models the form and the experts are also linear regression classification models then the whole model can fitted efficiently using the algorithm with iterative reweighted least squares being employed the step jordan and jacobs exercise section such model still has significant limitations due the use linear models for the gating and expert functions much more flexible model obtained using multilevel gating function give the hierarchical mixture experts hme model jordan and jacobs understand the structure this model imagine mixture distribution which each component the mixture itself mixture distribution for simple unconditional mixtures this hierarchical mixture trivially equivalent single flat mixture distribution however when the mixing coefficients are input dependent this hierarchical model becomes nontrivial the hme model can also viewed probabilistic version decision trees discussed section and can again trained efficiently maximum likelihood using algorithm with irls the step bayesian treatment the hme has been given bishop and svens based variational inference shall not discuss the hme detail here however worth pointing out the close connection with the mixture density network discussed section the principal advantage the mixtures experts model that can optimized which the step for each mixture component and gating model involves convex optimization although the overall optimization nonconvex contrast the advantage the mixture density network approach that the component conditional mixture models figure illustration mixture logistic regression models the left plot shows data points drawn from two classes denoted red and blue which the background colour which varies from pure red pure blue denotes the true probability the class label the centre plot shows the result fitting single logistic regression model using maximum likelihood which the background colour denotes the corresponding probability the class label because the colour near uniform purple see that the model assigns probability around each the classes over most input space the right plot shows the result fitting mixture two logistic regression models which now gives much higher probability the correct labels for many the points the blue class combining models exercises densities and the mixing coefficients share the hidden units the neural network furthermore the mixture density network the splits the input space are further relaxed compared the hierarchical mixture experts that they are not only soft and not constrained axis aligned but they can also nonlinear"}}